{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0x01 文本数据预处理\n",
    "最终的任务是按照：句子 --- 句子 的形式（sentence to sentence）\n",
    "数据预处理的目的是将句子向量化。需要以下步骤：\n",
    "1. 将句子转换成为单元 tokenized\n",
    "2. 根据单元构建词典 vocab\n",
    "3. 根据vocab来对句子向量化\n",
    "最终得到[1,3,4,5,...]和[5,4,2,...]之间的转换\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> tokenized() defined.\n",
      "Example for tokenized():\n",
      "------------------------------------\n",
      "Input sentence: \n",
      "Hello world, I am a student.\n",
      "Output tokens: \n",
      "['Hello', 'world', ',', 'I', 'am', 'a', 'student', '.']\n",
      "------------------------------------\n",
      "The size of vocabulary before filtering is:  11581\n",
      "--> build_vocab() defined.\n",
      "Example for build_vocab():\n",
      "------------------------------------\n",
      "The size of vocabulary is:  6247\n",
      "The index of word \"the\" is:  3\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def tokenized(sentence):\n",
    "    sentence = sentence.replace(',',' ,').replace('.',' .')\n",
    "    return sentence.split()\n",
    "print('--> tokenized() defined.')\n",
    "print('Example for tokenized():')\n",
    "print('------------------------------------')\n",
    "temp = 'Hello world, I am a student.'\n",
    "print('Input sentence: ')\n",
    "print(temp)\n",
    "print('Output tokens: ')\n",
    "print(tokenized(temp))\n",
    "print('------------------------------------')\n",
    "\n",
    "# torchtext.vocab can help us to build vocabulary\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter\n",
    "def build_vocab_my(tokenizer,filepath,min_freq=2,specials=None):\n",
    "    ''' \n",
    "    tokenizer: function to tokenize sentence\n",
    "    filepath: path to the file\n",
    "    min_freq: the minimum frequency of the word\n",
    "    specials: the special tokens\n",
    "    '''\n",
    "    counter = Counter()\n",
    "    if specials is None:\n",
    "        specials = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = tokenizer(line.strip())\n",
    "            counter.update(words)\n",
    "    print('The size of vocabulary before filtering is: ',len(counter))\n",
    "    return vocab(counter,min_freq=min_freq,specials=specials)\n",
    "\n",
    "path = '../input/translate/train.en'\n",
    "vocab = build_vocab(tokenized,path,min_freq=2)\n",
    "print('--> build_vocab() defined.')\n",
    "print('Example for build_vocab():')\n",
    "print('------------------------------------')\n",
    "print('The size of vocabulary is: ',len(vocab))\n",
    "print('The index of word \"the\" is: ',vocab['<eos>'])\n",
    "print('------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0x02 定义数据读取类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of vocabulary before filtering is:  11581\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'min_freq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb 单元格 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m batchsize \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m shuffle \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m english_data \u001b[39m=\u001b[39m LoadEnglishData(tokenized,train_path,batchsize,shuffle)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m train_dataloader,val_dataloader,test_dataloader \u001b[39m=\u001b[39m english_data\u001b[39m.\u001b[39mload_dataloader(train_path,test_path,valid_path)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m train_dataloader:\n",
      "\u001b[1;32m/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb 单元格 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mThe size of vocabulary before filtering is: \u001b[39m\u001b[39m'\u001b[39m,\u001b[39mlen\u001b[39m(counter))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m vocab(counter,min_freq\u001b[39m=\u001b[39mmin_freq,specials\u001b[39m=\u001b[39mspecials)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39men_vocab \u001b[39m=\u001b[39m build_vocab_my(tokenized,path[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mde_vocab \u001b[39m=\u001b[39m build_vocab_my(tokenized,path[\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatchsize \u001b[39m=\u001b[39m batchsize\n",
      "\u001b[1;32m/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb 单元格 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m         counter\u001b[39m.\u001b[39mupdate(words)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mThe size of vocabulary before filtering is: \u001b[39m\u001b[39m'\u001b[39m,\u001b[39mlen\u001b[39m(counter))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m vocab(counter,min_freq\u001b[39m=\u001b[39;49mmin_freq,specials\u001b[39m=\u001b[39;49mspecials)\n",
      "File \u001b[0;32m~/anaconda3/envs/xclds/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'min_freq'"
     ]
    }
   ],
   "source": [
    "# torchtext.data.utils.get_tokenizer can help us to tokenize sentence\n",
    "from torch.utils.data import DataLoader\n",
    "class LoadEnglishData:\n",
    "    def __init__(self,tokenizer,path,batchsize=8,shuffle=True):\n",
    "        '''\n",
    "        tokenizer: function to tokenize sentence \n",
    "        '''\n",
    "        self.tokenizer = tokenizer\n",
    "        self.path = path\n",
    "        # self.en_tokenizer = get_tokenizer('spacy', language='../input/translate/en_core_web_sm-3.0.0/en_core_web_sm/en_core_web_sm-3.0.0/')\n",
    "        # self.de_tokenizer = get_tokenizer('spacy', language='../input/translate/de_core_news_sm-3.0.0/de_core_news_sm/de_core_news_sm-3.0.0/')\n",
    "        def build_vocab_my(tokenizer,filepath,min_freq=2,specials=None):\n",
    "            ''' \n",
    "            tokenizer: function to tokenize sentence\n",
    "            filepath: path to the file\n",
    "            min_freq: the minimum frequency of the word\n",
    "            specials: the special tokens\n",
    "            '''\n",
    "            counter = Counter()\n",
    "            if specials is None:\n",
    "                specials = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "            with open(filepath, encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    words = tokenizer(line.strip())\n",
    "                    counter.update(words)\n",
    "            print('The size of vocabulary before filtering is: ',len(counter))\n",
    "            return vocab(counter,min_freq=min_freq,specials=specials)\n",
    "        self.en_vocab = build_vocab_my(tokenized,path[0])\n",
    "        self.de_vocab = build_vocab_my(tokenized,path[1])\n",
    "        self.batchsize = batchsize\n",
    "        self.PAD_IDX = self.en_vocab['<pad>']\n",
    "        self.BOS_IDX = self.en_vocab['<bos>']\n",
    "        self.EOS_IDX = self.en_vocab['<eos>']\n",
    "        self.specials = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "    \n",
    "    def data_process(self,path):\n",
    "        '''\n",
    "        vectorize the data to dataset\n",
    "        \n",
    "        :param path: path to the file\n",
    "        '''\n",
    "        raw_de_iter = iter(open(path[0], encoding=\"utf8\"))\n",
    "        raw_en_iter = iter(open(path[1], encoding=\"utf8\"))\n",
    "        data = []\n",
    "        for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "            de_tensor_ = torch.tensor([int(self.de_vocab[token]) for token in self.tokenizer(raw_de)], dtype=torch.long)\n",
    "            en_tensor_ = torch.tensor([int(self.en_vocab[token]) for token in self.tokenizer(raw_en)], dtype=torch.long)\n",
    "            data.append((de_tensor_, en_tensor_))\n",
    "        return data\n",
    "\n",
    "    def load_dataloader(self,train_path,test_path,valid_path):\n",
    "        '''\n",
    "        load data from the file\n",
    "        '''\n",
    "        train_data = self.data_process(train_path)\n",
    "        val_data = self.data_process(valid_path)\n",
    "        test_data = self.data_process(test_path)\n",
    "\n",
    "        train_dataloader = DataLoader(train_data, batch_size=self.batchsize, shuffle=True, collate_fn=self.collate_fnX)\n",
    "        val_dataloader = DataLoader(val_data, batch_size=self.batchsize, shuffle=True, collate_fn=self.collate_fnX)\n",
    "        test_dataloader = DataLoader(test_data, batch_size=self.batchsize, shuffle=True, collate_fn=self.collate_fnX)\n",
    "        return train_dataloader,val_dataloader,test_dataloader\n",
    "    \n",
    "    def collate_fnX(self,data_batch):\n",
    "        '''\n",
    "        Attention : which we need to collate the function\n",
    "        1. 为decoder增加bos和eos\n",
    "        2. 在同一个batch中将数据padding到同一长度\n",
    "        不同batch的长度不一样\n",
    "        transform data with padding\n",
    "        '''\n",
    "        de_batch, en_batch = [], []\n",
    "        for (de_item, en_item) in data_batch:\n",
    "            de_batch.append(de_item)\n",
    "            en_batch.append(torch.cat([torch.tensor([self.en_vocab['<bos>']]), en_item, torch.tensor([self.en_vocab['<eos>']])], dim=0))\n",
    "        de_batch = torch.nn.utils.rnn.pad_sequence(de_batch, padding_value=self.PAD_IDX)\n",
    "        en_batch = torch.nn.utils.rnn.pad_sequence(en_batch, padding_value=self.PAD_IDX)\n",
    "        return de_batch, en_batch\n",
    "        \n",
    "\n",
    "    def create_mask(self,x,y,device='cpu'):\n",
    "        '''\n",
    "        create mask\n",
    "        '''\n",
    "        x_seq_len = x.size(0)\n",
    "        y_seq_len = y.size(0)\n",
    "        # encoder mask (for masking the padding tokens)\n",
    "        x_mask = torch.zeros((x_seq_len,x_seq_len),device=device).type(torch.bool)\n",
    "\n",
    "        # decoder mask (for masking the future tokens)\n",
    "        tgt_mask = torch.triu(torch.ones((y_seq_len,y_seq_len),device=device)).type(torch.bool)\n",
    "        y_mask = tgt_mask.float().masked_fill(tgt_mask==0,float('-inf')).masked_fill(tgt_mask==1,float(0.0))\n",
    "        y_mask = y_mask.transpose(0,1)\n",
    "\n",
    "        # x_padding mask (for masking out the padding tokens    )\n",
    "        x_padding_mask = (x == self.PAD_IDX).transpose(0,1)\n",
    "        # y_padding_mask (for masking out the padding tokens)\n",
    "        y_padding_mask = (y == self.PAD_IDX).transpose(0,1)\n",
    "        return x_mask,y_mask,x_padding_mask,y_padding_mask\n",
    "\n",
    "\n",
    "train_path = ['../input/translate/train.en','../input/translate/train.de']  \n",
    "valid_path = ['../input/translate/val.en','../input/translate/val.de']\n",
    "test_path = ['../input/translate/test_2016_flickr.en','../input/translate/test_2016_flickr.de']\n",
    "batchsize = 8\n",
    "shuffle = True\n",
    "english_data = LoadEnglishData(tokenized,train_path,batchsize,shuffle)\n",
    "\n",
    "train_dataloader,val_dataloader,test_dataloader = english_data.load_dataloader(train_path,test_path,valid_path)\n",
    "for x,y in train_dataloader:\n",
    "    print('The shape of x is: ',x.shape)\n",
    "    print('The shape of y is: ',y.shape)\n",
    "    print(english_data.create_mask(x,y))\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0x03 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of x is:  torch.Size([19, 8])\n",
      "The shape of y is:  torch.Size([20, 8])\n",
      "The shape of x_mask is:  torch.Size([19, 19])\n",
      "The shape of y_mask is:  torch.Size([20, 20])\n",
      "The shape of x_padding_mask is:  torch.Size([8, 19])\n",
      "The shape of y_padding_mask is:  torch.Size([8, 20])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb 单元格 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=119'>120</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mThe shape of x_padding_mask is: \u001b[39m\u001b[39m'\u001b[39m,x_padding_mask\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=120'>121</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mThe shape of y_padding_mask is: \u001b[39m\u001b[39m'\u001b[39m,y_padding_mask\u001b[39m.\u001b[39mshape)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=121'>122</a>\u001b[0m output \u001b[39m=\u001b[39m model(x,y,x_mask,y_mask,x_padding_mask,y_padding_mask,x_padding_mask)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=122'>123</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mThe shape of output is: \u001b[39m\u001b[39m'\u001b[39m,output\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=123'>124</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/xclds/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb 单元格 7\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m             x,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m             y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m             y_padding_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m             memory_key_padding_mask):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m     x_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_embedding(x))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     y_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_embedding(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_embedding(y))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mThe shape of x_embedding is: \u001b[39m\u001b[39m'\u001b[39m,x_embedding\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/xclds/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb 单元格 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.16.2.243/home/step/data/python-smelly-cat/zoo_dl/transformer.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding(x\u001b[39m.\u001b[39;49mlong())\u001b[39m*\u001b[39mmath\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/xclds/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/xclds/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    163\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    164\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/anaconda3/envs/xclds/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch \n",
    "from torch.nn import Transformer \n",
    "import math\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size,embedding_size) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_size)\n",
    "        self.embedding_size = embedding_size\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.embedding(x.long())*math.sqrt(self.embedding_size)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,dimen,dropout=0.1,max_len=10000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        PE = torch.zeros(max_len,dimen)\n",
    "        position = torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0,dimen,2).float() * (-math.log(10000.0) / dimen))\n",
    "        PE[:,0::2] = torch.sin(position * div_term)\n",
    "        PE[:,1::2] = torch.cos(position * div_term)\n",
    "        PE = PE.unsqueeze(0).transpose(0,1)\n",
    "        self.register_buffer('PE',PE)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x + self.PE[:x.size(0),:]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class TranslationModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 x_vocab_size,\n",
    "                 y_vocab_size,\n",
    "                 d_model,\n",
    "                 nhead,\n",
    "                 num_encoder_layers,\n",
    "                 num_decoder_layers,\n",
    "                 dim_feedforward,\n",
    "                 dropout,\n",
    "                 device):\n",
    "        super(TranslationModel,self).__init__()\n",
    "        \n",
    "        self.x_embedding = TokenEmbedding(x_vocab_size,d_model)\n",
    "        self.y_embedding = TokenEmbedding(y_vocab_size,d_model)\n",
    "        self.positional_embedding = PositionalEncoding(d_model,dropout)\n",
    "        self.transformer = Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.generator = nn.Linear(d_model,y_vocab_size)\n",
    "        self._reset_parameters()\n",
    "    \n",
    "    def forward(self,\n",
    "                x,\n",
    "                y,\n",
    "                x_mask,\n",
    "                y_mask,\n",
    "                x_padding_mask,\n",
    "                y_padding_mask,\n",
    "                memory_key_padding_mask):\n",
    "        x_embedding = self.positional_embedding(self.x_embedding(x))\n",
    "        y_embedding = self.positional_embedding(self.y_embedding(y))\n",
    "        print('The shape of x_embedding is: ',x_embedding.shape)\n",
    "        print('The shape of y_embedding is: ',y_embedding.shape)\n",
    "        # output = self.transformer(x_embedding,\n",
    "        #                           y_embedding,\n",
    "        #                           src_mask=x_mask,\n",
    "        #                           tgt_mask=y_mask,\n",
    "        #                           src_key_padding_mask=x_padding_mask,\n",
    "        #                           tgt_key_padding_mask=y_padding_mask,\n",
    "        #                           memory_key_padding_mask=memory_key_padding_mask)\n",
    "        # output = self.generator(output)\n",
    "        # return output\n",
    "    \n",
    "    def inference(self,x,y,max_len=100):\n",
    "        x_embedding = self.positional_embedding(self.x_embedding(x))\n",
    "        memory = self.transformer.encoder(x_embedding)\n",
    "\n",
    "        y_embedding = self.positional_embedding(self.y_embedding(y))\n",
    "        outputs =  self.transformer.decoder(y_embedding,memory)\n",
    "        return outputs\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.generator.weight)\n",
    "        self.generator.bias.data.zero_()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x_vocab_size = len(vocab)\n",
    "y_vocab_size = len(vocab)\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "model = TranslationModel(x_vocab_size,\n",
    "                            y_vocab_size,\n",
    "                            d_model,\n",
    "                            nhead,\n",
    "                            num_encoder_layers,\n",
    "                            num_decoder_layers,\n",
    "                            dim_feedforward,\n",
    "                            dropout,\n",
    "                            device)\n",
    "\n",
    "for x,y in train_dataloader:\n",
    "    x_mask,y_mask,x_padding_mask,y_padding_mask = english_data.create_mask(x,y,device)\n",
    "    print('The shape of x is: ',x.shape)\n",
    "    print('The shape of y is: ',y.shape)\n",
    "    print('The shape of x_mask is: ',x_mask.shape)\n",
    "    print('The shape of y_mask is: ',y_mask.shape)\n",
    "    print('The shape of x_padding_mask is: ',x_padding_mask.shape)\n",
    "    print('The shape of y_padding_mask is: ',y_padding_mask.shape)\n",
    "    output = model(x,y,x_mask,y_mask,x_padding_mask,y_padding_mask,x_padding_mask)\n",
    "    print('The shape of output is: ',output.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xclds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
