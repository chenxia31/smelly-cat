{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfomer 从零开始\n",
    "基于Encoder-Decoder框架，面向Seq2Seq类别任务，采用Self- attention和Multi-head attention机制，同时使用Token embedding+Position embedding、残差连接、layer norm、Mask等tricks，实现了惊为天人的Transformer。\n",
    "\n",
    "潜在直觉：在ED框架下，最不可忍受的推理的不可并行，仔细分析背后的过程，可以看到是因为隐藏状态需要一个一个输入。那么是否有一种方式可以直接得到所有的隐藏状态，并且用一种线性变化就可以得到预测后的输出，我觉得这个是transformer的解决的问题。\n",
    "1. Attention解决了并行性，扩大了感受野的广度\n",
    "2. Position embedding、mask完善了时序信息\n",
    "3. 残差连接、layer norm是基本的网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0x01 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input size is  torch.Size([5, 2])\n",
      "TokenEmbedding size is  tensor([[[ 23.3701, -43.0523, -41.0195,  ..., -19.0137,   7.7349, -10.3376],\n",
      "         [  4.5575, -17.0709, -20.0782,  ...,  13.5287,  17.3372,  16.6027]],\n",
      "\n",
      "        [[-12.3629,  -4.9283,   0.2701,  ...,  16.3192,   9.2782,  -9.4326],\n",
      "         [-18.9739, -20.1240, -50.7473,  ...,   3.4778,  20.8694,  20.9355]],\n",
      "\n",
      "        [[ 26.6863,  36.4356,  22.5699,  ..., -10.3694, -23.3673, -11.5537],\n",
      "         [ 26.6863,  36.4356,  22.5699,  ..., -10.3694, -23.3673, -11.5537]],\n",
      "\n",
      "        [[ -8.3441,   7.6558,  23.0942,  ...,   7.3344,  -0.1192, -10.6960],\n",
      "         [ -3.8388, -37.2422,  13.3843,  ...,  28.1898, -30.3586,   8.6297]],\n",
      "\n",
      "        [[  8.4035, -13.6004, -29.5773,  ...,   3.1634,  36.3206,  34.4238],\n",
      "         [-26.4645,  31.2842,  25.9679,  ..., -21.6577,  34.6043,  -3.4274]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "token embedding size is  torch.Size([5, 2, 512])\n",
      "Position embedding size is torch.Size([5, 2, 512])\n",
      "The size must be [seq_len,batchsize,embedding_size]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size,embedding_size) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_size)\n",
    "        self.embedding_size = embedding_size\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.embedding(x.long())*math.sqrt(self.embedding_size)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,dimen,dropout=0.1,max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        PE = torch.zeros(max_len,dimen)\n",
    "        position = torch.arange(0,max_len,dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0,dimen,2).float() * (-math.log(10000.0) / dimen))\n",
    "        PE[:,0::2] = torch.sin(position * div_term)\n",
    "        PE[:,1::2] = torch.cos(position * div_term)\n",
    "        PE = PE.unsqueeze(0).transpose(0,1)\n",
    "        self.register_buffer('PE',PE)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x + self.PE[:x.size(0),:]\n",
    "        return self.dropout(x)\n",
    "\n",
    "x = torch.tensor([[1,2,3,4,5],[5,6,7,8,9]],dtype=torch.float)\n",
    "# size of the x is [seq_len,batch_size],不用one-hot表示\n",
    "x = x.reshape(5,2) \n",
    "print('The input size is ',x.shape)\n",
    "vocab_size = 10\n",
    "embedding_size = 512\n",
    "\n",
    "# size of the x is [seq_len,batch_size,embedding_size]\n",
    "token_embedding = TokenEmbedding(vocab_size,embedding_size)\n",
    "x = token_embedding(x)\n",
    "print('TokenEmbedding size is ',x)\n",
    "print('token embedding size is ',x.shape)\n",
    "pos_embedding = PositionalEncoding(embedding_size)\n",
    "x = pos_embedding(x)\n",
    "print('Position embedding size is',x.shape)\n",
    "print('The size must be [seq_len,batchsize,embedding_size]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0x02 Encoder layer 实现\n",
    "主要是实现基于attention的encoder和decoder layer，并搭建好encoder和decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the transformer encoder layer\n"
     ]
    }
   ],
   "source": [
    "class TransformerEncoderLayerStractch(nn.Module):\n",
    "    def __init__(self,dimen,nhead,dim_forward=2048,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(dimen,nhead,dropout=dropout)\n",
    "        self.dropoutAttn = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(dimen)\n",
    "        self.linear1 = nn.Linear(dimen,dim_forward)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_forward,dimen)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(dimen)\n",
    "    \n",
    "    def forward(self,x,mask=None,src_key_padding_mask=None):\n",
    "        '''\n",
    "        x: [seq_len,batch_size,embedding_size]\n",
    "        '''\n",
    "        x2,_ = self.self_attn(x,x,x,attn_mask=mask,key_padding_mask=src_key_padding_mask)\n",
    "        # x2: [seq_len,batch_size,embedding_size*nhead]\n",
    "        x = x + self.dropoutAttn(x2)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x2 = self.activation(self.linear1(x))\n",
    "        x2 = self.linear2(self.dropout1(x2))\n",
    "        x = x + self.dropout2(x2)\n",
    "\n",
    "        x = self.norm2(x)\n",
    "        return x \n",
    "print('Test the transformer encoder layer')\n",
    "nhead = 8\n",
    "layer = TransformerEncoderLayerStractch(embedding_size,nhead)\n",
    "x = layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the transformer encoder\n",
      "tensor(0.5778, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "def get_copy(module,N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "                         \n",
    "class TransformerEncoderStractch(nn.Module):\n",
    "    def __init__(self,encoder_layer,num_layers,norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([encoder_layer for _ in range(num_layers)])\n",
    "        self.norm = norm\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self,x,mask=None,src_key_padding_mask=None):\n",
    "        output = x\n",
    "        for layer in self.layers:\n",
    "            output = layer(output,mask=mask,src_key_padding_mask=src_key_padding_mask)\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        return output \n",
    "\n",
    "encoder = TransformerEncoderStractch(layer,6)\n",
    "x = encoder(x)\n",
    "print('Test the transformer encoder')\n",
    "loss = nn.MSELoss()\n",
    "print(loss(x,encoder(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0x03 Decoder实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the transformer decoder layer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2137, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class TransformerDecoderLayerStractch(nn.Module):\n",
    "    def __init__(self,dimen,nhead,dim_forward=2048,dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(dimen,nhead,dropout=dropout)\n",
    "        self.dropoutAttn = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(dimen)\n",
    "        self.multihead_attn = nn.MultiheadAttention(dimen,nhead,dropout=dropout)\n",
    "        self.dropoutAttn2 = nn.Dropout(dropout)\n",
    "        self.norm2 = nn.LayerNorm(dimen)\n",
    "        self.linear1 = nn.Linear(dimen,dim_forward)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_forward,dimen)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm3 = nn.LayerNorm(dimen)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self,tgt,memory,tgt_mask=None,memory_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None):\n",
    "        ''' \n",
    "        tgt = [tgt_len,batch_size,embedding_size]\n",
    "        memory = [seq_len,batch_size,embedding_size]\n",
    "        '''\n",
    "        tgt2,_ = self.self_attn(tgt,tgt,tgt,attn_mask=tgt_mask,key_padding_mask=tgt_key_padding_mask)\n",
    "        tgt = tgt + self.dropoutAttn(tgt2)\n",
    "        tgt = self.norm1(tgt)\n",
    "        # tgt = [tgt_len,batch_size,embedding_size]\n",
    "\n",
    "        tgt2,_ = self.multihead_attn(tgt,memory,memory,attn_mask=memory_mask,key_padding_mask=memory_key_padding_mask)\n",
    "        tgt = tgt + self.dropoutAttn2(tgt2)\n",
    "        tgt = self.norm2(tgt)\n",
    "        # tgt = [tgt_len,batch_size,embedding_size]\n",
    "\n",
    "        tgt2 = self.activation(self.linear1(tgt))\n",
    "        tgt2 = self.linear2(self.dropout1(tgt2))\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "\n",
    "        tgt = self.norm3(tgt)\n",
    "        # tgt = [tgt_len,batch_size,embedding_size]\n",
    "        return tgt\n",
    "print('Test the transformer decoder layer')\n",
    "decoder_layer = TransformerDecoderLayerStractch(embedding_size,nhead)\n",
    "x = decoder_layer(x,layer(x))\n",
    "print(loss(x,decoder_layer(x,layer(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderStractch(nn.Module):\n",
    "    def __init__(self,decoder_layer,num_layers,norm=None):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer) for _ in range(num_layers)])\n",
    "        self.norm = norm\n",
    "        self.num_layers = num_layers\n",
    "    \n",
    "    def forward(self,tgt,memory,tgt_mask=None,memory_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None):\n",
    "        output = tgt\n",
    "        for layer in self.layers:\n",
    "            output = layer(output,memory,tgt_mask=tgt_mask,memory_mask=memory_mask,tgt_key_padding_mask=tgt_key_padding_mask,memory_key_padding_mask=memory_key_padding_mask)\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "        return output\n",
    "encoder = TransformerEncoderStractch(layer,6)\n",
    "decoder = TransformerDecoderStractch(decoder_layer,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulid the transformer\n",
    "class MyTransformer(nn.Module):\n",
    "    def __init__(self,dimen=512,nhead=8,num_encoder_layers=6,num_decoder_layers=6,dim_forward=2048,dropout=0.1):\n",
    "        super().__init__()\n",
    "        encoder_layer = TransformerEncoderLayerStractch(dimen,nhead,dim_forward,dropout)\n",
    "        encoder_norm = nn.LayerNorm(dimen)\n",
    "        self.encoder = TransformerEncoderStractch(encoder_layer,num_encoder_layers,encoder_norm)\n",
    "\n",
    "        decoder_layer = TransformerDecoderLayerStractch(dimen,nhead,dim_forward,dropout)\n",
    "        decoder_norm = nn.LayerNorm(dimen)\n",
    "        self.decoder = TransformerDecoderStractch(decoder_layer,num_decoder_layers,decoder_norm)\n",
    "\n",
    "        self._reset_parameters()\n",
    "        self.dimen = dimen\n",
    "        self.nhead = nhead\n",
    "    \n",
    "    def forward(self,src,tgt,src_mask=None,tgt_mask=None,memory_mask=None,src_key_padding_mask=None,tgt_key_padding_mask=None,memory_key_padding_mask=None):\n",
    "        memory = self.encoder(src,mask=src_mask,src_key_padding_mask=src_key_padding_mask)\n",
    "        output = self.decoder(tgt,memory,tgt_mask=tgt_mask,memory_mask=memory_mask,tgt_key_padding_mask=tgt_key_padding_mask,memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return output\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def generate_square_subsequent_mask(self,sz):\n",
    "        mask = (torch.triu(torch.ones(sz,sz)) == 1).transpose(0,1)\n",
    "        mask = mask.float().masked_fill(mask == 0,float('-inf')).masked_fill(mask == 1,float(0.0))\n",
    "        return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input size is  torch.Size([12, 2, 128])\n",
      "Test the transformer\n",
      "tensor(0.4967, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seq_len = 12\n",
    "batch_size = 2\n",
    "dimen = 128\n",
    "tag_len = 10\n",
    "nhead = 8\n",
    "input = torch.randn(seq_len,batch_size,dimen)\n",
    "target = torch.randn(tag_len,batch_size,dimen)\n",
    "print('The input size is ',input.shape)\n",
    "\n",
    "model = MyTransformer(dimen=dimen,nhead=nhead,num_decoder_layers=6,num_encoder_layers=6,dim_forward=2048,dropout=0.1)\n",
    "\n",
    "tgt_mask = model.generate_square_subsequent_mask(tag_len)\n",
    "out = model(input,target,tgt_mask=tgt_mask)\n",
    "print('Test the transformer')\n",
    "print(loss(out,model(input,target,tgt_mask=tgt_mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 细品Torch的Multi head attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size is torch.Size([12, 2, 128])\n",
      "torch.Size([12, 2, 128])\n",
      "torch.Size([2, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "layer = nn.MultiheadAttention(embed_dim=dimen,\n",
    "                              num_heads=nhead,\n",
    "                              dropout=0.1)\n",
    "input = torch.randn(seq_len,batch_size,dimen)\n",
    "print('input size is',input.shape)\n",
    "for out in layer(input,input,input):\n",
    "    print(out.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xclds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
