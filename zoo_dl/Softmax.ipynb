{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset:Fashion mnist from the torchvision.datasets,save it in the 'root'\n",
    "# Transform is need to convert the images.type = list or other to the torch.Tensor\n",
    "trans = transforms.ToTensor()\n",
    "\n",
    "# First code to download the dataset\n",
    "# mnist_train = torchvision.datasets.FashionMNIST(root='../data',\n",
    "#                                                 train=True,\n",
    "#                                                 transform=trans,\n",
    "#                                                 download=True)\n",
    "# mnist_test = torchvision.datasets.FashionMNIST(root='../data/',\n",
    "#                                                 train=False,\n",
    "#                                                 transform=trans,\n",
    "#                                                 download=True)\n",
    "\n",
    "mnist_train = torchvision.datasets.FashionMNIST(root='../data',\n",
    "                                                train=True,\n",
    "                                                transform=trans,\n",
    "                                                download=False)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(root='../data/',\n",
    "                                                train=False,\n",
    "                                                transform=trans,\n",
    "                                                download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataloader from the datasets, batchSize is need!\n",
    "device = torch.device('cpu')\n",
    "batch_size = 256\n",
    "train_iter = data.DataLoader(mnist_train,batch_size,shuffle=True)\n",
    "test_iter = data.DataLoader(mnist_test,batch_size,shuffle=False)\n",
    "# for X,y in train_iter:\n",
    "#     print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 12.2915,  14.3853,  12.0534,  ...,  14.0313,  16.5090,  -2.0760],\n",
      "        [  6.6129,   4.9342,  16.3623,  ...,  12.2046,  14.1523,   7.1053],\n",
      "        [ 13.1024,  16.2782,  15.5495,  ...,  10.9177,   6.0375,  -2.5060],\n",
      "        ...,\n",
      "        [  3.5284,   2.7789,   5.9629,  ...,   3.7091,   9.2960,   5.1145],\n",
      "        [  2.4659,  -0.5861,   6.8151,  ...,   1.8687,  11.2332,   6.6702],\n",
      "        [ -0.5806,   2.3784,  19.4988,  ..., -13.8162,  10.2793,   1.1812]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define the network for the softmax regression\n",
    "# One way is use the torch.nn.Sequential() which is easy but not recommended for big network\n",
    "# Another way is using the torch.nn.Module() which is robust\n",
    "import torch.nn as nn \n",
    "# net = nn.Sequential(nn.Flatten(),nn.Linear(28*28,10))\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight,0.01)\n",
    "# net.apply(init_weights)\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.lay1 = nn.Flatten()\n",
    "        self.lay2 = nn.Linear(28*28,10)\n",
    "        \n",
    "        # Method 2\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Linear):\n",
    "                nn.init.normal_(m.weight,0.01)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "            if isinstance(m,nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,mode='fan_out',nonlinearity='relu')\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.lay2(self.lay1(x))\n",
    "\n",
    "# Torch has set the layer's parameters defaultly. If we want to change the parameters we self,there is two way\n",
    "# 1. use the apply() which using the DFS mechanism to find the layers\n",
    "# 2. in the init, use the loop of self.modules() to get the finally check\n",
    "net = MyNet()\n",
    "net.to(device)\n",
    "\n",
    "# Method 2 is show as below:\n",
    "# net.apply(init_weights)\n",
    "for X,y in train_iter:\n",
    "    X=X.to(device)\n",
    "    print(net(X))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss functions,as in the torch is given \n",
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer as SGD\n",
    "opt = torch.optim.SGD(net.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch0is\n",
      "tensor(2.1326)\n",
      "epoch1is\n",
      "tensor(1.6963)\n",
      "epoch2is\n",
      "tensor(1.4781)\n",
      "epoch3is\n",
      "tensor(1.3482)\n",
      "epoch4is\n",
      "tensor(1.3198)\n",
      "epoch5is\n",
      "tensor(1.2067)\n",
      "epoch6is\n",
      "tensor(1.1573)\n",
      "epoch7is\n",
      "tensor(1.1165)\n",
      "epoch8is\n",
      "tensor(1.0697)\n",
      "epoch9is\n",
      "tensor(1.0678)\n"
     ]
    }
   ],
   "source": [
    "# Super parameters\n",
    "epoches = 10\n",
    "for epoch in range(epoches):\n",
    "    net.train()\n",
    "    for X,y in train_iter:\n",
    "        X,y=X.to(device),y.to(device)\n",
    "        \n",
    "        y_hat = net(X)\n",
    "        opt.zero_grad()\n",
    "        l = torch.mean(loss(y_hat,y))\n",
    "        l.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    val = torch.tensor(0,dtype=torch.float,device=device)\n",
    "    with torch.no_grad():\n",
    "        for X,y in test_iter:\n",
    "            X,y=X.to(device),y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat,y)\n",
    "            val += l.sum()\n",
    "        print('epoch'+str(epoch)+'is')\n",
    "        print(val/len(mnist_test))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新坑\n",
    "假如torch的版本小于1.12的话，是不支持Apple的mps设备的，可能会出现inf的情况！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比较重要的代码：\n",
    "明白如何初始化参数，两种方法\n",
    "如何使用nn.module来构建网络模型\n",
    "如何将模型迁移到device上\n",
    "对于train过程中发现的细节，首先在上班部分的model.train()的过程中，先将优化的梯度设置为zero-grad()，在计算出loss之后的梯度下降的过程中backward()，再更新参数step(),来达到梯度下降的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
