{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/step/anaconda3/envs/xclds/lib/python3.9/site-packages/ale_py/roms/__init__.py:89: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
      "  ROMS = resolve_roms()\n"
     ]
    }
   ],
   "source": [
    "import gym, random, pickle, os.path, math, glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import pdb\n",
    "\n",
    "from atari_wrappers import make_atari, wrap_deepmind,LazyFrames\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gym.envs.atari' has no attribute 'AtariEnv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create and wrap the environment\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m make_atari(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPongNoFrameskip-v4\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# only use in no frameskip environment\u001b[39;00m\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m wrap_deepmind(env, scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, frame_stack\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m )\n\u001b[1;32m      4\u001b[0m n_actions \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n",
      "File \u001b[0;32m/media/step/data/python-smelly-cat/zoo_rl/atari_wrappers.py:292\u001b[0m, in \u001b[0;36mmake_atari\u001b[0;34m(env_id, max_episode_steps)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_atari\u001b[39m(env_id, max_episode_steps\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 292\u001b[0m     env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(env_id)\n\u001b[1;32m    293\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mNoFrameskip\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m env\u001b[39m.\u001b[39mspec\u001b[39m.\u001b[39mid\n\u001b[1;32m    294\u001b[0m     env \u001b[39m=\u001b[39m NoopResetEnv(env, noop_max\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/xclds/lib/python3.9/site-packages/gym/envs/registration.py:581\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     env_creator \u001b[39m=\u001b[39m spec_\u001b[39m.\u001b[39mentry_point\n\u001b[1;32m    579\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[39m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     env_creator \u001b[39m=\u001b[39m load(spec_\u001b[39m.\u001b[39;49mentry_point)\n\u001b[1;32m    583\u001b[0m mode \u001b[39m=\u001b[39m _kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrender_mode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    584\u001b[0m apply_human_rendering \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/xclds/lib/python3.9/site-packages/gym/envs/registration.py:62\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     60\u001b[0m mod_name, attr_name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     61\u001b[0m mod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(mod_name)\n\u001b[0;32m---> 62\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(mod, attr_name)\n\u001b[1;32m     63\u001b[0m \u001b[39mreturn\u001b[39;00m fn\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gym.envs.atari' has no attribute 'AtariEnv'"
     ]
    }
   ],
   "source": [
    "# Create and wrap the environment\n",
    "env = make_atari('PongNoFrameskip-v4') # only use in no frameskip environment\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True )\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "# env.render()\n",
    "test = env.reset()\n",
    "for i in range(100):\n",
    "    test = env.step(env.action_space.sample())[0]\n",
    "\n",
    "plt.imshow(test._force()[...,0])\n",
    "\n",
    "#plt.imshow(env.render(\"rgb_array\"))\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, num_actions=5):\n",
    "        \"\"\"\n",
    "        Initialize a deep Q-learning network as described in\n",
    "        https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "        Arguments:\n",
    "            in_channels: number of channel of input.\n",
    "                i.e The number of most recent frames stacked together as describe in the paper\n",
    "            num_actions: number of action-value to output, one-to-one correspondence to action in game.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.reshape(x.size(0), -1)))\n",
    "        return self.fc5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory_Buffer(object):\n",
    "    def __init__(self, memory_size=100000):\n",
    "        self.buffer = []\n",
    "        self.memory_size = memory_size\n",
    "        self.next_idx = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        data = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) <= self.memory_size: # buffer not full\n",
    "            self.buffer.append(data)\n",
    "        else: # buffer is full\n",
    "            self.buffer[self.next_idx] = data\n",
    "        self.next_idx = (self.next_idx + 1) % self.memory_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            idx = random.randint(0, self.size() - 1)\n",
    "            data = self.buffer[idx]\n",
    "            state, action, reward, next_state, done= data\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            \n",
    "        return np.concatenate(states), actions, rewards, np.concatenate(next_states), dones\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent: \n",
    "    def __init__(self, in_channels = 1, action_space = [], USE_CUDA = False, memory_size = 10000, epsilon  = 1, lr = 1e-4):\n",
    "        self.epsilon = epsilon\n",
    "        self.action_space = action_space\n",
    "        self.memory_buffer = Memory_Buffer(memory_size)\n",
    "        self.DQN = DQN(in_channels = in_channels, num_actions = action_space.n)\n",
    "        self.DQN_target = DQN(in_channels = in_channels, num_actions = action_space.n)\n",
    "        self.DQN_target.load_state_dict(self.DQN.state_dict())\n",
    "\n",
    "\n",
    "        self.USE_CUDA = USE_CUDA\n",
    "        if USE_CUDA:\n",
    "            self.DQN = self.DQN.cuda()\n",
    "            self.DQN_target = self.DQN_target.cuda()\n",
    "        self.optimizer = optim.RMSprop(self.DQN.parameters(),lr=lr, eps=0.001, alpha=0.95)\n",
    "\n",
    "    def observe(self, lazyframe):\n",
    "        # from Lazy frame to tensor\n",
    "        state =  torch.from_numpy(lazyframe._force().transpose(2,0,1)[None]/255).float()\n",
    "        if self.USE_CUDA:\n",
    "            state = state.cuda()\n",
    "        return state\n",
    "\n",
    "    def value(self, state):\n",
    "        q_values = self.DQN(state)\n",
    "        return q_values\n",
    "    \n",
    "    def act(self, state, epsilon = None):\n",
    "        \"\"\"\n",
    "        sample actions with epsilon-greedy policy\n",
    "        recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "        \"\"\"\n",
    "        if epsilon is None: epsilon = self.epsilon\n",
    "\n",
    "        q_values = self.value(state).cpu().detach().numpy()\n",
    "        if random.random()<epsilon:\n",
    "            aciton = random.randrange(self.action_space.n)\n",
    "        else:\n",
    "            aciton = q_values.argmax(1)[0]\n",
    "        return aciton\n",
    "    \n",
    "    def compute_td_loss(self, states, actions, rewards, next_states, is_done, gamma=0.99):\n",
    "        \"\"\" Compute td loss using torch operations only. Use the formula above. \"\"\"\n",
    "        actions = torch.tensor(actions).long()    # shape: [batch_size]\n",
    "        rewards = torch.tensor(rewards, dtype =torch.float)  # shape: [batch_size]\n",
    "        is_done = torch.tensor(is_done).bool()  # shape: [batch_size]\n",
    "        \n",
    "        if self.USE_CUDA:\n",
    "            actions = actions.cuda()\n",
    "            rewards = rewards.cuda()\n",
    "            is_done = is_done.cuda()\n",
    "\n",
    "        # get q-values for all actions in current states\n",
    "        predicted_qvalues = self.DQN(states)\n",
    "\n",
    "        # select q-values for chosen actions\n",
    "        predicted_qvalues_for_actions = predicted_qvalues[\n",
    "          range(states.shape[0]), actions\n",
    "        ]\n",
    "\n",
    "        # compute q-values for all actions in next states\n",
    "        predicted_next_qvalues = self.DQN_target(next_states) # YOUR CODE\n",
    "\n",
    "        # compute V*(next_states) using predicted next q-values\n",
    "        next_state_values =  predicted_next_qvalues.max(-1)[0] # YOUR CODE\n",
    "\n",
    "        # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "        target_qvalues_for_actions = rewards + gamma *next_state_values # YOUR CODE\n",
    "\n",
    "        # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "        target_qvalues_for_actions = torch.where(\n",
    "            is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "        # mean squared error loss to minimize\n",
    "        #loss = torch.mean((predicted_qvalues_for_actions -\n",
    "        #                   target_qvalues_for_actions.detach()) ** 2)\n",
    "        loss = F.smooth_l1_loss(predicted_qvalues_for_actions, target_qvalues_for_actions.detach())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def sample_from_buffer(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            idx = random.randint(0, self.memory_buffer.size() - 1)\n",
    "            data = self.memory_buffer.buffer[idx]\n",
    "            frame, action, reward, next_frame, done= data\n",
    "            states.append(self.observe(frame))\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(self.observe(next_frame))\n",
    "            dones.append(done)\n",
    "        return torch.cat(states), actions, rewards, torch.cat(next_states), dones\n",
    "\n",
    "    def learn_from_experience(self, batch_size):\n",
    "        if self.memory_buffer.size() > batch_size:\n",
    "            states, actions, rewards, next_states, dones = self.sample_from_buffer(batch_size)\n",
    "            td_loss = self.compute_td_loss(states, actions, rewards, next_states, dones)\n",
    "            self.optimizer.zero_grad()\n",
    "            td_loss.backward()\n",
    "            for param in self.DQN.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            return(td_loss.item())\n",
    "        else:\n",
    "            return(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/step/anaconda3/envs/xclds/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/step/anaconda3/envs/xclds/lib/python3.9/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frames:     0, reward:   nan, loss: 0.000000, epsilon: 1.000000, episode:    0\n",
      "frames:  1000, reward: -20.000000, loss: 0.000000, epsilon: 0.968855, episode:    1\n",
      "frames:  2000, reward: -20.500000, loss: 0.000000, epsilon: 0.938732, episode:    2\n",
      "frames:  3000, reward: -20.333333, loss: 0.000000, epsilon: 0.909596, episode:    3\n",
      "frames:  4000, reward: -20.500000, loss: 0.000000, epsilon: 0.881415, episode:    4\n",
      "frames:  5000, reward: -20.600000, loss: 0.000000, epsilon: 0.854158, episode:    5\n",
      "frames:  6000, reward: -20.714286, loss: 0.000000, epsilon: 0.827794, episode:    7\n",
      "frames:  7000, reward: -20.750000, loss: 0.000000, epsilon: 0.802295, episode:    8\n",
      "frames:  8000, reward: -20.777778, loss: 0.000000, epsilon: 0.777632, episode:    9\n",
      "frames:  9000, reward: -20.900000, loss: 0.000000, epsilon: 0.753777, episode:   11\n",
      "frames: 10000, reward: -20.900000, loss: 0.017042, epsilon: 0.730705, episode:   12\n",
      "frames: 11000, reward: -21.000000, loss: 0.030375, epsilon: 0.708389, episode:   13\n",
      "frames: 12000, reward: -21.000000, loss: 0.000394, epsilon: 0.686804, episode:   14\n",
      "frames: 13000, reward: -21.000000, loss: 0.000342, epsilon: 0.665927, episode:   15\n",
      "frames: 14000, reward: -21.000000, loss: 0.000290, epsilon: 0.645735, episode:   17\n",
      "frames: 15000, reward: -20.900000, loss: 0.000543, epsilon: 0.626204, episode:   18\n",
      "frames: 16000, reward: -20.900000, loss: 0.015000, epsilon: 0.607314, episode:   19\n",
      "frames: 17000, reward: -20.900000, loss: 0.044504, epsilon: 0.589043, episode:   20\n",
      "frames: 18000, reward: -20.900000, loss: 0.029691, epsilon: 0.571371, episode:   21\n",
      "frames: 19000, reward: -20.900000, loss: 0.030148, epsilon: 0.554278, episode:   22\n",
      "frames: 20000, reward: -20.900000, loss: 0.030627, epsilon: 0.537746, episode:   23\n",
      "frames: 21000, reward: -20.800000, loss: 0.015105, epsilon: 0.521756, episode:   24\n",
      "frames: 22000, reward: -20.500000, loss: 0.040278, epsilon: 0.506290, episode:   25\n",
      "frames: 23000, reward: -20.500000, loss: 0.001502, epsilon: 0.491331, episode:   26\n",
      "frames: 24000, reward: -20.400000, loss: 0.015273, epsilon: 0.476863, episode:   27\n",
      "frames: 25000, reward: -20.500000, loss: 0.000127, epsilon: 0.462868, episode:   28\n",
      "frames: 26000, reward: -20.500000, loss: 0.000252, epsilon: 0.449333, episode:   29\n",
      "frames: 27000, reward: -20.400000, loss: 0.030305, epsilon: 0.436241, episode:   30\n",
      "frames: 28000, reward: -20.400000, loss: 0.015590, epsilon: 0.423579, episode:   31\n",
      "frames: 29000, reward: -20.400000, loss: 0.000572, epsilon: 0.411331, episode:   32\n",
      "frames: 30000, reward: -20.400000, loss: 0.030193, epsilon: 0.399485, episode:   33\n",
      "frames: 31000, reward: -20.400000, loss: 0.044137, epsilon: 0.388028, episode:   34\n",
      "frames: 32000, reward: -20.500000, loss: 0.000083, epsilon: 0.376946, episode:   36\n",
      "frames: 33000, reward: -20.500000, loss: 0.015672, epsilon: 0.366228, episode:   37\n",
      "frames: 34000, reward: -20.400000, loss: 0.000605, epsilon: 0.355860, episode:   38\n",
      "frames: 35000, reward: -20.300000, loss: 0.015179, epsilon: 0.345833, episode:   39\n",
      "frames: 36000, reward: -20.300000, loss: 0.000288, epsilon: 0.336135, episode:   40\n",
      "frames: 37000, reward: -20.200000, loss: 0.015605, epsilon: 0.326754, episode:   41\n",
      "frames: 38000, reward: -20.200000, loss: 0.000044, epsilon: 0.317681, episode:   42\n",
      "frames: 39000, reward: -20.100000, loss: 0.015583, epsilon: 0.308905, episode:   43\n",
      "frames: 40000, reward: -20.200000, loss: 0.016002, epsilon: 0.300417, episode:   44\n",
      "frames: 41000, reward: -20.200000, loss: 0.000575, epsilon: 0.292208, episode:   45\n",
      "frames: 42000, reward: -20.200000, loss: 0.000236, epsilon: 0.284267, episode:   46\n",
      "frames: 43000, reward: -20.400000, loss: 0.000438, epsilon: 0.276587, episode:   48\n",
      "frames: 44000, reward: -20.500000, loss: 0.000329, epsilon: 0.269159, episode:   49\n",
      "frames: 45000, reward: -20.400000, loss: 0.000210, epsilon: 0.261974, episode:   50\n",
      "frames: 46000, reward: -20.500000, loss: 0.015367, epsilon: 0.255024, episode:   51\n",
      "frames: 47000, reward: -20.400000, loss: 0.000139, epsilon: 0.248303, episode:   52\n",
      "frames: 48000, reward: -20.500000, loss: 0.000661, epsilon: 0.241802, episode:   53\n",
      "frames: 49000, reward: -20.500000, loss: 0.029628, epsilon: 0.235514, episode:   54\n",
      "frames: 50000, reward: -20.500000, loss: 0.014564, epsilon: 0.229432, episode:   55\n",
      "frames: 51000, reward: -20.500000, loss: 0.000270, epsilon: 0.223549, episode:   56\n",
      "frames: 52000, reward: -20.300000, loss: 0.000363, epsilon: 0.217860, episode:   58\n",
      "frames: 53000, reward: -20.300000, loss: 0.015559, epsilon: 0.212357, episode:   59\n",
      "frames: 54000, reward: -20.500000, loss: 0.000184, epsilon: 0.207034, episode:   60\n",
      "frames: 55000, reward: -20.500000, loss: 0.000125, epsilon: 0.201886, episode:   61\n",
      "frames: 56000, reward: -20.500000, loss: 0.000290, epsilon: 0.196906, episode:   62\n",
      "frames: 57000, reward: -20.500000, loss: 0.000192, epsilon: 0.192090, episode:   64\n",
      "frames: 58000, reward: -20.600000, loss: 0.016074, epsilon: 0.187432, episode:   65\n",
      "frames: 59000, reward: -20.700000, loss: 0.000026, epsilon: 0.182926, episode:   66\n",
      "frames: 60000, reward: -20.800000, loss: 0.000189, epsilon: 0.178569, episode:   67\n",
      "frames: 61000, reward: -20.800000, loss: 0.000302, epsilon: 0.174354, episode:   68\n",
      "frames: 62000, reward: -20.700000, loss: 0.030613, epsilon: 0.170277, episode:   69\n",
      "frames: 63000, reward: -20.600000, loss: 0.015602, epsilon: 0.166334, episode:   70\n",
      "frames: 64000, reward: -20.600000, loss: 0.000171, epsilon: 0.162520, episode:   72\n",
      "frames: 65000, reward: -20.400000, loss: 0.000252, epsilon: 0.158831, episode:   73\n",
      "frames: 66000, reward: -20.300000, loss: 0.000467, epsilon: 0.155263, episode:   74\n",
      "frames: 67000, reward: -20.100000, loss: 0.015048, epsilon: 0.151812, episode:   75\n",
      "frames: 68000, reward: -20.000000, loss: 0.000276, epsilon: 0.148474, episode:   76\n",
      "frames: 69000, reward: -20.000000, loss: 0.023191, epsilon: 0.145246, episode:   77\n",
      "frames: 70000, reward: -20.100000, loss: 0.014856, epsilon: 0.142123, episode:   78\n",
      "frames: 71000, reward: -20.100000, loss: 0.029950, epsilon: 0.139103, episode:   79\n",
      "frames: 72000, reward: -20.200000, loss: 0.014022, epsilon: 0.136182, episode:   80\n",
      "frames: 73000, reward: -20.100000, loss: 0.000525, epsilon: 0.133357, episode:   81\n",
      "frames: 74000, reward: -20.300000, loss: 0.008418, epsilon: 0.130624, episode:   83\n",
      "frames: 75000, reward: -20.100000, loss: 0.030847, epsilon: 0.127981, episode:   84\n",
      "frames: 76000, reward: -20.100000, loss: 0.033121, epsilon: 0.125424, episode:   84\n",
      "frames: 77000, reward: -20.100000, loss: 0.001913, epsilon: 0.122952, episode:   86\n",
      "frames: 78000, reward: -20.100000, loss: 0.001018, epsilon: 0.120560, episode:   87\n",
      "frames: 79000, reward: -20.000000, loss: 0.002295, epsilon: 0.118247, episode:   88\n",
      "frames: 80000, reward: -20.100000, loss: 0.001575, epsilon: 0.116009, episode:   89\n",
      "frames: 81000, reward: -20.000000, loss: 0.004076, epsilon: 0.113845, episode:   90\n",
      "frames: 82000, reward: -19.900000, loss: 0.014996, epsilon: 0.111752, episode:   91\n",
      "frames: 83000, reward: -19.900000, loss: 0.015600, epsilon: 0.109728, episode:   92\n",
      "frames: 84000, reward: -20.200000, loss: 0.001203, epsilon: 0.107770, episode:   94\n",
      "frames: 85000, reward: -20.300000, loss: 0.000820, epsilon: 0.105876, episode:   95\n",
      "frames: 86000, reward: -20.400000, loss: 0.000473, epsilon: 0.104044, episode:   96\n",
      "frames: 87000, reward: -20.400000, loss: 0.001611, epsilon: 0.102272, episode:   97\n",
      "frames: 88000, reward: -20.500000, loss: 0.000680, epsilon: 0.100558, episode:   98\n",
      "frames: 89000, reward: -20.300000, loss: 0.001343, epsilon: 0.098901, episode:   99\n",
      "frames: 90000, reward: -20.400000, loss: 0.001939, epsilon: 0.097298, episode:  100\n",
      "frames: 91000, reward: -20.400000, loss: 0.001172, epsilon: 0.095747, episode:  101\n",
      "frames: 92000, reward: -20.400000, loss: 0.000952, epsilon: 0.094247, episode:  102\n",
      "frames: 93000, reward: -20.200000, loss: 0.004888, epsilon: 0.092797, episode:  103\n",
      "frames: 94000, reward: -20.200000, loss: 0.001766, epsilon: 0.091394, episode:  104\n",
      "frames: 95000, reward: -20.200000, loss: 0.003646, epsilon: 0.090037, episode:  105\n",
      "frames: 96000, reward: -20.100000, loss: 0.000799, epsilon: 0.088724, episode:  106\n",
      "frames: 97000, reward: -20.100000, loss: 0.002903, epsilon: 0.087455, episode:  108\n",
      "frames: 98000, reward: -20.100000, loss: 0.001044, epsilon: 0.086227, episode:  108\n",
      "frames: 99000, reward: -19.600000, loss: 0.001039, epsilon: 0.085039, episode:  109\n",
      "frames: 100000, reward: -19.600000, loss: 0.001476, epsilon: 0.083890, episode:  110\n",
      "frames: 101000, reward: -19.600000, loss: 0.001120, epsilon: 0.082779, episode:  111\n",
      "frames: 102000, reward: -19.500000, loss: 0.002635, epsilon: 0.081705, episode:  112\n",
      "frames: 103000, reward: -19.300000, loss: 0.002523, epsilon: 0.080665, episode:  113\n",
      "frames: 104000, reward: -19.300000, loss: 0.003026, epsilon: 0.079660, episode:  114\n",
      "frames: 105000, reward: -19.200000, loss: 0.002327, epsilon: 0.078688, episode:  115\n",
      "frames: 106000, reward: -19.300000, loss: 0.003310, epsilon: 0.077747, episode:  116\n",
      "frames: 107000, reward: -19.300000, loss: 0.015802, epsilon: 0.076837, episode:  117\n",
      "frames: 108000, reward: -19.200000, loss: 0.000948, epsilon: 0.075958, episode:  118\n",
      "frames: 109000, reward: -19.900000, loss: 0.001759, epsilon: 0.075107, episode:  120\n",
      "frames: 110000, reward: -20.100000, loss: 0.001425, epsilon: 0.074283, episode:  121\n",
      "frames: 111000, reward: -20.300000, loss: 0.002241, epsilon: 0.073487, episode:  122\n",
      "frames: 112000, reward: -20.700000, loss: 0.002154, epsilon: 0.072717, episode:  123\n",
      "frames: 113000, reward: -20.700000, loss: 0.000681, epsilon: 0.071973, episode:  123\n",
      "frames: 114000, reward: -20.600000, loss: 0.001837, epsilon: 0.071252, episode:  124\n",
      "frames: 115000, reward: -20.700000, loss: 0.002029, epsilon: 0.070556, episode:  125\n",
      "frames: 116000, reward: -20.600000, loss: 0.001419, epsilon: 0.069882, episode:  126\n",
      "frames: 117000, reward: -20.600000, loss: 0.001882, epsilon: 0.069230, episode:  127\n",
      "frames: 118000, reward: -20.600000, loss: 0.002323, epsilon: 0.068599, episode:  128\n",
      "frames: 119000, reward: -20.500000, loss: 0.003455, epsilon: 0.067990, episode:  129\n",
      "frames: 120000, reward: -20.500000, loss: 0.000666, epsilon: 0.067400, episode:  130\n",
      "frames: 121000, reward: -20.400000, loss: 0.001151, epsilon: 0.066829, episode:  131\n",
      "frames: 122000, reward: -20.300000, loss: 0.008583, epsilon: 0.066278, episode:  132\n",
      "frames: 123000, reward: -20.300000, loss: 0.004354, epsilon: 0.065744, episode:  133\n",
      "frames: 124000, reward: -20.300000, loss: 0.001381, epsilon: 0.065228, episode:  134\n",
      "frames: 125000, reward: -20.300000, loss: 0.000551, epsilon: 0.064729, episode:  135\n",
      "frames: 126000, reward: -20.300000, loss: 0.000899, epsilon: 0.064246, episode:  135\n",
      "frames: 127000, reward: -20.400000, loss: 0.002148, epsilon: 0.063779, episode:  136\n",
      "frames: 128000, reward: -20.300000, loss: 0.000217, epsilon: 0.063327, episode:  137\n",
      "frames: 129000, reward: -20.400000, loss: 0.004198, epsilon: 0.062890, episode:  138\n",
      "frames: 130000, reward: -20.400000, loss: 0.001869, epsilon: 0.062468, episode:  139\n",
      "frames: 131000, reward: -20.400000, loss: 0.003494, epsilon: 0.062059, episode:  139\n",
      "frames: 132000, reward: -20.200000, loss: 0.003688, epsilon: 0.061663, episode:  140\n",
      "frames: 133000, reward: -20.200000, loss: 0.002046, epsilon: 0.061281, episode:  141\n",
      "frames: 134000, reward: -20.300000, loss: 0.005736, epsilon: 0.060911, episode:  142\n",
      "frames: 135000, reward: -20.300000, loss: 0.003319, epsilon: 0.060554, episode:  142\n",
      "frames: 136000, reward: -20.100000, loss: 0.002116, epsilon: 0.060208, episode:  143\n",
      "frames: 137000, reward: -20.200000, loss: 0.000773, epsilon: 0.059873, episode:  144\n",
      "frames: 138000, reward: -20.300000, loss: 0.000977, epsilon: 0.059549, episode:  145\n",
      "frames: 139000, reward: -20.300000, loss: 0.002341, epsilon: 0.059236, episode:  145\n",
      "frames: 140000, reward: -20.100000, loss: 0.002338, epsilon: 0.058933, episode:  146\n",
      "frames: 141000, reward: -20.200000, loss: 0.006482, epsilon: 0.058641, episode:  147\n",
      "frames: 142000, reward: -20.200000, loss: 0.007094, epsilon: 0.058357, episode:  147\n",
      "frames: 143000, reward: -20.100000, loss: 0.001345, epsilon: 0.058083, episode:  148\n",
      "frames: 144000, reward: -20.100000, loss: 0.002353, epsilon: 0.057818, episode:  149\n",
      "frames: 145000, reward: -20.100000, loss: 0.002570, epsilon: 0.057562, episode:  149\n",
      "frames: 146000, reward: -20.200000, loss: 0.001255, epsilon: 0.057314, episode:  150\n",
      "frames: 147000, reward: -20.100000, loss: 0.002906, epsilon: 0.057074, episode:  151\n",
      "frames: 148000, reward: -20.100000, loss: 0.008672, epsilon: 0.056842, episode:  151\n",
      "frames: 149000, reward: -19.900000, loss: 0.001528, epsilon: 0.056618, episode:  152\n",
      "frames: 150000, reward: -19.900000, loss: 0.001456, epsilon: 0.056401, episode:  152\n",
      "frames: 151000, reward: -19.900000, loss: 0.000625, epsilon: 0.056191, episode:  153\n",
      "frames: 152000, reward: -19.900000, loss: 0.001674, epsilon: 0.055988, episode:  153\n",
      "frames: 153000, reward: -19.900000, loss: 0.002953, epsilon: 0.055792, episode:  154\n",
      "frames: 154000, reward: -19.600000, loss: 0.002393, epsilon: 0.055602, episode:  155\n",
      "frames: 155000, reward: -19.600000, loss: 0.002904, epsilon: 0.055418, episode:  155\n",
      "frames: 156000, reward: -19.300000, loss: 0.003519, epsilon: 0.055241, episode:  156\n",
      "frames: 157000, reward: -19.300000, loss: 0.003118, epsilon: 0.055069, episode:  156\n",
      "frames: 158000, reward: -19.100000, loss: 0.000881, epsilon: 0.054903, episode:  157\n",
      "frames: 159000, reward: -19.100000, loss: 0.006877, epsilon: 0.054742, episode:  157\n",
      "frames: 160000, reward: -19.000000, loss: 0.003112, epsilon: 0.054587, episode:  158\n",
      "frames: 161000, reward: -19.000000, loss: 0.002997, epsilon: 0.054436, episode:  159\n",
      "frames: 162000, reward: -19.000000, loss: 0.002434, epsilon: 0.054291, episode:  159\n",
      "frames: 163000, reward: -19.100000, loss: 0.002310, epsilon: 0.054150, episode:  160\n",
      "frames: 164000, reward: -19.100000, loss: 0.002235, epsilon: 0.054014, episode:  160\n",
      "frames: 165000, reward: -19.000000, loss: 0.002510, epsilon: 0.053882, episode:  161\n",
      "frames: 166000, reward: -19.000000, loss: 0.004565, epsilon: 0.053755, episode:  161\n",
      "frames: 167000, reward: -19.100000, loss: 0.002184, epsilon: 0.053632, episode:  162\n",
      "frames: 168000, reward: -19.100000, loss: 0.001140, epsilon: 0.053513, episode:  162\n",
      "frames: 169000, reward: -19.200000, loss: 0.002191, epsilon: 0.053398, episode:  163\n",
      "frames: 170000, reward: -19.200000, loss: 0.003171, epsilon: 0.053286, episode:  163\n",
      "frames: 171000, reward: -19.200000, loss: 0.001975, epsilon: 0.053179, episode:  164\n",
      "frames: 172000, reward: -19.200000, loss: 0.003857, epsilon: 0.053074, episode:  164\n",
      "frames: 173000, reward: -19.300000, loss: 0.004777, epsilon: 0.052974, episode:  165\n",
      "frames: 174000, reward: -19.300000, loss: 0.003652, epsilon: 0.052876, episode:  165\n",
      "frames: 175000, reward: -19.700000, loss: 0.002070, epsilon: 0.052782, episode:  166\n",
      "frames: 176000, reward: -19.700000, loss: 0.002640, epsilon: 0.052691, episode:  166\n",
      "frames: 177000, reward: -19.800000, loss: 0.001602, epsilon: 0.052602, episode:  167\n",
      "frames: 178000, reward: -20.000000, loss: 0.002904, epsilon: 0.052517, episode:  168\n",
      "frames: 179000, reward: -20.000000, loss: 0.011073, epsilon: 0.052435, episode:  168\n",
      "frames: 180000, reward: -20.000000, loss: 0.002405, epsilon: 0.052355, episode:  168\n",
      "frames: 181000, reward: -19.700000, loss: 0.001143, epsilon: 0.052278, episode:  169\n",
      "frames: 182000, reward: -19.700000, loss: 0.002695, epsilon: 0.052203, episode:  169\n",
      "frames: 183000, reward: -19.300000, loss: 0.001297, epsilon: 0.052131, episode:  170\n",
      "frames: 184000, reward: -19.300000, loss: 0.001777, epsilon: 0.052061, episode:  170\n",
      "frames: 185000, reward: -19.500000, loss: 0.002798, epsilon: 0.051993, episode:  171\n",
      "frames: 186000, reward: -19.500000, loss: 0.000946, epsilon: 0.051928, episode:  171\n",
      "frames: 187000, reward: -19.500000, loss: 0.002193, epsilon: 0.051865, episode:  171\n",
      "frames: 188000, reward: -19.000000, loss: 0.002503, epsilon: 0.051804, episode:  172\n",
      "frames: 189000, reward: -19.000000, loss: 0.005014, epsilon: 0.051744, episode:  172\n",
      "frames: 190000, reward: -19.100000, loss: 0.004261, epsilon: 0.051687, episode:  173\n",
      "frames: 191000, reward: -19.100000, loss: 0.001856, epsilon: 0.051632, episode:  173\n",
      "frames: 192000, reward: -19.000000, loss: 0.003933, epsilon: 0.051578, episode:  174\n",
      "frames: 193000, reward: -19.000000, loss: 0.001304, epsilon: 0.051527, episode:  174\n",
      "frames: 194000, reward: -19.100000, loss: 0.002406, epsilon: 0.051477, episode:  175\n",
      "frames: 195000, reward: -19.100000, loss: 0.001075, epsilon: 0.051428, episode:  175\n",
      "frames: 196000, reward: -19.100000, loss: 0.002271, epsilon: 0.051381, episode:  175\n",
      "frames: 197000, reward: -19.100000, loss: 0.001726, epsilon: 0.051336, episode:  176\n",
      "frames: 198000, reward: -19.100000, loss: 0.003245, epsilon: 0.051292, episode:  176\n",
      "frames: 199000, reward: -18.800000, loss: 0.001296, epsilon: 0.051250, episode:  177\n",
      "frames: 200000, reward: -18.800000, loss: 0.000881, epsilon: 0.051209, episode:  177\n",
      "frames: 201000, reward: -18.800000, loss: 0.001187, epsilon: 0.051169, episode:  177\n",
      "frames: 202000, reward: -18.700000, loss: 0.001797, epsilon: 0.051131, episode:  178\n",
      "frames: 203000, reward: -18.700000, loss: 0.001396, epsilon: 0.051094, episode:  178\n",
      "frames: 204000, reward: -18.800000, loss: 0.001731, epsilon: 0.051058, episode:  179\n",
      "frames: 205000, reward: -18.800000, loss: 0.001783, epsilon: 0.051023, episode:  179\n",
      "frames: 206000, reward: -18.800000, loss: 0.001553, epsilon: 0.050990, episode:  179\n",
      "frames: 207000, reward: -19.100000, loss: 0.004044, epsilon: 0.050957, episode:  180\n",
      "frames: 208000, reward: -19.100000, loss: 0.002472, epsilon: 0.050926, episode:  180\n",
      "frames: 209000, reward: -19.100000, loss: 0.001440, epsilon: 0.050896, episode:  180\n",
      "frames: 210000, reward: -18.800000, loss: 0.000791, epsilon: 0.050866, episode:  181\n",
      "frames: 211000, reward: -18.800000, loss: 0.001700, epsilon: 0.050838, episode:  181\n",
      "frames: 212000, reward: -19.300000, loss: 0.017239, epsilon: 0.050810, episode:  182\n",
      "frames: 213000, reward: -19.300000, loss: 0.001958, epsilon: 0.050784, episode:  182\n",
      "frames: 214000, reward: -19.300000, loss: 0.002058, epsilon: 0.050758, episode:  182\n",
      "frames: 215000, reward: -19.200000, loss: 0.001072, epsilon: 0.050733, episode:  183\n",
      "frames: 216000, reward: -19.200000, loss: 0.008290, epsilon: 0.050709, episode:  183\n",
      "frames: 217000, reward: -19.200000, loss: 0.001405, epsilon: 0.050686, episode:  183\n",
      "frames: 218000, reward: -19.100000, loss: 0.001524, epsilon: 0.050664, episode:  184\n",
      "frames: 219000, reward: -19.100000, loss: 0.000813, epsilon: 0.050642, episode:  184\n",
      "frames: 220000, reward: -19.100000, loss: 0.001614, epsilon: 0.050621, episode:  184\n",
      "frames: 221000, reward: -18.600000, loss: 0.001369, epsilon: 0.050600, episode:  185\n",
      "frames: 222000, reward: -18.600000, loss: 0.001787, epsilon: 0.050581, episode:  185\n",
      "frames: 223000, reward: -18.300000, loss: 0.001601, epsilon: 0.050562, episode:  186\n",
      "frames: 224000, reward: -18.300000, loss: 0.001377, epsilon: 0.050543, episode:  186\n",
      "frames: 225000, reward: -18.300000, loss: 0.001326, epsilon: 0.050525, episode:  186\n",
      "frames: 226000, reward: -18.400000, loss: 0.005216, epsilon: 0.050508, episode:  187\n",
      "frames: 227000, reward: -18.400000, loss: 0.001807, epsilon: 0.050492, episode:  187\n",
      "frames: 228000, reward: -18.400000, loss: 0.003543, epsilon: 0.050475, episode:  187\n",
      "frames: 229000, reward: -17.900000, loss: 0.001532, epsilon: 0.050460, episode:  188\n",
      "frames: 230000, reward: -17.900000, loss: 0.001826, epsilon: 0.050445, episode:  188\n",
      "frames: 231000, reward: -17.900000, loss: 0.004578, epsilon: 0.050430, episode:  188\n",
      "frames: 232000, reward: -17.700000, loss: 0.001925, epsilon: 0.050416, episode:  189\n",
      "frames: 233000, reward: -17.700000, loss: 0.001564, epsilon: 0.050402, episode:  189\n",
      "frames: 234000, reward: -17.700000, loss: 0.001888, epsilon: 0.050389, episode:  189\n",
      "frames: 235000, reward: -17.500000, loss: 0.000746, epsilon: 0.050376, episode:  190\n",
      "frames: 236000, reward: -17.500000, loss: 0.001410, epsilon: 0.050364, episode:  190\n",
      "frames: 237000, reward: -17.600000, loss: 0.000741, epsilon: 0.050352, episode:  191\n",
      "frames: 238000, reward: -17.600000, loss: 0.001038, epsilon: 0.050341, episode:  191\n",
      "frames: 239000, reward: -17.600000, loss: 0.001677, epsilon: 0.050329, episode:  191\n",
      "frames: 240000, reward: -17.000000, loss: 0.000640, epsilon: 0.050319, episode:  192\n",
      "frames: 241000, reward: -17.000000, loss: 0.001894, epsilon: 0.050308, episode:  192\n",
      "frames: 242000, reward: -17.000000, loss: 0.001695, epsilon: 0.050298, episode:  192\n",
      "frames: 243000, reward: -17.000000, loss: 0.001809, epsilon: 0.050288, episode:  193\n",
      "frames: 244000, reward: -17.000000, loss: 0.000690, epsilon: 0.050279, episode:  193\n",
      "frames: 245000, reward: -17.200000, loss: 0.001103, epsilon: 0.050270, episode:  194\n",
      "frames: 246000, reward: -17.200000, loss: 0.003006, epsilon: 0.050261, episode:  194\n",
      "frames: 247000, reward: -17.200000, loss: 0.000301, epsilon: 0.050252, episode:  194\n",
      "frames: 248000, reward: -17.300000, loss: 0.000796, epsilon: 0.050244, episode:  195\n",
      "frames: 249000, reward: -17.300000, loss: 0.001691, epsilon: 0.050236, episode:  195\n",
      "frames: 250000, reward: -17.300000, loss: 0.001670, epsilon: 0.050228, episode:  195\n",
      "frames: 251000, reward: -17.500000, loss: 0.000431, epsilon: 0.050221, episode:  196\n",
      "frames: 252000, reward: -17.500000, loss: 0.000779, epsilon: 0.050214, episode:  196\n",
      "frames: 253000, reward: -17.500000, loss: 0.004114, epsilon: 0.050207, episode:  196\n",
      "frames: 254000, reward: -17.400000, loss: 0.000824, epsilon: 0.050200, episode:  197\n",
      "frames: 255000, reward: -17.400000, loss: 0.001775, epsilon: 0.050193, episode:  197\n",
      "frames: 256000, reward: -17.400000, loss: 0.000641, epsilon: 0.050187, episode:  197\n",
      "frames: 257000, reward: -17.400000, loss: 0.001730, epsilon: 0.050181, episode:  198\n",
      "frames: 258000, reward: -17.400000, loss: 0.000620, epsilon: 0.050175, episode:  198\n",
      "frames: 259000, reward: -17.600000, loss: 0.001721, epsilon: 0.050169, episode:  199\n",
      "frames: 260000, reward: -17.600000, loss: 0.002317, epsilon: 0.050164, episode:  199\n",
      "frames: 261000, reward: -17.600000, loss: 0.000849, epsilon: 0.050158, episode:  199\n",
      "frames: 262000, reward: -17.600000, loss: 0.001756, epsilon: 0.050153, episode:  200\n",
      "frames: 263000, reward: -17.600000, loss: 0.002325, epsilon: 0.050148, episode:  200\n",
      "frames: 264000, reward: -17.600000, loss: 0.002843, epsilon: 0.050143, episode:  200\n",
      "frames: 265000, reward: -17.100000, loss: 0.001537, epsilon: 0.050139, episode:  201\n",
      "frames: 266000, reward: -17.100000, loss: 0.000658, epsilon: 0.050134, episode:  201\n",
      "frames: 267000, reward: -17.100000, loss: 0.004329, epsilon: 0.050130, episode:  201\n",
      "frames: 268000, reward: -17.100000, loss: 0.001425, epsilon: 0.050125, episode:  201\n",
      "frames: 269000, reward: -17.300000, loss: 0.002183, epsilon: 0.050121, episode:  202\n",
      "frames: 270000, reward: -17.300000, loss: 0.002064, epsilon: 0.050117, episode:  202\n",
      "frames: 271000, reward: -17.300000, loss: 0.002402, epsilon: 0.050113, episode:  202\n",
      "frames: 272000, reward: -16.300000, loss: 0.008348, epsilon: 0.050110, episode:  203\n",
      "frames: 273000, reward: -16.300000, loss: 0.001554, epsilon: 0.050106, episode:  203\n",
      "frames: 274000, reward: -16.300000, loss: 0.004620, epsilon: 0.050103, episode:  203\n",
      "frames: 275000, reward: -16.000000, loss: 0.002835, epsilon: 0.050099, episode:  204\n",
      "frames: 276000, reward: -16.000000, loss: 0.002739, epsilon: 0.050096, episode:  204\n",
      "frames: 277000, reward: -16.000000, loss: 0.001981, epsilon: 0.050093, episode:  204\n",
      "frames: 278000, reward: -15.900000, loss: 0.001364, epsilon: 0.050090, episode:  205\n",
      "frames: 279000, reward: -15.900000, loss: 0.001116, epsilon: 0.050087, episode:  205\n",
      "frames: 280000, reward: -15.900000, loss: 0.004499, epsilon: 0.050084, episode:  205\n",
      "frames: 281000, reward: -15.900000, loss: 0.001678, epsilon: 0.050081, episode:  206\n",
      "frames: 282000, reward: -15.900000, loss: 0.002651, epsilon: 0.050079, episode:  206\n",
      "frames: 283000, reward: -15.900000, loss: 0.000867, epsilon: 0.050076, episode:  206\n",
      "frames: 284000, reward: -15.900000, loss: 0.001755, epsilon: 0.050074, episode:  206\n",
      "frames: 285000, reward: -15.700000, loss: 0.000845, epsilon: 0.050071, episode:  207\n",
      "frames: 286000, reward: -15.700000, loss: 0.001007, epsilon: 0.050069, episode:  207\n",
      "frames: 287000, reward: -15.700000, loss: 0.000459, epsilon: 0.050067, episode:  207\n",
      "frames: 288000, reward: -15.900000, loss: 0.008475, epsilon: 0.050064, episode:  208\n",
      "frames: 289000, reward: -15.900000, loss: 0.003671, epsilon: 0.050062, episode:  208\n",
      "frames: 290000, reward: -15.900000, loss: 0.001220, epsilon: 0.050060, episode:  208\n",
      "frames: 291000, reward: -15.900000, loss: 0.001002, epsilon: 0.050058, episode:  209\n",
      "frames: 292000, reward: -15.900000, loss: 0.000636, epsilon: 0.050056, episode:  209\n",
      "frames: 293000, reward: -16.100000, loss: 0.001300, epsilon: 0.050054, episode:  210\n",
      "frames: 294000, reward: -16.100000, loss: 0.003574, epsilon: 0.050053, episode:  210\n",
      "frames: 295000, reward: -16.100000, loss: 0.001160, epsilon: 0.050051, episode:  210\n",
      "frames: 296000, reward: -16.300000, loss: 0.000607, epsilon: 0.050049, episode:  211\n",
      "frames: 297000, reward: -16.300000, loss: 0.001450, epsilon: 0.050048, episode:  211\n",
      "frames: 298000, reward: -16.400000, loss: 0.001151, epsilon: 0.050046, episode:  212\n",
      "frames: 299000, reward: -16.400000, loss: 0.002769, epsilon: 0.050045, episode:  212\n",
      "frames: 300000, reward: -16.400000, loss: 0.003412, epsilon: 0.050043, episode:  212\n",
      "frames: 301000, reward: -17.100000, loss: 0.001185, epsilon: 0.050042, episode:  213\n",
      "frames: 302000, reward: -17.100000, loss: 0.000700, epsilon: 0.050040, episode:  213\n",
      "frames: 303000, reward: -17.100000, loss: 0.000773, epsilon: 0.050039, episode:  214\n",
      "frames: 304000, reward: -17.100000, loss: 0.003182, epsilon: 0.050038, episode:  214\n",
      "frames: 305000, reward: -17.100000, loss: 0.001099, epsilon: 0.050037, episode:  214\n",
      "frames: 306000, reward: -16.800000, loss: 0.003038, epsilon: 0.050035, episode:  215\n",
      "frames: 307000, reward: -16.800000, loss: 0.001562, epsilon: 0.050034, episode:  215\n",
      "frames: 308000, reward: -16.800000, loss: 0.001163, epsilon: 0.050033, episode:  215\n",
      "frames: 309000, reward: -16.800000, loss: 0.001094, epsilon: 0.050032, episode:  215\n",
      "frames: 310000, reward: -16.300000, loss: 0.000901, epsilon: 0.050031, episode:  216\n",
      "frames: 311000, reward: -16.300000, loss: 0.000856, epsilon: 0.050030, episode:  216\n",
      "frames: 312000, reward: -16.300000, loss: 0.001849, epsilon: 0.050029, episode:  216\n",
      "frames: 313000, reward: -16.000000, loss: 0.000913, epsilon: 0.050028, episode:  217\n",
      "frames: 314000, reward: -16.000000, loss: 0.000602, epsilon: 0.050027, episode:  217\n",
      "frames: 315000, reward: -16.000000, loss: 0.001118, epsilon: 0.050026, episode:  217\n",
      "frames: 316000, reward: -16.000000, loss: 0.002494, epsilon: 0.050025, episode:  217\n",
      "frames: 317000, reward: -15.800000, loss: 0.000654, epsilon: 0.050024, episode:  218\n",
      "frames: 318000, reward: -15.800000, loss: 0.001285, epsilon: 0.050024, episode:  218\n",
      "frames: 319000, reward: -15.800000, loss: 0.000643, epsilon: 0.050023, episode:  218\n",
      "frames: 320000, reward: -15.800000, loss: 0.003795, epsilon: 0.050022, episode:  219\n",
      "frames: 321000, reward: -15.800000, loss: 0.001418, epsilon: 0.050021, episode:  219\n",
      "frames: 322000, reward: -15.800000, loss: 0.003938, epsilon: 0.050021, episode:  219\n",
      "frames: 323000, reward: -15.800000, loss: 0.000497, epsilon: 0.050020, episode:  219\n",
      "frames: 324000, reward: -15.100000, loss: 0.002069, epsilon: 0.050019, episode:  220\n",
      "frames: 325000, reward: -15.100000, loss: 0.003375, epsilon: 0.050019, episode:  220\n",
      "frames: 326000, reward: -15.100000, loss: 0.002473, epsilon: 0.050018, episode:  220\n",
      "frames: 327000, reward: -15.300000, loss: 0.002771, epsilon: 0.050018, episode:  221\n",
      "frames: 328000, reward: -15.300000, loss: 0.001041, epsilon: 0.050017, episode:  221\n",
      "frames: 329000, reward: -15.300000, loss: 0.000802, epsilon: 0.050016, episode:  221\n",
      "frames: 330000, reward: -15.300000, loss: 0.004775, epsilon: 0.050016, episode:  221\n",
      "frames: 331000, reward: -14.000000, loss: 0.001094, epsilon: 0.050015, episode:  222\n",
      "frames: 332000, reward: -14.000000, loss: 0.003053, epsilon: 0.050015, episode:  222\n",
      "frames: 333000, reward: -14.000000, loss: 0.002202, epsilon: 0.050014, episode:  222\n",
      "frames: 334000, reward: -14.000000, loss: 0.001725, epsilon: 0.050014, episode:  222\n",
      "frames: 335000, reward: -13.200000, loss: 0.006670, epsilon: 0.050013, episode:  223\n",
      "frames: 336000, reward: -13.200000, loss: 0.006539, epsilon: 0.050013, episode:  223\n",
      "frames: 337000, reward: -13.200000, loss: 0.003452, epsilon: 0.050013, episode:  223\n",
      "frames: 338000, reward: -12.700000, loss: 0.000565, epsilon: 0.050012, episode:  224\n",
      "frames: 339000, reward: -12.700000, loss: 0.001872, epsilon: 0.050012, episode:  224\n",
      "frames: 340000, reward: -13.200000, loss: 0.002177, epsilon: 0.050011, episode:  225\n",
      "frames: 341000, reward: -13.200000, loss: 0.007305, epsilon: 0.050011, episode:  225\n",
      "frames: 342000, reward: -13.200000, loss: 0.001541, epsilon: 0.050011, episode:  225\n",
      "frames: 343000, reward: -13.400000, loss: 0.003996, epsilon: 0.050010, episode:  226\n",
      "frames: 344000, reward: -13.400000, loss: 0.002833, epsilon: 0.050010, episode:  226\n",
      "frames: 345000, reward: -13.400000, loss: 0.000803, epsilon: 0.050010, episode:  226\n",
      "frames: 346000, reward: -13.900000, loss: 0.000899, epsilon: 0.050009, episode:  227\n",
      "frames: 347000, reward: -13.900000, loss: 0.002169, epsilon: 0.050009, episode:  227\n",
      "frames: 348000, reward: -14.200000, loss: 0.001159, epsilon: 0.050009, episode:  228\n",
      "frames: 349000, reward: -14.200000, loss: 0.002718, epsilon: 0.050008, episode:  228\n",
      "frames: 350000, reward: -14.200000, loss: 0.002884, epsilon: 0.050008, episode:  228\n",
      "frames: 351000, reward: -14.300000, loss: 0.001623, epsilon: 0.050008, episode:  229\n",
      "frames: 352000, reward: -14.300000, loss: 0.004227, epsilon: 0.050008, episode:  229\n",
      "frames: 353000, reward: -14.300000, loss: 0.001940, epsilon: 0.050007, episode:  229\n",
      "frames: 354000, reward: -14.700000, loss: 0.003171, epsilon: 0.050007, episode:  230\n",
      "frames: 355000, reward: -14.700000, loss: 0.000621, epsilon: 0.050007, episode:  230\n",
      "frames: 356000, reward: -14.700000, loss: 0.003705, epsilon: 0.050007, episode:  230\n",
      "frames: 357000, reward: -14.700000, loss: 0.000858, epsilon: 0.050006, episode:  230\n",
      "frames: 358000, reward: -13.900000, loss: 0.000436, epsilon: 0.050006, episode:  231\n",
      "frames: 359000, reward: -13.900000, loss: 0.003300, epsilon: 0.050006, episode:  231\n",
      "frames: 360000, reward: -15.000000, loss: 0.000926, epsilon: 0.050006, episode:  232\n",
      "frames: 361000, reward: -15.000000, loss: 0.001645, epsilon: 0.050006, episode:  232\n",
      "frames: 362000, reward: -15.000000, loss: 0.003199, epsilon: 0.050005, episode:  232\n",
      "frames: 363000, reward: -15.000000, loss: 0.001367, epsilon: 0.050005, episode:  232\n",
      "frames: 364000, reward: -14.500000, loss: 0.001123, epsilon: 0.050005, episode:  233\n",
      "frames: 365000, reward: -14.500000, loss: 0.002296, epsilon: 0.050005, episode:  233\n",
      "frames: 366000, reward: -14.500000, loss: 0.002290, epsilon: 0.050005, episode:  233\n",
      "frames: 367000, reward: -14.500000, loss: 0.000744, epsilon: 0.050005, episode:  234\n",
      "frames: 368000, reward: -14.500000, loss: 0.002857, epsilon: 0.050004, episode:  234\n",
      "frames: 369000, reward: -14.500000, loss: 0.000873, epsilon: 0.050004, episode:  234\n",
      "frames: 370000, reward: -14.600000, loss: 0.003087, epsilon: 0.050004, episode:  235\n",
      "frames: 371000, reward: -14.600000, loss: 0.001435, epsilon: 0.050004, episode:  235\n",
      "frames: 372000, reward: -14.600000, loss: 0.002759, epsilon: 0.050004, episode:  235\n",
      "frames: 373000, reward: -14.600000, loss: 0.000957, epsilon: 0.050004, episode:  236\n",
      "frames: 374000, reward: -14.600000, loss: 0.002552, epsilon: 0.050004, episode:  236\n",
      "frames: 375000, reward: -14.600000, loss: 0.001113, epsilon: 0.050004, episode:  236\n",
      "frames: 376000, reward: -14.600000, loss: 0.001596, epsilon: 0.050003, episode:  236\n",
      "frames: 377000, reward: -13.700000, loss: 0.001528, epsilon: 0.050003, episode:  237\n",
      "frames: 378000, reward: -13.700000, loss: 0.001052, epsilon: 0.050003, episode:  237\n",
      "frames: 379000, reward: -13.700000, loss: 0.001569, epsilon: 0.050003, episode:  237\n",
      "frames: 380000, reward: -13.200000, loss: 0.004104, epsilon: 0.050003, episode:  238\n",
      "frames: 381000, reward: -13.200000, loss: 0.004107, epsilon: 0.050003, episode:  238\n",
      "frames: 382000, reward: -13.200000, loss: 0.002610, epsilon: 0.050003, episode:  238\n",
      "frames: 383000, reward: -13.200000, loss: 0.001302, epsilon: 0.050003, episode:  238\n",
      "frames: 384000, reward: -12.100000, loss: 0.002072, epsilon: 0.050003, episode:  239\n",
      "frames: 385000, reward: -12.100000, loss: 0.001335, epsilon: 0.050003, episode:  239\n",
      "frames: 386000, reward: -12.100000, loss: 0.001130, epsilon: 0.050002, episode:  239\n",
      "frames: 387000, reward: -11.900000, loss: 0.001159, epsilon: 0.050002, episode:  240\n",
      "frames: 388000, reward: -11.900000, loss: 0.001068, epsilon: 0.050002, episode:  240\n",
      "frames: 389000, reward: -11.900000, loss: 0.001325, epsilon: 0.050002, episode:  240\n",
      "frames: 390000, reward: -12.300000, loss: 0.001107, epsilon: 0.050002, episode:  241\n",
      "frames: 391000, reward: -12.300000, loss: 0.002354, epsilon: 0.050002, episode:  241\n",
      "frames: 392000, reward: -12.400000, loss: 0.001694, epsilon: 0.050002, episode:  242\n",
      "frames: 393000, reward: -12.400000, loss: 0.001460, epsilon: 0.050002, episode:  242\n",
      "frames: 394000, reward: -12.400000, loss: 0.001225, epsilon: 0.050002, episode:  242\n",
      "frames: 395000, reward: -13.400000, loss: 0.001564, epsilon: 0.050002, episode:  243\n",
      "frames: 396000, reward: -13.400000, loss: 0.001069, epsilon: 0.050002, episode:  243\n",
      "frames: 397000, reward: -13.400000, loss: 0.001957, epsilon: 0.050002, episode:  243\n",
      "frames: 398000, reward: -13.500000, loss: 0.004047, epsilon: 0.050002, episode:  244\n",
      "frames: 399000, reward: -13.500000, loss: 0.001307, epsilon: 0.050002, episode:  244\n",
      "frames: 400000, reward: -13.500000, loss: 0.003164, epsilon: 0.050002, episode:  244\n",
      "frames: 401000, reward: -13.200000, loss: 0.003530, epsilon: 0.050001, episode:  245\n",
      "frames: 402000, reward: -13.200000, loss: 0.001295, epsilon: 0.050001, episode:  245\n",
      "frames: 403000, reward: -13.200000, loss: 0.001307, epsilon: 0.050001, episode:  246\n",
      "frames: 404000, reward: -13.200000, loss: 0.003550, epsilon: 0.050001, episode:  246\n",
      "frames: 405000, reward: -13.200000, loss: 0.002495, epsilon: 0.050001, episode:  246\n",
      "frames: 406000, reward: -13.200000, loss: 0.002986, epsilon: 0.050001, episode:  246\n",
      "frames: 407000, reward: -13.900000, loss: 0.000766, epsilon: 0.050001, episode:  247\n",
      "frames: 408000, reward: -13.900000, loss: 0.000621, epsilon: 0.050001, episode:  247\n",
      "frames: 409000, reward: -13.900000, loss: 0.001704, epsilon: 0.050001, episode:  247\n",
      "frames: 410000, reward: -13.900000, loss: 0.001099, epsilon: 0.050001, episode:  248\n",
      "frames: 411000, reward: -13.900000, loss: 0.005707, epsilon: 0.050001, episode:  248\n",
      "frames: 412000, reward: -13.900000, loss: 0.000690, epsilon: 0.050001, episode:  248\n",
      "frames: 413000, reward: -13.900000, loss: 0.001931, epsilon: 0.050001, episode:  248\n",
      "frames: 414000, reward: -13.700000, loss: 0.001852, epsilon: 0.050001, episode:  249\n",
      "frames: 415000, reward: -13.700000, loss: 0.002567, epsilon: 0.050001, episode:  249\n",
      "frames: 416000, reward: -13.700000, loss: 0.001843, epsilon: 0.050001, episode:  249\n",
      "frames: 417000, reward: -12.800000, loss: 0.000693, epsilon: 0.050001, episode:  250\n",
      "frames: 418000, reward: -12.800000, loss: 0.000998, epsilon: 0.050001, episode:  250\n",
      "frames: 419000, reward: -12.800000, loss: 0.001457, epsilon: 0.050001, episode:  250\n",
      "frames: 420000, reward: -12.800000, loss: 0.000628, epsilon: 0.050001, episode:  250\n",
      "frames: 421000, reward: -12.600000, loss: 0.001098, epsilon: 0.050001, episode:  251\n",
      "frames: 422000, reward: -12.600000, loss: 0.002790, epsilon: 0.050001, episode:  251\n",
      "frames: 423000, reward: -12.600000, loss: 0.002227, epsilon: 0.050001, episode:  251\n",
      "frames: 424000, reward: -12.200000, loss: 0.001105, epsilon: 0.050001, episode:  252\n",
      "frames: 425000, reward: -12.200000, loss: 0.001957, epsilon: 0.050001, episode:  252\n",
      "frames: 426000, reward: -12.200000, loss: 0.001218, epsilon: 0.050001, episode:  252\n",
      "frames: 427000, reward: -12.200000, loss: 0.001863, epsilon: 0.050001, episode:  252\n",
      "frames: 428000, reward: -11.800000, loss: 0.001747, epsilon: 0.050001, episode:  253\n",
      "frames: 429000, reward: -11.900000, loss: 0.011409, epsilon: 0.050001, episode:  254\n",
      "frames: 430000, reward: -11.900000, loss: 0.001025, epsilon: 0.050001, episode:  254\n",
      "frames: 431000, reward: -11.900000, loss: 0.001737, epsilon: 0.050001, episode:  254\n",
      "frames: 432000, reward: -11.900000, loss: 0.002151, epsilon: 0.050001, episode:  255\n",
      "frames: 433000, reward: -11.900000, loss: 0.002827, epsilon: 0.050001, episode:  255\n",
      "frames: 434000, reward: -11.500000, loss: 0.001763, epsilon: 0.050000, episode:  256\n",
      "frames: 435000, reward: -11.500000, loss: 0.009682, epsilon: 0.050000, episode:  256\n",
      "frames: 436000, reward: -11.500000, loss: 0.001408, epsilon: 0.050000, episode:  256\n",
      "frames: 437000, reward: -11.500000, loss: 0.005605, epsilon: 0.050000, episode:  256\n",
      "frames: 438000, reward: -11.100000, loss: 0.002513, epsilon: 0.050000, episode:  257\n",
      "frames: 439000, reward: -11.100000, loss: 0.001638, epsilon: 0.050000, episode:  257\n",
      "frames: 440000, reward: -11.100000, loss: 0.005660, epsilon: 0.050000, episode:  257\n",
      "frames: 441000, reward: -11.200000, loss: 0.001094, epsilon: 0.050000, episode:  258\n",
      "frames: 442000, reward: -11.200000, loss: 0.004031, epsilon: 0.050000, episode:  258\n",
      "frames: 443000, reward: -11.200000, loss: 0.001046, epsilon: 0.050000, episode:  258\n",
      "frames: 444000, reward: -11.200000, loss: 0.001609, epsilon: 0.050000, episode:  258\n",
      "frames: 445000, reward: -11.000000, loss: 0.001491, epsilon: 0.050000, episode:  259\n",
      "frames: 446000, reward: -11.000000, loss: 0.001900, epsilon: 0.050000, episode:  259\n",
      "frames: 447000, reward: -11.000000, loss: 0.002474, epsilon: 0.050000, episode:  259\n",
      "frames: 448000, reward: -11.800000, loss: 0.004248, epsilon: 0.050000, episode:  260\n",
      "frames: 449000, reward: -11.800000, loss: 0.001777, epsilon: 0.050000, episode:  260\n",
      "frames: 450000, reward: -11.800000, loss: 0.007586, epsilon: 0.050000, episode:  260\n",
      "frames: 451000, reward: -12.300000, loss: 0.005134, epsilon: 0.050000, episode:  261\n",
      "frames: 452000, reward: -12.300000, loss: 0.003578, epsilon: 0.050000, episode:  261\n",
      "frames: 453000, reward: -12.400000, loss: 0.002420, epsilon: 0.050000, episode:  262\n",
      "frames: 454000, reward: -12.400000, loss: 0.002999, epsilon: 0.050000, episode:  262\n",
      "frames: 455000, reward: -12.400000, loss: 0.001201, epsilon: 0.050000, episode:  262\n",
      "frames: 456000, reward: -12.400000, loss: 0.005062, epsilon: 0.050000, episode:  262\n",
      "frames: 457000, reward: -12.200000, loss: 0.002365, epsilon: 0.050000, episode:  263\n",
      "frames: 458000, reward: -12.200000, loss: 0.002904, epsilon: 0.050000, episode:  263\n",
      "frames: 459000, reward: -12.400000, loss: 0.002606, epsilon: 0.050000, episode:  264\n",
      "frames: 460000, reward: -12.400000, loss: 0.000922, epsilon: 0.050000, episode:  264\n",
      "frames: 461000, reward: -12.400000, loss: 0.010614, epsilon: 0.050000, episode:  264\n",
      "frames: 462000, reward: -12.600000, loss: 0.000988, epsilon: 0.050000, episode:  265\n",
      "frames: 463000, reward: -12.600000, loss: 0.001369, epsilon: 0.050000, episode:  265\n",
      "frames: 464000, reward: -12.600000, loss: 0.001629, epsilon: 0.050000, episode:  265\n",
      "frames: 465000, reward: -12.900000, loss: 0.001980, epsilon: 0.050000, episode:  266\n",
      "frames: 466000, reward: -12.900000, loss: 0.001429, epsilon: 0.050000, episode:  266\n",
      "frames: 467000, reward: -12.900000, loss: 0.001180, epsilon: 0.050000, episode:  266\n",
      "frames: 468000, reward: -12.800000, loss: 0.002847, epsilon: 0.050000, episode:  267\n",
      "frames: 469000, reward: -12.800000, loss: 0.004660, epsilon: 0.050000, episode:  267\n",
      "frames: 470000, reward: -12.800000, loss: 0.005457, epsilon: 0.050000, episode:  267\n",
      "frames: 471000, reward: -12.700000, loss: 0.001241, epsilon: 0.050000, episode:  268\n",
      "frames: 472000, reward: -12.700000, loss: 0.001542, epsilon: 0.050000, episode:  268\n",
      "frames: 473000, reward: -12.700000, loss: 0.001444, epsilon: 0.050000, episode:  268\n",
      "frames: 474000, reward: -12.700000, loss: 0.001676, epsilon: 0.050000, episode:  268\n",
      "frames: 475000, reward: -12.700000, loss: 0.000399, epsilon: 0.050000, episode:  269\n",
      "frames: 476000, reward: -12.700000, loss: 0.002363, epsilon: 0.050000, episode:  269\n",
      "frames: 477000, reward: -12.700000, loss: 0.000517, epsilon: 0.050000, episode:  269\n",
      "frames: 478000, reward: -12.700000, loss: 0.001336, epsilon: 0.050000, episode:  269\n",
      "frames: 479000, reward: -10.700000, loss: 0.001883, epsilon: 0.050000, episode:  270\n",
      "frames: 480000, reward: -10.700000, loss: 0.002313, epsilon: 0.050000, episode:  270\n",
      "frames: 481000, reward: -10.200000, loss: 0.001174, epsilon: 0.050000, episode:  271\n",
      "frames: 482000, reward: -10.200000, loss: 0.001934, epsilon: 0.050000, episode:  271\n",
      "frames: 483000, reward: -10.200000, loss: 0.003043, epsilon: 0.050000, episode:  271\n",
      "frames: 484000, reward: -10.200000, loss: 0.001856, epsilon: 0.050000, episode:  271\n",
      "frames: 485000, reward: -8.600000, loss: 0.001466, epsilon: 0.050000, episode:  272\n",
      "frames: 486000, reward: -8.600000, loss: 0.001207, epsilon: 0.050000, episode:  272\n",
      "frames: 487000, reward: -8.600000, loss: 0.002144, epsilon: 0.050000, episode:  272\n",
      "frames: 488000, reward: -8.700000, loss: 0.003526, epsilon: 0.050000, episode:  273\n",
      "frames: 489000, reward: -8.700000, loss: 0.002443, epsilon: 0.050000, episode:  273\n",
      "frames: 490000, reward: -8.700000, loss: 0.005485, epsilon: 0.050000, episode:  273\n",
      "frames: 491000, reward: -8.700000, loss: 0.004290, epsilon: 0.050000, episode:  273\n",
      "frames: 492000, reward: -6.600000, loss: 0.001750, epsilon: 0.050000, episode:  274\n",
      "frames: 493000, reward: -6.600000, loss: 0.003485, epsilon: 0.050000, episode:  274\n",
      "frames: 494000, reward: -6.600000, loss: 0.002246, epsilon: 0.050000, episode:  274\n",
      "frames: 495000, reward: -3.400000, loss: 0.002677, epsilon: 0.050000, episode:  275\n",
      "frames: 496000, reward: -3.400000, loss: 0.001079, epsilon: 0.050000, episode:  275\n",
      "frames: 497000, reward: -3.400000, loss: 0.001387, epsilon: 0.050000, episode:  275\n",
      "frames: 498000, reward: -0.500000, loss: 0.006067, epsilon: 0.050000, episode:  276\n",
      "frames: 499000, reward: -0.500000, loss: 0.001404, epsilon: 0.050000, episode:  276\n",
      "frames: 500000, reward: -0.500000, loss: 0.001174, epsilon: 0.050000, episode:  276\n",
      "frames: 501000, reward: -0.500000, loss: 0.003707, epsilon: 0.050000, episode:  276\n",
      "frames: 502000, reward: -0.200000, loss: 0.002197, epsilon: 0.050000, episode:  277\n",
      "frames: 503000, reward: -0.200000, loss: 0.001284, epsilon: 0.050000, episode:  277\n",
      "frames: 504000, reward: 2.100000, loss: 0.001459, epsilon: 0.050000, episode:  278\n",
      "frames: 505000, reward: 2.100000, loss: 0.006348, epsilon: 0.050000, episode:  278\n",
      "frames: 506000, reward: 2.100000, loss: 0.000946, epsilon: 0.050000, episode:  278\n",
      "frames: 507000, reward: 3.400000, loss: 0.003086, epsilon: 0.050000, episode:  279\n",
      "frames: 508000, reward: 3.400000, loss: 0.011632, epsilon: 0.050000, episode:  279\n",
      "frames: 509000, reward: 3.400000, loss: 0.007809, epsilon: 0.050000, episode:  279\n",
      "frames: 510000, reward: 1.900000, loss: 0.001035, epsilon: 0.050000, episode:  280\n",
      "frames: 511000, reward: 1.900000, loss: 0.000602, epsilon: 0.050000, episode:  280\n",
      "frames: 512000, reward: 1.900000, loss: 0.002203, epsilon: 0.050000, episode:  280\n",
      "frames: 513000, reward: 4.300000, loss: 0.000897, epsilon: 0.050000, episode:  281\n",
      "frames: 514000, reward: 4.300000, loss: 0.001770, epsilon: 0.050000, episode:  281\n",
      "frames: 515000, reward: 4.300000, loss: 0.005799, epsilon: 0.050000, episode:  281\n",
      "frames: 516000, reward: 4.700000, loss: 0.001711, epsilon: 0.050000, episode:  282\n",
      "frames: 517000, reward: 4.700000, loss: 0.002534, epsilon: 0.050000, episode:  282\n",
      "frames: 518000, reward: 4.700000, loss: 0.006921, epsilon: 0.050000, episode:  282\n",
      "frames: 519000, reward: 4.100000, loss: 0.002343, epsilon: 0.050000, episode:  283\n",
      "frames: 520000, reward: 4.100000, loss: 0.001614, epsilon: 0.050000, episode:  283\n",
      "frames: 521000, reward: 4.100000, loss: 0.000990, epsilon: 0.050000, episode:  283\n",
      "frames: 522000, reward: 4.100000, loss: 0.016350, epsilon: 0.050000, episode:  283\n",
      "frames: 523000, reward: 3.200000, loss: 0.002213, epsilon: 0.050000, episode:  284\n",
      "frames: 524000, reward: 3.200000, loss: 0.002408, epsilon: 0.050000, episode:  284\n",
      "frames: 525000, reward: 3.200000, loss: 0.003662, epsilon: 0.050000, episode:  284\n",
      "frames: 526000, reward: 3.200000, loss: 0.001418, epsilon: 0.050000, episode:  284\n",
      "frames: 527000, reward: 2.000000, loss: 0.019043, epsilon: 0.050000, episode:  285\n",
      "frames: 528000, reward: 2.000000, loss: 0.000742, epsilon: 0.050000, episode:  285\n",
      "frames: 529000, reward: 2.000000, loss: 0.002078, epsilon: 0.050000, episode:  285\n",
      "frames: 530000, reward: 2.000000, loss: 0.001191, epsilon: 0.050000, episode:  285\n",
      "frames: 531000, reward: 1.400000, loss: 0.001426, epsilon: 0.050000, episode:  286\n",
      "frames: 532000, reward: 1.400000, loss: 0.001065, epsilon: 0.050000, episode:  286\n",
      "frames: 533000, reward: 1.400000, loss: 0.004694, epsilon: 0.050000, episode:  286\n",
      "frames: 534000, reward: 0.500000, loss: 0.000793, epsilon: 0.050000, episode:  287\n",
      "frames: 535000, reward: 0.500000, loss: 0.001458, epsilon: 0.050000, episode:  287\n",
      "frames: 536000, reward: 1.100000, loss: 0.002337, epsilon: 0.050000, episode:  288\n",
      "frames: 537000, reward: 1.100000, loss: 0.001129, epsilon: 0.050000, episode:  288\n",
      "frames: 538000, reward: 1.100000, loss: 0.001268, epsilon: 0.050000, episode:  288\n",
      "frames: 539000, reward: 1.300000, loss: 0.006390, epsilon: 0.050000, episode:  289\n",
      "frames: 540000, reward: 1.300000, loss: 0.001379, epsilon: 0.050000, episode:  289\n",
      "frames: 541000, reward: 1.300000, loss: 0.000981, epsilon: 0.050000, episode:  289\n",
      "frames: 542000, reward: 0.900000, loss: 0.000987, epsilon: 0.050000, episode:  290\n",
      "frames: 543000, reward: 0.900000, loss: 0.000913, epsilon: 0.050000, episode:  290\n",
      "frames: 544000, reward: 0.900000, loss: 0.001783, epsilon: 0.050000, episode:  290\n",
      "frames: 545000, reward: -1.900000, loss: 0.003195, epsilon: 0.050000, episode:  291\n",
      "frames: 546000, reward: -1.900000, loss: 0.001459, epsilon: 0.050000, episode:  291\n",
      "frames: 547000, reward: -1.100000, loss: 0.004545, epsilon: 0.050000, episode:  292\n",
      "frames: 548000, reward: -1.100000, loss: 0.000528, epsilon: 0.050000, episode:  292\n",
      "frames: 549000, reward: -1.100000, loss: 0.000632, epsilon: 0.050000, episode:  292\n",
      "frames: 550000, reward: -1.100000, loss: 0.001763, epsilon: 0.050000, episode:  292\n",
      "frames: 551000, reward: 0.500000, loss: 0.001202, epsilon: 0.050000, episode:  293\n",
      "frames: 552000, reward: 0.500000, loss: 0.000998, epsilon: 0.050000, episode:  293\n",
      "frames: 553000, reward: 0.500000, loss: 0.003302, epsilon: 0.050000, episode:  293\n",
      "frames: 554000, reward: 2.400000, loss: 0.002544, epsilon: 0.050000, episode:  294\n",
      "frames: 555000, reward: 2.400000, loss: 0.002627, epsilon: 0.050000, episode:  294\n",
      "frames: 556000, reward: 2.400000, loss: 0.000736, epsilon: 0.050000, episode:  294\n",
      "frames: 557000, reward: 1.100000, loss: 0.000563, epsilon: 0.050000, episode:  295\n",
      "frames: 558000, reward: 1.100000, loss: 0.002975, epsilon: 0.050000, episode:  295\n",
      "frames: 559000, reward: 1.100000, loss: 0.000783, epsilon: 0.050000, episode:  295\n",
      "frames: 560000, reward: 1.100000, loss: 0.001547, epsilon: 0.050000, episode:  295\n",
      "frames: 561000, reward: -0.100000, loss: 0.001505, epsilon: 0.050000, episode:  296\n",
      "frames: 562000, reward: -0.100000, loss: 0.002373, epsilon: 0.050000, episode:  296\n",
      "frames: 563000, reward: -0.100000, loss: 0.001303, epsilon: 0.050000, episode:  296\n",
      "frames: 564000, reward: -0.100000, loss: 0.002452, epsilon: 0.050000, episode:  296\n",
      "frames: 565000, reward: 1.100000, loss: 0.001929, epsilon: 0.050000, episode:  297\n",
      "frames: 566000, reward: 1.100000, loss: 0.002230, epsilon: 0.050000, episode:  297\n",
      "frames: 567000, reward: 1.100000, loss: 0.002337, epsilon: 0.050000, episode:  297\n",
      "frames: 568000, reward: -1.600000, loss: 0.002319, epsilon: 0.050000, episode:  298\n",
      "frames: 569000, reward: -1.600000, loss: 0.003116, epsilon: 0.050000, episode:  298\n",
      "frames: 570000, reward: -1.600000, loss: 0.002989, epsilon: 0.050000, episode:  298\n",
      "frames: 571000, reward: -1.900000, loss: 0.001470, epsilon: 0.050000, episode:  299\n",
      "frames: 572000, reward: -1.900000, loss: 0.000933, epsilon: 0.050000, episode:  299\n",
      "frames: 573000, reward: -1.900000, loss: 0.001566, epsilon: 0.050000, episode:  299\n",
      "frames: 574000, reward: -1.900000, loss: 0.003215, epsilon: 0.050000, episode:  299\n",
      "frames: 575000, reward: -0.800000, loss: 0.002297, epsilon: 0.050000, episode:  300\n",
      "frames: 576000, reward: -0.800000, loss: 0.002424, epsilon: 0.050000, episode:  300\n",
      "frames: 577000, reward: -0.800000, loss: 0.000985, epsilon: 0.050000, episode:  300\n",
      "frames: 578000, reward: 2.500000, loss: 0.001587, epsilon: 0.050000, episode:  301\n",
      "frames: 579000, reward: 2.500000, loss: 0.002061, epsilon: 0.050000, episode:  301\n",
      "frames: 580000, reward: 2.500000, loss: 0.001263, epsilon: 0.050000, episode:  301\n",
      "frames: 581000, reward: 2.300000, loss: 0.001016, epsilon: 0.050000, episode:  302\n",
      "frames: 582000, reward: 2.300000, loss: 0.001174, epsilon: 0.050000, episode:  302\n",
      "frames: 583000, reward: 2.300000, loss: 0.001041, epsilon: 0.050000, episode:  302\n",
      "frames: 584000, reward: 2.300000, loss: 0.002175, epsilon: 0.050000, episode:  302\n",
      "frames: 585000, reward: 2.800000, loss: 0.002055, epsilon: 0.050000, episode:  303\n",
      "frames: 586000, reward: 2.800000, loss: 0.003518, epsilon: 0.050000, episode:  303\n",
      "frames: 587000, reward: 2.800000, loss: 0.004651, epsilon: 0.050000, episode:  303\n",
      "frames: 588000, reward: 2.800000, loss: 0.001637, epsilon: 0.050000, episode:  303\n",
      "frames: 589000, reward: 2.100000, loss: 0.002011, epsilon: 0.050000, episode:  304\n",
      "frames: 590000, reward: 2.100000, loss: 0.003801, epsilon: 0.050000, episode:  304\n",
      "frames: 591000, reward: 2.100000, loss: 0.001589, epsilon: 0.050000, episode:  304\n",
      "frames: 592000, reward: 3.400000, loss: 0.001540, epsilon: 0.050000, episode:  305\n",
      "frames: 593000, reward: 3.400000, loss: 0.001097, epsilon: 0.050000, episode:  305\n",
      "frames: 594000, reward: 3.400000, loss: 0.001065, epsilon: 0.050000, episode:  305\n",
      "frames: 595000, reward: 3.400000, loss: 0.008098, epsilon: 0.050000, episode:  305\n",
      "frames: 596000, reward: 3.200000, loss: 0.002900, epsilon: 0.050000, episode:  306\n",
      "frames: 597000, reward: 3.200000, loss: 0.002702, epsilon: 0.050000, episode:  306\n",
      "frames: 598000, reward: 3.200000, loss: 0.003143, epsilon: 0.050000, episode:  306\n",
      "frames: 599000, reward: 4.800000, loss: 0.001393, epsilon: 0.050000, episode:  307\n",
      "frames: 600000, reward: 4.800000, loss: 0.003610, epsilon: 0.050000, episode:  307\n",
      "frames: 601000, reward: 4.800000, loss: 0.001874, epsilon: 0.050000, episode:  307\n",
      "frames: 602000, reward: 4.800000, loss: 0.001920, epsilon: 0.050000, episode:  307\n",
      "frames: 603000, reward: 5.500000, loss: 0.001268, epsilon: 0.050000, episode:  308\n",
      "frames: 604000, reward: 5.500000, loss: 0.000639, epsilon: 0.050000, episode:  308\n",
      "frames: 605000, reward: 5.800000, loss: 0.001223, epsilon: 0.050000, episode:  309\n",
      "frames: 606000, reward: 5.800000, loss: 0.001098, epsilon: 0.050000, episode:  309\n",
      "frames: 607000, reward: 5.800000, loss: 0.000678, epsilon: 0.050000, episode:  309\n",
      "frames: 608000, reward: 5.800000, loss: 0.001465, epsilon: 0.050000, episode:  309\n",
      "frames: 609000, reward: 6.200000, loss: 0.000432, epsilon: 0.050000, episode:  310\n",
      "frames: 610000, reward: 6.200000, loss: 0.001205, epsilon: 0.050000, episode:  310\n",
      "frames: 611000, reward: 6.200000, loss: 0.001145, epsilon: 0.050000, episode:  310\n",
      "frames: 612000, reward: 5.400000, loss: 0.006426, epsilon: 0.050000, episode:  311\n",
      "frames: 613000, reward: 5.400000, loss: 0.003024, epsilon: 0.050000, episode:  311\n",
      "frames: 614000, reward: 5.400000, loss: 0.002175, epsilon: 0.050000, episode:  311\n",
      "frames: 615000, reward: 5.400000, loss: 0.003001, epsilon: 0.050000, episode:  311\n",
      "frames: 616000, reward: 4.800000, loss: 0.002836, epsilon: 0.050000, episode:  312\n",
      "frames: 617000, reward: 4.800000, loss: 0.000911, epsilon: 0.050000, episode:  312\n",
      "frames: 618000, reward: 4.800000, loss: 0.001502, epsilon: 0.050000, episode:  312\n",
      "frames: 619000, reward: 4.900000, loss: 0.001404, epsilon: 0.050000, episode:  313\n",
      "frames: 620000, reward: 4.900000, loss: 0.001024, epsilon: 0.050000, episode:  313\n",
      "frames: 621000, reward: 4.900000, loss: 0.000838, epsilon: 0.050000, episode:  313\n",
      "frames: 622000, reward: 4.900000, loss: 0.000999, epsilon: 0.050000, episode:  313\n",
      "frames: 623000, reward: 3.800000, loss: 0.001377, epsilon: 0.050000, episode:  314\n",
      "frames: 624000, reward: 3.800000, loss: 0.009395, epsilon: 0.050000, episode:  314\n",
      "frames: 625000, reward: 3.800000, loss: 0.004036, epsilon: 0.050000, episode:  314\n",
      "frames: 626000, reward: 3.800000, loss: 0.004701, epsilon: 0.050000, episode:  314\n",
      "frames: 627000, reward: 3.800000, loss: 0.005560, epsilon: 0.050000, episode:  315\n",
      "frames: 628000, reward: 3.800000, loss: 0.001099, epsilon: 0.050000, episode:  315\n",
      "frames: 629000, reward: 3.800000, loss: 0.000855, epsilon: 0.050000, episode:  315\n",
      "frames: 630000, reward: 3.800000, loss: 0.001564, epsilon: 0.050000, episode:  315\n",
      "frames: 631000, reward: 5.000000, loss: 0.004158, epsilon: 0.050000, episode:  316\n",
      "frames: 632000, reward: 5.000000, loss: 0.001236, epsilon: 0.050000, episode:  316\n",
      "frames: 633000, reward: 5.000000, loss: 0.004220, epsilon: 0.050000, episode:  316\n",
      "frames: 634000, reward: 4.600000, loss: 0.001661, epsilon: 0.050000, episode:  317\n",
      "frames: 635000, reward: 4.600000, loss: 0.000633, epsilon: 0.050000, episode:  317\n",
      "frames: 636000, reward: 6.800000, loss: 0.001188, epsilon: 0.050000, episode:  318\n",
      "frames: 637000, reward: 6.800000, loss: 0.002377, epsilon: 0.050000, episode:  318\n",
      "frames: 638000, reward: 6.800000, loss: 0.000655, epsilon: 0.050000, episode:  318\n",
      "frames: 639000, reward: 6.800000, loss: 0.002390, epsilon: 0.050000, episode:  318\n",
      "frames: 640000, reward: 6.800000, loss: 0.001151, epsilon: 0.050000, episode:  318\n",
      "frames: 641000, reward: 5.600000, loss: 0.002994, epsilon: 0.050000, episode:  319\n",
      "frames: 642000, reward: 5.600000, loss: 0.001358, epsilon: 0.050000, episode:  319\n",
      "frames: 643000, reward: 5.600000, loss: 0.002891, epsilon: 0.050000, episode:  319\n",
      "frames: 644000, reward: 5.700000, loss: 0.001473, epsilon: 0.050000, episode:  320\n",
      "frames: 645000, reward: 5.700000, loss: 0.000946, epsilon: 0.050000, episode:  320\n",
      "frames: 646000, reward: 5.700000, loss: 0.001252, epsilon: 0.050000, episode:  320\n",
      "frames: 647000, reward: 6.100000, loss: 0.001058, epsilon: 0.050000, episode:  321\n",
      "frames: 648000, reward: 6.100000, loss: 0.008345, epsilon: 0.050000, episode:  321\n",
      "frames: 649000, reward: 6.800000, loss: 0.001061, epsilon: 0.050000, episode:  322\n",
      "frames: 650000, reward: 6.800000, loss: 0.002683, epsilon: 0.050000, episode:  322\n",
      "frames: 651000, reward: 6.800000, loss: 0.004105, epsilon: 0.050000, episode:  322\n",
      "frames: 652000, reward: 6.900000, loss: 0.001405, epsilon: 0.050000, episode:  323\n",
      "frames: 653000, reward: 6.900000, loss: 0.001520, epsilon: 0.050000, episode:  323\n",
      "frames: 654000, reward: 6.900000, loss: 0.002964, epsilon: 0.050000, episode:  323\n",
      "frames: 655000, reward: 6.900000, loss: 0.002099, epsilon: 0.050000, episode:  323\n",
      "frames: 656000, reward: 8.100000, loss: 0.000925, epsilon: 0.050000, episode:  324\n",
      "frames: 657000, reward: 8.100000, loss: 0.000818, epsilon: 0.050000, episode:  324\n",
      "frames: 658000, reward: 9.500000, loss: 0.001169, epsilon: 0.050000, episode:  325\n",
      "frames: 659000, reward: 9.500000, loss: 0.001067, epsilon: 0.050000, episode:  325\n",
      "frames: 660000, reward: 10.800000, loss: 0.001895, epsilon: 0.050000, episode:  326\n",
      "frames: 661000, reward: 10.800000, loss: 0.001129, epsilon: 0.050000, episode:  326\n",
      "frames: 662000, reward: 10.800000, loss: 0.006926, epsilon: 0.050000, episode:  326\n",
      "frames: 663000, reward: 11.400000, loss: 0.002085, epsilon: 0.050000, episode:  327\n",
      "frames: 664000, reward: 11.400000, loss: 0.001909, epsilon: 0.050000, episode:  327\n",
      "frames: 665000, reward: 11.400000, loss: 0.001883, epsilon: 0.050000, episode:  327\n",
      "frames: 666000, reward: 10.300000, loss: 0.000707, epsilon: 0.050000, episode:  328\n",
      "frames: 667000, reward: 10.300000, loss: 0.001438, epsilon: 0.050000, episode:  328\n",
      "frames: 668000, reward: 10.300000, loss: 0.000523, epsilon: 0.050000, episode:  328\n",
      "frames: 669000, reward: 11.400000, loss: 0.007796, epsilon: 0.050000, episode:  329\n",
      "frames: 670000, reward: 11.400000, loss: 0.002623, epsilon: 0.050000, episode:  329\n",
      "frames: 671000, reward: 11.400000, loss: 0.000793, epsilon: 0.050000, episode:  329\n",
      "frames: 672000, reward: 12.400000, loss: 0.020442, epsilon: 0.050000, episode:  330\n",
      "frames: 673000, reward: 12.400000, loss: 0.000886, epsilon: 0.050000, episode:  330\n",
      "frames: 674000, reward: 12.700000, loss: 0.000995, epsilon: 0.050000, episode:  331\n",
      "frames: 675000, reward: 12.700000, loss: 0.000570, epsilon: 0.050000, episode:  331\n",
      "frames: 676000, reward: 12.700000, loss: 0.000555, epsilon: 0.050000, episode:  331\n",
      "frames: 677000, reward: 12.500000, loss: 0.001756, epsilon: 0.050000, episode:  332\n",
      "frames: 678000, reward: 12.500000, loss: 0.006255, epsilon: 0.050000, episode:  332\n",
      "frames: 679000, reward: 12.500000, loss: 0.000518, epsilon: 0.050000, episode:  332\n",
      "frames: 680000, reward: 12.500000, loss: 0.001912, epsilon: 0.050000, episode:  332\n",
      "frames: 681000, reward: 12.400000, loss: 0.001177, epsilon: 0.050000, episode:  333\n",
      "frames: 682000, reward: 12.400000, loss: 0.000464, epsilon: 0.050000, episode:  333\n",
      "frames: 683000, reward: 12.400000, loss: 0.000466, epsilon: 0.050000, episode:  333\n",
      "frames: 684000, reward: 12.400000, loss: 0.002675, epsilon: 0.050000, episode:  334\n",
      "frames: 685000, reward: 12.400000, loss: 0.000829, epsilon: 0.050000, episode:  334\n",
      "frames: 686000, reward: 12.400000, loss: 0.000901, epsilon: 0.050000, episode:  334\n",
      "frames: 687000, reward: 12.400000, loss: 0.002234, epsilon: 0.050000, episode:  334\n",
      "frames: 688000, reward: 11.300000, loss: 0.001766, epsilon: 0.050000, episode:  335\n",
      "frames: 689000, reward: 11.200000, loss: 0.001509, epsilon: 0.050000, episode:  336\n",
      "frames: 690000, reward: 11.200000, loss: 0.001277, epsilon: 0.050000, episode:  336\n",
      "frames: 691000, reward: 11.200000, loss: 0.000840, epsilon: 0.050000, episode:  336\n",
      "frames: 692000, reward: 11.100000, loss: 0.000884, epsilon: 0.050000, episode:  337\n",
      "frames: 693000, reward: 11.100000, loss: 0.000781, epsilon: 0.050000, episode:  337\n",
      "frames: 694000, reward: 11.100000, loss: 0.000711, epsilon: 0.050000, episode:  337\n",
      "frames: 695000, reward: 11.000000, loss: 0.000416, epsilon: 0.050000, episode:  338\n",
      "frames: 696000, reward: 11.000000, loss: 0.000721, epsilon: 0.050000, episode:  338\n",
      "frames: 697000, reward: 11.000000, loss: 0.001165, epsilon: 0.050000, episode:  338\n",
      "frames: 698000, reward: 11.000000, loss: 0.000691, epsilon: 0.050000, episode:  338\n",
      "frames: 699000, reward: 10.400000, loss: 0.001373, epsilon: 0.050000, episode:  339\n",
      "frames: 700000, reward: 10.400000, loss: 0.001262, epsilon: 0.050000, episode:  339\n",
      "frames: 701000, reward: 10.700000, loss: 0.000724, epsilon: 0.050000, episode:  340\n",
      "frames: 702000, reward: 10.700000, loss: 0.001711, epsilon: 0.050000, episode:  340\n",
      "frames: 703000, reward: 10.700000, loss: 0.002149, epsilon: 0.050000, episode:  340\n",
      "frames: 704000, reward: 10.700000, loss: 0.000900, epsilon: 0.050000, episode:  341\n",
      "frames: 705000, reward: 10.700000, loss: 0.000400, epsilon: 0.050000, episode:  341\n",
      "frames: 706000, reward: 10.800000, loss: 0.001041, epsilon: 0.050000, episode:  342\n",
      "frames: 707000, reward: 10.800000, loss: 0.001886, epsilon: 0.050000, episode:  342\n",
      "frames: 708000, reward: 10.800000, loss: 0.000729, epsilon: 0.050000, episode:  342\n",
      "frames: 709000, reward: 11.300000, loss: 0.003137, epsilon: 0.050000, episode:  343\n",
      "frames: 710000, reward: 11.300000, loss: 0.001894, epsilon: 0.050000, episode:  343\n",
      "frames: 711000, reward: 11.300000, loss: 0.001857, epsilon: 0.050000, episode:  343\n",
      "frames: 712000, reward: 10.700000, loss: 0.001013, epsilon: 0.050000, episode:  344\n",
      "frames: 713000, reward: 10.700000, loss: 0.003401, epsilon: 0.050000, episode:  344\n",
      "frames: 714000, reward: 10.700000, loss: 0.005467, epsilon: 0.050000, episode:  344\n",
      "frames: 715000, reward: 11.600000, loss: 0.001971, epsilon: 0.050000, episode:  345\n",
      "frames: 716000, reward: 11.600000, loss: 0.001367, epsilon: 0.050000, episode:  345\n",
      "frames: 717000, reward: 11.600000, loss: 0.002699, epsilon: 0.050000, episode:  345\n",
      "frames: 718000, reward: 10.900000, loss: 0.000732, epsilon: 0.050000, episode:  346\n",
      "frames: 719000, reward: 10.900000, loss: 0.001292, epsilon: 0.050000, episode:  346\n",
      "frames: 720000, reward: 8.400000, loss: 0.016945, epsilon: 0.050000, episode:  347\n",
      "frames: 721000, reward: 8.400000, loss: 0.001438, epsilon: 0.050000, episode:  347\n",
      "frames: 722000, reward: 8.400000, loss: 0.001892, epsilon: 0.050000, episode:  347\n",
      "frames: 723000, reward: 8.400000, loss: 0.000977, epsilon: 0.050000, episode:  347\n",
      "frames: 724000, reward: 8.400000, loss: 0.001801, epsilon: 0.050000, episode:  348\n",
      "frames: 725000, reward: 8.400000, loss: 0.000638, epsilon: 0.050000, episode:  348\n",
      "frames: 726000, reward: 8.400000, loss: 0.001618, epsilon: 0.050000, episode:  348\n",
      "frames: 727000, reward: 8.300000, loss: 0.003114, epsilon: 0.050000, episode:  349\n",
      "frames: 728000, reward: 8.300000, loss: 0.000630, epsilon: 0.050000, episode:  349\n",
      "frames: 729000, reward: 8.300000, loss: 0.001058, epsilon: 0.050000, episode:  349\n",
      "frames: 730000, reward: 6.600000, loss: 0.000625, epsilon: 0.050000, episode:  350\n",
      "frames: 731000, reward: 6.600000, loss: 0.006424, epsilon: 0.050000, episode:  350\n",
      "frames: 732000, reward: 6.600000, loss: 0.001250, epsilon: 0.050000, episode:  350\n",
      "frames: 733000, reward: 6.200000, loss: 0.000534, epsilon: 0.050000, episode:  351\n",
      "frames: 734000, reward: 6.200000, loss: 0.004316, epsilon: 0.050000, episode:  351\n",
      "frames: 735000, reward: 6.200000, loss: 0.002257, epsilon: 0.050000, episode:  351\n",
      "frames: 736000, reward: 6.100000, loss: 0.001352, epsilon: 0.050000, episode:  352\n",
      "frames: 737000, reward: 6.100000, loss: 0.000889, epsilon: 0.050000, episode:  352\n",
      "frames: 738000, reward: 6.100000, loss: 0.000772, epsilon: 0.050000, episode:  352\n",
      "frames: 739000, reward: 6.100000, loss: 0.000744, epsilon: 0.050000, episode:  352\n",
      "frames: 740000, reward: 4.700000, loss: 0.000719, epsilon: 0.050000, episode:  353\n",
      "frames: 741000, reward: 4.700000, loss: 0.001083, epsilon: 0.050000, episode:  353\n",
      "frames: 742000, reward: 4.700000, loss: 0.001409, epsilon: 0.050000, episode:  353\n",
      "frames: 743000, reward: 5.300000, loss: 0.000854, epsilon: 0.050000, episode:  354\n",
      "frames: 744000, reward: 5.300000, loss: 0.001048, epsilon: 0.050000, episode:  354\n",
      "frames: 745000, reward: 5.300000, loss: 0.000830, epsilon: 0.050000, episode:  354\n",
      "frames: 746000, reward: 5.300000, loss: 0.001093, epsilon: 0.050000, episode:  354\n",
      "frames: 747000, reward: 4.100000, loss: 0.003482, epsilon: 0.050000, episode:  355\n",
      "frames: 748000, reward: 4.100000, loss: 0.006926, epsilon: 0.050000, episode:  355\n",
      "frames: 749000, reward: 4.900000, loss: 0.002166, epsilon: 0.050000, episode:  356\n",
      "frames: 750000, reward: 4.900000, loss: 0.000758, epsilon: 0.050000, episode:  356\n",
      "frames: 751000, reward: 7.600000, loss: 0.001030, epsilon: 0.050000, episode:  357\n",
      "frames: 752000, reward: 7.600000, loss: 0.001551, epsilon: 0.050000, episode:  357\n",
      "frames: 753000, reward: 7.600000, loss: 0.001334, epsilon: 0.050000, episode:  357\n",
      "frames: 754000, reward: 8.500000, loss: 0.000755, epsilon: 0.050000, episode:  358\n",
      "frames: 755000, reward: 8.500000, loss: 0.000999, epsilon: 0.050000, episode:  358\n",
      "frames: 756000, reward: 8.500000, loss: 0.001696, epsilon: 0.050000, episode:  358\n",
      "frames: 757000, reward: 8.300000, loss: 0.000786, epsilon: 0.050000, episode:  359\n",
      "frames: 758000, reward: 8.300000, loss: 0.001424, epsilon: 0.050000, episode:  359\n",
      "frames: 759000, reward: 8.300000, loss: 0.001934, epsilon: 0.050000, episode:  359\n",
      "frames: 760000, reward: 8.300000, loss: 0.007004, epsilon: 0.050000, episode:  359\n",
      "frames: 761000, reward: 8.700000, loss: 0.000760, epsilon: 0.050000, episode:  360\n",
      "frames: 762000, reward: 8.700000, loss: 0.005807, epsilon: 0.050000, episode:  360\n",
      "frames: 763000, reward: 9.000000, loss: 0.000914, epsilon: 0.050000, episode:  361\n",
      "frames: 764000, reward: 9.000000, loss: 0.000291, epsilon: 0.050000, episode:  361\n",
      "frames: 765000, reward: 9.000000, loss: 0.001296, epsilon: 0.050000, episode:  361\n",
      "frames: 766000, reward: 9.100000, loss: 0.000765, epsilon: 0.050000, episode:  362\n",
      "frames: 767000, reward: 9.100000, loss: 0.000850, epsilon: 0.050000, episode:  362\n",
      "frames: 768000, reward: 9.100000, loss: 0.000517, epsilon: 0.050000, episode:  362\n",
      "frames: 769000, reward: 10.500000, loss: 0.001479, epsilon: 0.050000, episode:  363\n",
      "frames: 770000, reward: 10.500000, loss: 0.002107, epsilon: 0.050000, episode:  363\n",
      "frames: 771000, reward: 10.500000, loss: 0.001231, epsilon: 0.050000, episode:  363\n",
      "frames: 772000, reward: 11.200000, loss: 0.000499, epsilon: 0.050000, episode:  364\n",
      "frames: 773000, reward: 11.200000, loss: 0.000382, epsilon: 0.050000, episode:  364\n",
      "frames: 774000, reward: 11.200000, loss: 0.002177, epsilon: 0.050000, episode:  364\n",
      "frames: 775000, reward: 11.800000, loss: 0.002696, epsilon: 0.050000, episode:  365\n",
      "frames: 776000, reward: 11.800000, loss: 0.000965, epsilon: 0.050000, episode:  365\n",
      "frames: 777000, reward: 11.100000, loss: 0.010791, epsilon: 0.050000, episode:  366\n",
      "frames: 778000, reward: 11.100000, loss: 0.001141, epsilon: 0.050000, episode:  366\n",
      "frames: 779000, reward: 11.100000, loss: 0.000855, epsilon: 0.050000, episode:  366\n",
      "frames: 780000, reward: 11.100000, loss: 0.001191, epsilon: 0.050000, episode:  366\n",
      "frames: 781000, reward: 9.700000, loss: 0.002133, epsilon: 0.050000, episode:  367\n",
      "frames: 782000, reward: 9.700000, loss: 0.005936, epsilon: 0.050000, episode:  367\n",
      "frames: 783000, reward: 10.100000, loss: 0.000619, epsilon: 0.050000, episode:  368\n",
      "frames: 784000, reward: 10.100000, loss: 0.000807, epsilon: 0.050000, episode:  368\n",
      "frames: 785000, reward: 10.100000, loss: 0.001182, epsilon: 0.050000, episode:  368\n",
      "frames: 786000, reward: 11.300000, loss: 0.001089, epsilon: 0.050000, episode:  369\n",
      "frames: 787000, reward: 11.300000, loss: 0.003405, epsilon: 0.050000, episode:  369\n",
      "frames: 788000, reward: 11.300000, loss: 0.002107, epsilon: 0.050000, episode:  369\n",
      "frames: 789000, reward: 12.000000, loss: 0.000914, epsilon: 0.050000, episode:  370\n",
      "frames: 790000, reward: 12.000000, loss: 0.002480, epsilon: 0.050000, episode:  370\n",
      "frames: 791000, reward: 12.000000, loss: 0.000605, epsilon: 0.050000, episode:  370\n",
      "frames: 792000, reward: 11.600000, loss: 0.001210, epsilon: 0.050000, episode:  371\n",
      "frames: 793000, reward: 11.600000, loss: 0.001580, epsilon: 0.050000, episode:  371\n",
      "frames: 794000, reward: 11.800000, loss: 0.000947, epsilon: 0.050000, episode:  372\n",
      "frames: 795000, reward: 11.800000, loss: 0.001832, epsilon: 0.050000, episode:  372\n",
      "frames: 796000, reward: 11.800000, loss: 0.000995, epsilon: 0.050000, episode:  372\n",
      "frames: 797000, reward: 11.900000, loss: 0.001142, epsilon: 0.050000, episode:  373\n",
      "frames: 798000, reward: 11.900000, loss: 0.000786, epsilon: 0.050000, episode:  373\n",
      "frames: 799000, reward: 12.200000, loss: 0.001243, epsilon: 0.050000, episode:  374\n",
      "frames: 800000, reward: 12.200000, loss: 0.003817, epsilon: 0.050000, episode:  374\n",
      "frames: 801000, reward: 12.200000, loss: 0.001647, epsilon: 0.050000, episode:  374\n",
      "frames: 802000, reward: 13.000000, loss: 0.004039, epsilon: 0.050000, episode:  375\n",
      "frames: 803000, reward: 13.000000, loss: 0.001722, epsilon: 0.050000, episode:  375\n",
      "frames: 804000, reward: 13.500000, loss: 0.000513, epsilon: 0.050000, episode:  376\n",
      "frames: 805000, reward: 13.500000, loss: 0.000553, epsilon: 0.050000, episode:  376\n",
      "frames: 806000, reward: 14.900000, loss: 0.000753, epsilon: 0.050000, episode:  377\n",
      "frames: 807000, reward: 14.900000, loss: 0.000419, epsilon: 0.050000, episode:  377\n",
      "frames: 808000, reward: 14.900000, loss: 0.001403, epsilon: 0.050000, episode:  377\n",
      "frames: 809000, reward: 14.300000, loss: 0.001384, epsilon: 0.050000, episode:  378\n",
      "frames: 810000, reward: 14.300000, loss: 0.000785, epsilon: 0.050000, episode:  378\n",
      "frames: 811000, reward: 14.300000, loss: 0.001767, epsilon: 0.050000, episode:  378\n",
      "frames: 812000, reward: 14.900000, loss: 0.001231, epsilon: 0.050000, episode:  379\n",
      "frames: 813000, reward: 14.900000, loss: 0.002406, epsilon: 0.050000, episode:  379\n",
      "frames: 814000, reward: 15.700000, loss: 0.001813, epsilon: 0.050000, episode:  380\n",
      "frames: 815000, reward: 15.700000, loss: 0.000820, epsilon: 0.050000, episode:  380\n",
      "frames: 816000, reward: 15.700000, loss: 0.001233, epsilon: 0.050000, episode:  380\n",
      "frames: 817000, reward: 15.800000, loss: 0.000676, epsilon: 0.050000, episode:  381\n",
      "frames: 818000, reward: 15.800000, loss: 0.005273, epsilon: 0.050000, episode:  381\n",
      "frames: 819000, reward: 16.100000, loss: 0.000581, epsilon: 0.050000, episode:  382\n",
      "frames: 820000, reward: 16.100000, loss: 0.000904, epsilon: 0.050000, episode:  382\n",
      "frames: 821000, reward: 16.100000, loss: 0.001422, epsilon: 0.050000, episode:  382\n",
      "frames: 822000, reward: 16.100000, loss: 0.002125, epsilon: 0.050000, episode:  383\n",
      "frames: 823000, reward: 16.100000, loss: 0.001951, epsilon: 0.050000, episode:  383\n",
      "frames: 824000, reward: 16.100000, loss: 0.001940, epsilon: 0.050000, episode:  383\n",
      "frames: 825000, reward: 15.900000, loss: 0.001545, epsilon: 0.050000, episode:  384\n",
      "frames: 826000, reward: 15.900000, loss: 0.001388, epsilon: 0.050000, episode:  384\n",
      "frames: 827000, reward: 15.600000, loss: 0.001688, epsilon: 0.050000, episode:  385\n",
      "frames: 828000, reward: 15.600000, loss: 0.000752, epsilon: 0.050000, episode:  385\n",
      "frames: 829000, reward: 15.600000, loss: 0.000598, epsilon: 0.050000, episode:  385\n",
      "frames: 830000, reward: 15.400000, loss: 0.000227, epsilon: 0.050000, episode:  386\n",
      "frames: 831000, reward: 15.400000, loss: 0.001879, epsilon: 0.050000, episode:  386\n",
      "frames: 832000, reward: 15.400000, loss: 0.000701, epsilon: 0.050000, episode:  386\n",
      "frames: 833000, reward: 15.100000, loss: 0.006447, epsilon: 0.050000, episode:  387\n",
      "frames: 834000, reward: 15.100000, loss: 0.001859, epsilon: 0.050000, episode:  387\n",
      "frames: 835000, reward: 15.300000, loss: 0.000402, epsilon: 0.050000, episode:  388\n",
      "frames: 836000, reward: 15.300000, loss: 0.000509, epsilon: 0.050000, episode:  388\n",
      "frames: 837000, reward: 15.300000, loss: 0.000698, epsilon: 0.050000, episode:  388\n",
      "frames: 838000, reward: 14.800000, loss: 0.002830, epsilon: 0.050000, episode:  389\n",
      "frames: 839000, reward: 14.800000, loss: 0.000827, epsilon: 0.050000, episode:  389\n",
      "frames: 840000, reward: 14.800000, loss: 0.001619, epsilon: 0.050000, episode:  389\n",
      "frames: 841000, reward: 14.500000, loss: 0.002074, epsilon: 0.050000, episode:  390\n",
      "frames: 842000, reward: 14.500000, loss: 0.000491, epsilon: 0.050000, episode:  390\n",
      "frames: 843000, reward: 14.700000, loss: 0.000592, epsilon: 0.050000, episode:  391\n",
      "frames: 844000, reward: 14.700000, loss: 0.000916, epsilon: 0.050000, episode:  391\n",
      "frames: 845000, reward: 14.700000, loss: 0.000691, epsilon: 0.050000, episode:  391\n",
      "frames: 846000, reward: 14.600000, loss: 0.000884, epsilon: 0.050000, episode:  392\n",
      "frames: 847000, reward: 14.600000, loss: 0.001209, epsilon: 0.050000, episode:  392\n",
      "frames: 848000, reward: 14.600000, loss: 0.002671, epsilon: 0.050000, episode:  393\n",
      "frames: 849000, reward: 14.600000, loss: 0.001298, epsilon: 0.050000, episode:  393\n",
      "frames: 850000, reward: 14.600000, loss: 0.000595, epsilon: 0.050000, episode:  393\n",
      "frames: 851000, reward: 14.400000, loss: 0.000448, epsilon: 0.050000, episode:  394\n",
      "frames: 852000, reward: 14.400000, loss: 0.000572, epsilon: 0.050000, episode:  394\n",
      "frames: 853000, reward: 14.400000, loss: 0.000443, epsilon: 0.050000, episode:  394\n",
      "frames: 854000, reward: 14.200000, loss: 0.000689, epsilon: 0.050000, episode:  395\n",
      "frames: 855000, reward: 14.200000, loss: 0.000919, epsilon: 0.050000, episode:  395\n",
      "frames: 856000, reward: 14.200000, loss: 0.000427, epsilon: 0.050000, episode:  395\n",
      "frames: 857000, reward: 14.000000, loss: 0.000634, epsilon: 0.050000, episode:  396\n",
      "frames: 858000, reward: 14.000000, loss: 0.000496, epsilon: 0.050000, episode:  396\n",
      "frames: 859000, reward: 14.400000, loss: 0.003030, epsilon: 0.050000, episode:  397\n",
      "frames: 860000, reward: 14.400000, loss: 0.001472, epsilon: 0.050000, episode:  397\n",
      "frames: 861000, reward: 14.900000, loss: 0.001161, epsilon: 0.050000, episode:  398\n",
      "frames: 862000, reward: 14.900000, loss: 0.002409, epsilon: 0.050000, episode:  398\n",
      "frames: 863000, reward: 15.100000, loss: 0.000794, epsilon: 0.050000, episode:  399\n",
      "frames: 864000, reward: 15.100000, loss: 0.000404, epsilon: 0.050000, episode:  399\n",
      "frames: 865000, reward: 15.100000, loss: 0.000761, epsilon: 0.050000, episode:  399\n",
      "frames: 866000, reward: 15.100000, loss: 0.000456, epsilon: 0.050000, episode:  399\n",
      "frames: 867000, reward: 14.100000, loss: 0.001016, epsilon: 0.050000, episode:  400\n",
      "frames: 868000, reward: 14.100000, loss: 0.000944, epsilon: 0.050000, episode:  400\n",
      "frames: 869000, reward: 14.400000, loss: 0.001614, epsilon: 0.050000, episode:  401\n",
      "frames: 870000, reward: 14.400000, loss: 0.001459, epsilon: 0.050000, episode:  401\n",
      "frames: 871000, reward: 14.400000, loss: 0.000644, epsilon: 0.050000, episode:  401\n",
      "frames: 872000, reward: 14.200000, loss: 0.000868, epsilon: 0.050000, episode:  402\n",
      "frames: 873000, reward: 14.200000, loss: 0.025105, epsilon: 0.050000, episode:  402\n",
      "frames: 874000, reward: 14.200000, loss: 0.001249, epsilon: 0.050000, episode:  403\n",
      "frames: 875000, reward: 14.200000, loss: 0.000839, epsilon: 0.050000, episode:  403\n",
      "frames: 876000, reward: 14.200000, loss: 0.000866, epsilon: 0.050000, episode:  403\n",
      "frames: 877000, reward: 14.200000, loss: 0.001035, epsilon: 0.050000, episode:  403\n",
      "frames: 878000, reward: 13.500000, loss: 0.000735, epsilon: 0.050000, episode:  404\n",
      "frames: 879000, reward: 13.500000, loss: 0.000640, epsilon: 0.050000, episode:  404\n",
      "frames: 880000, reward: 13.400000, loss: 0.002341, epsilon: 0.050000, episode:  405\n",
      "frames: 881000, reward: 13.400000, loss: 0.000285, epsilon: 0.050000, episode:  405\n",
      "frames: 882000, reward: 13.400000, loss: 0.001583, epsilon: 0.050000, episode:  405\n",
      "frames: 883000, reward: 13.800000, loss: 0.000844, epsilon: 0.050000, episode:  406\n",
      "frames: 884000, reward: 13.800000, loss: 0.001126, epsilon: 0.050000, episode:  406\n",
      "frames: 885000, reward: 13.800000, loss: 0.000756, epsilon: 0.050000, episode:  406\n",
      "frames: 886000, reward: 13.300000, loss: 0.000510, epsilon: 0.050000, episode:  407\n",
      "frames: 887000, reward: 13.300000, loss: 0.002169, epsilon: 0.050000, episode:  407\n",
      "frames: 888000, reward: 12.800000, loss: 0.003254, epsilon: 0.050000, episode:  408\n",
      "frames: 889000, reward: 12.800000, loss: 0.001604, epsilon: 0.050000, episode:  408\n",
      "frames: 890000, reward: 12.800000, loss: 0.001213, epsilon: 0.050000, episode:  408\n",
      "frames: 891000, reward: 12.500000, loss: 0.003588, epsilon: 0.050000, episode:  409\n",
      "frames: 892000, reward: 12.500000, loss: 0.002278, epsilon: 0.050000, episode:  409\n",
      "frames: 893000, reward: 13.600000, loss: 0.000580, epsilon: 0.050000, episode:  410\n",
      "frames: 894000, reward: 13.600000, loss: 0.000411, epsilon: 0.050000, episode:  410\n",
      "frames: 895000, reward: 13.600000, loss: 0.005004, epsilon: 0.050000, episode:  410\n",
      "frames: 896000, reward: 13.300000, loss: 0.000444, epsilon: 0.050000, episode:  411\n",
      "frames: 897000, reward: 13.300000, loss: 0.000584, epsilon: 0.050000, episode:  411\n",
      "frames: 898000, reward: 13.300000, loss: 0.000441, epsilon: 0.050000, episode:  411\n",
      "frames: 899000, reward: 13.300000, loss: 0.000340, epsilon: 0.050000, episode:  412\n",
      "frames: 900000, reward: 13.300000, loss: 0.000959, epsilon: 0.050000, episode:  412\n",
      "frames: 901000, reward: 13.300000, loss: 0.001273, epsilon: 0.050000, episode:  412\n",
      "frames: 902000, reward: 12.800000, loss: 0.000426, epsilon: 0.050000, episode:  413\n",
      "frames: 903000, reward: 12.800000, loss: 0.004745, epsilon: 0.050000, episode:  413\n",
      "frames: 904000, reward: 13.800000, loss: 0.000433, epsilon: 0.050000, episode:  414\n",
      "frames: 905000, reward: 13.800000, loss: 0.001220, epsilon: 0.050000, episode:  414\n",
      "frames: 906000, reward: 14.200000, loss: 0.001724, epsilon: 0.050000, episode:  415\n",
      "frames: 907000, reward: 14.200000, loss: 0.000622, epsilon: 0.050000, episode:  415\n",
      "frames: 908000, reward: 14.200000, loss: 0.001006, epsilon: 0.050000, episode:  415\n",
      "frames: 909000, reward: 13.800000, loss: 0.000749, epsilon: 0.050000, episode:  416\n",
      "frames: 910000, reward: 13.800000, loss: 0.000672, epsilon: 0.050000, episode:  416\n",
      "frames: 911000, reward: 14.400000, loss: 0.000563, epsilon: 0.050000, episode:  417\n",
      "frames: 912000, reward: 14.400000, loss: 0.001161, epsilon: 0.050000, episode:  417\n",
      "frames: 913000, reward: 14.500000, loss: 0.000945, epsilon: 0.050000, episode:  418\n",
      "frames: 914000, reward: 14.500000, loss: 0.000524, epsilon: 0.050000, episode:  418\n",
      "frames: 915000, reward: 14.500000, loss: 0.001312, epsilon: 0.050000, episode:  418\n",
      "frames: 916000, reward: 14.600000, loss: 0.001075, epsilon: 0.050000, episode:  419\n",
      "frames: 917000, reward: 14.600000, loss: 0.001895, epsilon: 0.050000, episode:  419\n",
      "frames: 918000, reward: 14.500000, loss: 0.000421, epsilon: 0.050000, episode:  420\n",
      "frames: 919000, reward: 14.500000, loss: 0.001243, epsilon: 0.050000, episode:  420\n",
      "frames: 920000, reward: 14.800000, loss: 0.000856, epsilon: 0.050000, episode:  421\n",
      "frames: 921000, reward: 14.800000, loss: 0.000969, epsilon: 0.050000, episode:  421\n",
      "frames: 922000, reward: 14.900000, loss: 0.000338, epsilon: 0.050000, episode:  422\n",
      "frames: 923000, reward: 14.900000, loss: 0.000920, epsilon: 0.050000, episode:  422\n",
      "frames: 924000, reward: 16.100000, loss: 0.001174, epsilon: 0.050000, episode:  423\n",
      "frames: 925000, reward: 16.100000, loss: 0.000892, epsilon: 0.050000, episode:  423\n",
      "frames: 926000, reward: 16.100000, loss: 0.000703, epsilon: 0.050000, episode:  424\n",
      "frames: 927000, reward: 16.100000, loss: 0.001340, epsilon: 0.050000, episode:  424\n",
      "frames: 928000, reward: 16.100000, loss: 0.000731, epsilon: 0.050000, episode:  424\n",
      "frames: 929000, reward: 16.400000, loss: 0.001595, epsilon: 0.050000, episode:  425\n",
      "frames: 930000, reward: 16.400000, loss: 0.000619, epsilon: 0.050000, episode:  425\n",
      "frames: 931000, reward: 17.100000, loss: 0.001032, epsilon: 0.050000, episode:  426\n",
      "frames: 932000, reward: 17.100000, loss: 0.002195, epsilon: 0.050000, episode:  426\n",
      "frames: 933000, reward: 16.700000, loss: 0.000879, epsilon: 0.050000, episode:  427\n",
      "frames: 934000, reward: 16.700000, loss: 0.000971, epsilon: 0.050000, episode:  427\n",
      "frames: 935000, reward: 16.700000, loss: 0.000899, epsilon: 0.050000, episode:  427\n",
      "frames: 936000, reward: 16.700000, loss: 0.000793, epsilon: 0.050000, episode:  428\n",
      "frames: 937000, reward: 16.700000, loss: 0.004044, epsilon: 0.050000, episode:  428\n",
      "frames: 938000, reward: 16.700000, loss: 0.002897, epsilon: 0.050000, episode:  429\n",
      "frames: 939000, reward: 16.700000, loss: 0.001277, epsilon: 0.050000, episode:  429\n",
      "frames: 940000, reward: 16.700000, loss: 0.000493, epsilon: 0.050000, episode:  429\n",
      "frames: 941000, reward: 16.800000, loss: 0.000471, epsilon: 0.050000, episode:  430\n",
      "frames: 942000, reward: 16.800000, loss: 0.000651, epsilon: 0.050000, episode:  430\n",
      "frames: 943000, reward: 16.700000, loss: 0.001045, epsilon: 0.050000, episode:  431\n",
      "frames: 944000, reward: 16.700000, loss: 0.000867, epsilon: 0.050000, episode:  431\n",
      "frames: 945000, reward: 16.700000, loss: 0.002126, epsilon: 0.050000, episode:  431\n",
      "frames: 946000, reward: 15.200000, loss: 0.000951, epsilon: 0.050000, episode:  432\n",
      "frames: 947000, reward: 15.200000, loss: 0.000873, epsilon: 0.050000, episode:  432\n",
      "frames: 948000, reward: 15.100000, loss: 0.000445, epsilon: 0.050000, episode:  433\n",
      "frames: 949000, reward: 15.100000, loss: 0.000566, epsilon: 0.050000, episode:  433\n",
      "frames: 950000, reward: 15.100000, loss: 0.000831, epsilon: 0.050000, episode:  434\n",
      "frames: 951000, reward: 15.100000, loss: 0.000215, epsilon: 0.050000, episode:  434\n",
      "frames: 952000, reward: 15.000000, loss: 0.000927, epsilon: 0.050000, episode:  435\n",
      "frames: 953000, reward: 15.000000, loss: 0.000603, epsilon: 0.050000, episode:  435\n",
      "frames: 954000, reward: 14.800000, loss: 0.000359, epsilon: 0.050000, episode:  436\n",
      "frames: 955000, reward: 14.800000, loss: 0.000357, epsilon: 0.050000, episode:  436\n",
      "frames: 956000, reward: 14.800000, loss: 0.000496, epsilon: 0.050000, episode:  436\n",
      "frames: 957000, reward: 14.600000, loss: 0.000743, epsilon: 0.050000, episode:  437\n",
      "frames: 958000, reward: 14.600000, loss: 0.000928, epsilon: 0.050000, episode:  437\n",
      "frames: 959000, reward: 14.900000, loss: 0.000237, epsilon: 0.050000, episode:  438\n",
      "frames: 960000, reward: 14.900000, loss: 0.000583, epsilon: 0.050000, episode:  438\n",
      "frames: 961000, reward: 15.200000, loss: 0.000398, epsilon: 0.050000, episode:  439\n",
      "frames: 962000, reward: 15.200000, loss: 0.000631, epsilon: 0.050000, episode:  439\n",
      "frames: 963000, reward: 15.400000, loss: 0.000491, epsilon: 0.050000, episode:  440\n",
      "frames: 964000, reward: 15.400000, loss: 0.001667, epsilon: 0.050000, episode:  440\n",
      "frames: 965000, reward: 15.600000, loss: 0.000738, epsilon: 0.050000, episode:  441\n",
      "frames: 966000, reward: 15.600000, loss: 0.000479, epsilon: 0.050000, episode:  441\n",
      "frames: 967000, reward: 17.000000, loss: 0.001254, epsilon: 0.050000, episode:  442\n",
      "frames: 968000, reward: 17.000000, loss: 0.000458, epsilon: 0.050000, episode:  442\n",
      "frames: 969000, reward: 17.000000, loss: 0.000995, epsilon: 0.050000, episode:  442\n",
      "frames: 970000, reward: 16.500000, loss: 0.000450, epsilon: 0.050000, episode:  443\n",
      "frames: 971000, reward: 16.500000, loss: 0.000465, epsilon: 0.050000, episode:  443\n",
      "frames: 972000, reward: 16.700000, loss: 0.000618, epsilon: 0.050000, episode:  444\n",
      "frames: 973000, reward: 16.700000, loss: 0.000344, epsilon: 0.050000, episode:  444\n",
      "frames: 974000, reward: 16.500000, loss: 0.001073, epsilon: 0.050000, episode:  445\n",
      "frames: 975000, reward: 16.500000, loss: 0.000399, epsilon: 0.050000, episode:  445\n",
      "frames: 976000, reward: 16.500000, loss: 0.000426, epsilon: 0.050000, episode:  445\n",
      "frames: 977000, reward: 15.800000, loss: 0.000567, epsilon: 0.050000, episode:  446\n",
      "frames: 978000, reward: 15.800000, loss: 0.000776, epsilon: 0.050000, episode:  446\n",
      "frames: 979000, reward: 15.800000, loss: 0.000628, epsilon: 0.050000, episode:  446\n",
      "frames: 980000, reward: 16.400000, loss: 0.000352, epsilon: 0.050000, episode:  447\n",
      "frames: 981000, reward: 16.400000, loss: 0.001267, epsilon: 0.050000, episode:  447\n",
      "frames: 982000, reward: 15.900000, loss: 0.000281, epsilon: 0.050000, episode:  448\n",
      "frames: 983000, reward: 15.900000, loss: 0.000519, epsilon: 0.050000, episode:  448\n",
      "frames: 984000, reward: 16.300000, loss: 0.000660, epsilon: 0.050000, episode:  449\n",
      "frames: 985000, reward: 16.300000, loss: 0.000480, epsilon: 0.050000, episode:  449\n",
      "frames: 986000, reward: 16.300000, loss: 0.000753, epsilon: 0.050000, episode:  449\n",
      "frames: 987000, reward: 15.400000, loss: 0.001199, epsilon: 0.050000, episode:  450\n",
      "frames: 988000, reward: 15.400000, loss: 0.001982, epsilon: 0.050000, episode:  450\n",
      "frames: 989000, reward: 15.300000, loss: 0.001046, epsilon: 0.050000, episode:  451\n",
      "frames: 990000, reward: 15.300000, loss: 0.000639, epsilon: 0.050000, episode:  451\n",
      "frames: 991000, reward: 15.300000, loss: 0.000730, epsilon: 0.050000, episode:  452\n",
      "frames: 992000, reward: 15.300000, loss: 0.000541, epsilon: 0.050000, episode:  452\n",
      "frames: 993000, reward: 15.300000, loss: 0.002237, epsilon: 0.050000, episode:  452\n",
      "frames: 994000, reward: 15.300000, loss: 0.000422, epsilon: 0.050000, episode:  453\n",
      "frames: 995000, reward: 15.300000, loss: 0.000253, epsilon: 0.050000, episode:  453\n",
      "frames: 996000, reward: 14.800000, loss: 0.000610, epsilon: 0.050000, episode:  454\n",
      "frames: 997000, reward: 14.800000, loss: 0.002149, epsilon: 0.050000, episode:  454\n",
      "frames: 998000, reward: 14.800000, loss: 0.001451, epsilon: 0.050000, episode:  454\n",
      "frames: 999000, reward: 14.800000, loss: 0.000653, epsilon: 0.050000, episode:  455\n",
      "frames: 1000000, reward: 14.800000, loss: 0.000468, epsilon: 0.050000, episode:  455\n",
      "frames: 1001000, reward: 15.500000, loss: 0.004184, epsilon: 0.050000, episode:  456\n",
      "frames: 1002000, reward: 15.500000, loss: 0.000565, epsilon: 0.050000, episode:  456\n",
      "frames: 1003000, reward: 15.500000, loss: 0.001856, epsilon: 0.050000, episode:  457\n",
      "frames: 1004000, reward: 15.500000, loss: 0.001305, epsilon: 0.050000, episode:  457\n",
      "frames: 1005000, reward: 15.700000, loss: 0.000402, epsilon: 0.050000, episode:  458\n",
      "frames: 1006000, reward: 15.700000, loss: 0.001225, epsilon: 0.050000, episode:  458\n",
      "frames: 1007000, reward: 15.100000, loss: 0.001708, epsilon: 0.050000, episode:  459\n",
      "frames: 1008000, reward: 15.100000, loss: 0.000354, epsilon: 0.050000, episode:  459\n",
      "frames: 1009000, reward: 15.100000, loss: 0.001633, epsilon: 0.050000, episode:  459\n",
      "frames: 1010000, reward: 16.000000, loss: 0.001542, epsilon: 0.050000, episode:  460\n",
      "frames: 1011000, reward: 16.000000, loss: 0.003014, epsilon: 0.050000, episode:  460\n",
      "frames: 1012000, reward: 15.700000, loss: 0.001072, epsilon: 0.050000, episode:  461\n",
      "frames: 1013000, reward: 15.700000, loss: 0.000693, epsilon: 0.050000, episode:  461\n",
      "frames: 1014000, reward: 15.700000, loss: 0.000846, epsilon: 0.050000, episode:  461\n",
      "frames: 1015000, reward: 14.100000, loss: 0.000644, epsilon: 0.050000, episode:  462\n",
      "frames: 1016000, reward: 14.100000, loss: 0.000327, epsilon: 0.050000, episode:  462\n",
      "frames: 1017000, reward: 14.100000, loss: 0.000378, epsilon: 0.050000, episode:  462\n",
      "frames: 1018000, reward: 14.600000, loss: 0.000433, epsilon: 0.050000, episode:  463\n",
      "frames: 1019000, reward: 14.600000, loss: 0.000985, epsilon: 0.050000, episode:  463\n",
      "frames: 1020000, reward: 15.200000, loss: 0.000985, epsilon: 0.050000, episode:  464\n",
      "frames: 1021000, reward: 15.200000, loss: 0.000784, epsilon: 0.050000, episode:  464\n",
      "frames: 1022000, reward: 15.500000, loss: 0.000699, epsilon: 0.050000, episode:  465\n",
      "frames: 1023000, reward: 15.500000, loss: 0.001052, epsilon: 0.050000, episode:  465\n",
      "frames: 1024000, reward: 15.400000, loss: 0.001388, epsilon: 0.050000, episode:  466\n",
      "frames: 1025000, reward: 15.400000, loss: 0.002191, epsilon: 0.050000, episode:  466\n",
      "frames: 1026000, reward: 15.400000, loss: 0.000574, epsilon: 0.050000, episode:  466\n",
      "frames: 1027000, reward: 15.400000, loss: 0.000622, epsilon: 0.050000, episode:  467\n",
      "frames: 1028000, reward: 15.400000, loss: 0.000758, epsilon: 0.050000, episode:  467\n",
      "frames: 1029000, reward: 15.800000, loss: 0.000370, epsilon: 0.050000, episode:  468\n",
      "frames: 1030000, reward: 15.800000, loss: 0.001034, epsilon: 0.050000, episode:  468\n",
      "frames: 1031000, reward: 15.800000, loss: 0.000188, epsilon: 0.050000, episode:  468\n",
      "frames: 1032000, reward: 15.600000, loss: 0.006677, epsilon: 0.050000, episode:  469\n",
      "frames: 1033000, reward: 15.600000, loss: 0.001523, epsilon: 0.050000, episode:  469\n",
      "frames: 1034000, reward: 15.600000, loss: 0.000787, epsilon: 0.050000, episode:  469\n",
      "frames: 1035000, reward: 14.400000, loss: 0.000411, epsilon: 0.050000, episode:  470\n",
      "frames: 1036000, reward: 14.400000, loss: 0.000829, epsilon: 0.050000, episode:  470\n",
      "frames: 1037000, reward: 14.400000, loss: 0.002821, epsilon: 0.050000, episode:  471\n",
      "frames: 1038000, reward: 14.400000, loss: 0.001704, epsilon: 0.050000, episode:  471\n",
      "frames: 1039000, reward: 14.400000, loss: 0.001741, epsilon: 0.050000, episode:  471\n",
      "frames: 1040000, reward: 15.800000, loss: 0.000840, epsilon: 0.050000, episode:  472\n",
      "frames: 1041000, reward: 15.800000, loss: 0.000429, epsilon: 0.050000, episode:  472\n",
      "frames: 1042000, reward: 15.800000, loss: 0.001496, epsilon: 0.050000, episode:  472\n",
      "frames: 1043000, reward: 15.100000, loss: 0.001133, epsilon: 0.050000, episode:  473\n",
      "frames: 1044000, reward: 15.100000, loss: 0.000733, epsilon: 0.050000, episode:  473\n",
      "frames: 1045000, reward: 14.800000, loss: 0.000427, epsilon: 0.050000, episode:  474\n",
      "frames: 1046000, reward: 14.800000, loss: 0.000624, epsilon: 0.050000, episode:  474\n",
      "frames: 1047000, reward: 14.800000, loss: 0.000478, epsilon: 0.050000, episode:  474\n",
      "frames: 1048000, reward: 15.000000, loss: 0.001166, epsilon: 0.050000, episode:  475\n",
      "frames: 1049000, reward: 15.000000, loss: 0.001636, epsilon: 0.050000, episode:  475\n",
      "frames: 1050000, reward: 15.000000, loss: 0.002019, epsilon: 0.050000, episode:  476\n",
      "frames: 1051000, reward: 15.000000, loss: 0.000566, epsilon: 0.050000, episode:  476\n",
      "frames: 1052000, reward: 14.800000, loss: 0.000613, epsilon: 0.050000, episode:  477\n",
      "frames: 1053000, reward: 14.800000, loss: 0.000804, epsilon: 0.050000, episode:  477\n",
      "frames: 1054000, reward: 14.800000, loss: 0.000320, epsilon: 0.050000, episode:  477\n",
      "frames: 1055000, reward: 14.000000, loss: 0.000473, epsilon: 0.050000, episode:  478\n",
      "frames: 1056000, reward: 14.000000, loss: 0.000319, epsilon: 0.050000, episode:  478\n",
      "frames: 1057000, reward: 14.600000, loss: 0.000993, epsilon: 0.050000, episode:  479\n",
      "frames: 1058000, reward: 14.600000, loss: 0.000605, epsilon: 0.050000, episode:  479\n",
      "frames: 1059000, reward: 14.600000, loss: 0.000494, epsilon: 0.050000, episode:  479\n",
      "frames: 1060000, reward: 15.300000, loss: 0.000628, epsilon: 0.050000, episode:  480\n",
      "frames: 1061000, reward: 15.300000, loss: 0.000476, epsilon: 0.050000, episode:  480\n",
      "frames: 1062000, reward: 15.100000, loss: 0.001587, epsilon: 0.050000, episode:  481\n",
      "frames: 1063000, reward: 15.100000, loss: 0.000954, epsilon: 0.050000, episode:  481\n",
      "frames: 1064000, reward: 15.100000, loss: 0.000805, epsilon: 0.050000, episode:  481\n",
      "frames: 1065000, reward: 15.500000, loss: 0.000578, epsilon: 0.050000, episode:  482\n",
      "frames: 1066000, reward: 15.500000, loss: 0.000910, epsilon: 0.050000, episode:  482\n",
      "frames: 1067000, reward: 16.200000, loss: 0.000871, epsilon: 0.050000, episode:  483\n",
      "frames: 1068000, reward: 16.200000, loss: 0.001761, epsilon: 0.050000, episode:  483\n",
      "frames: 1069000, reward: 16.200000, loss: 0.000512, epsilon: 0.050000, episode:  484\n",
      "frames: 1070000, reward: 16.200000, loss: 0.001330, epsilon: 0.050000, episode:  484\n",
      "frames: 1071000, reward: 15.900000, loss: 0.000525, epsilon: 0.050000, episode:  485\n",
      "frames: 1072000, reward: 15.900000, loss: 0.001164, epsilon: 0.050000, episode:  485\n",
      "frames: 1073000, reward: 15.900000, loss: 0.000274, epsilon: 0.050000, episode:  485\n",
      "frames: 1074000, reward: 15.500000, loss: 0.000967, epsilon: 0.050000, episode:  486\n",
      "frames: 1075000, reward: 15.500000, loss: 0.000862, epsilon: 0.050000, episode:  486\n",
      "frames: 1076000, reward: 15.500000, loss: 0.000904, epsilon: 0.050000, episode:  486\n",
      "frames: 1077000, reward: 15.400000, loss: 0.002876, epsilon: 0.050000, episode:  487\n",
      "frames: 1078000, reward: 15.400000, loss: 0.000641, epsilon: 0.050000, episode:  487\n",
      "frames: 1079000, reward: 15.300000, loss: 0.000729, epsilon: 0.050000, episode:  488\n",
      "frames: 1080000, reward: 15.300000, loss: 0.000838, epsilon: 0.050000, episode:  488\n",
      "frames: 1081000, reward: 15.400000, loss: 0.001790, epsilon: 0.050000, episode:  489\n",
      "frames: 1082000, reward: 15.400000, loss: 0.000534, epsilon: 0.050000, episode:  489\n",
      "frames: 1083000, reward: 16.000000, loss: 0.000817, epsilon: 0.050000, episode:  490\n",
      "frames: 1084000, reward: 16.000000, loss: 0.000654, epsilon: 0.050000, episode:  490\n",
      "frames: 1085000, reward: 16.400000, loss: 0.001466, epsilon: 0.050000, episode:  491\n",
      "frames: 1086000, reward: 16.400000, loss: 0.001966, epsilon: 0.050000, episode:  491\n",
      "frames: 1087000, reward: 16.400000, loss: 0.000600, epsilon: 0.050000, episode:  491\n",
      "frames: 1088000, reward: 16.300000, loss: 0.000407, epsilon: 0.050000, episode:  492\n",
      "frames: 1089000, reward: 16.300000, loss: 0.000771, epsilon: 0.050000, episode:  492\n",
      "frames: 1090000, reward: 16.300000, loss: 0.000489, epsilon: 0.050000, episode:  492\n",
      "frames: 1091000, reward: 15.600000, loss: 0.000306, epsilon: 0.050000, episode:  493\n",
      "frames: 1092000, reward: 15.600000, loss: 0.000877, epsilon: 0.050000, episode:  493\n",
      "frames: 1093000, reward: 15.700000, loss: 0.000395, epsilon: 0.050000, episode:  494\n",
      "frames: 1094000, reward: 15.700000, loss: 0.000553, epsilon: 0.050000, episode:  494\n",
      "frames: 1095000, reward: 15.800000, loss: 0.000716, epsilon: 0.050000, episode:  495\n",
      "frames: 1096000, reward: 15.800000, loss: 0.000491, epsilon: 0.050000, episode:  495\n",
      "frames: 1097000, reward: 16.300000, loss: 0.000483, epsilon: 0.050000, episode:  496\n",
      "frames: 1098000, reward: 16.300000, loss: 0.000984, epsilon: 0.050000, episode:  496\n",
      "frames: 1099000, reward: 16.600000, loss: 0.001320, epsilon: 0.050000, episode:  497\n",
      "frames: 1100000, reward: 16.600000, loss: 0.000782, epsilon: 0.050000, episode:  497\n",
      "frames: 1101000, reward: 16.600000, loss: 0.000487, epsilon: 0.050000, episode:  497\n",
      "frames: 1102000, reward: 17.200000, loss: 0.000219, epsilon: 0.050000, episode:  498\n",
      "frames: 1103000, reward: 17.200000, loss: 0.000375, epsilon: 0.050000, episode:  498\n",
      "frames: 1104000, reward: 16.900000, loss: 0.000540, epsilon: 0.050000, episode:  499\n",
      "frames: 1105000, reward: 16.900000, loss: 0.000739, epsilon: 0.050000, episode:  499\n",
      "frames: 1106000, reward: 16.400000, loss: 0.000742, epsilon: 0.050000, episode:  500\n",
      "frames: 1107000, reward: 16.400000, loss: 0.001943, epsilon: 0.050000, episode:  500\n",
      "frames: 1108000, reward: 16.400000, loss: 0.000450, epsilon: 0.050000, episode:  501\n",
      "frames: 1109000, reward: 16.400000, loss: 0.001993, epsilon: 0.050000, episode:  501\n",
      "frames: 1110000, reward: 16.400000, loss: 0.001357, epsilon: 0.050000, episode:  501\n",
      "frames: 1111000, reward: 16.200000, loss: 0.000294, epsilon: 0.050000, episode:  502\n",
      "frames: 1112000, reward: 16.200000, loss: 0.000481, epsilon: 0.050000, episode:  502\n",
      "frames: 1113000, reward: 16.200000, loss: 0.000809, epsilon: 0.050000, episode:  502\n",
      "frames: 1114000, reward: 16.400000, loss: 0.000493, epsilon: 0.050000, episode:  503\n",
      "frames: 1115000, reward: 16.400000, loss: 0.000501, epsilon: 0.050000, episode:  503\n",
      "frames: 1116000, reward: 16.200000, loss: 0.000337, epsilon: 0.050000, episode:  504\n",
      "frames: 1117000, reward: 16.200000, loss: 0.007736, epsilon: 0.050000, episode:  504\n",
      "frames: 1118000, reward: 16.200000, loss: 0.000506, epsilon: 0.050000, episode:  504\n",
      "frames: 1119000, reward: 15.800000, loss: 0.000944, epsilon: 0.050000, episode:  505\n",
      "frames: 1120000, reward: 15.800000, loss: 0.000703, epsilon: 0.050000, episode:  505\n",
      "frames: 1121000, reward: 15.800000, loss: 0.000290, epsilon: 0.050000, episode:  506\n",
      "frames: 1122000, reward: 15.800000, loss: 0.000448, epsilon: 0.050000, episode:  506\n",
      "frames: 1123000, reward: 15.600000, loss: 0.000541, epsilon: 0.050000, episode:  507\n",
      "frames: 1124000, reward: 15.600000, loss: 0.000540, epsilon: 0.050000, episode:  507\n",
      "frames: 1125000, reward: 15.800000, loss: 0.000392, epsilon: 0.050000, episode:  508\n",
      "frames: 1126000, reward: 15.800000, loss: 0.001000, epsilon: 0.050000, episode:  508\n",
      "frames: 1127000, reward: 15.600000, loss: 0.000942, epsilon: 0.050000, episode:  509\n",
      "frames: 1128000, reward: 15.600000, loss: 0.000900, epsilon: 0.050000, episode:  509\n",
      "frames: 1129000, reward: 15.600000, loss: 0.000705, epsilon: 0.050000, episode:  509\n",
      "frames: 1130000, reward: 15.700000, loss: 0.000671, epsilon: 0.050000, episode:  510\n",
      "frames: 1131000, reward: 15.700000, loss: 0.000590, epsilon: 0.050000, episode:  510\n",
      "frames: 1132000, reward: 15.700000, loss: 0.000531, epsilon: 0.050000, episode:  511\n",
      "frames: 1133000, reward: 15.700000, loss: 0.001266, epsilon: 0.050000, episode:  511\n",
      "frames: 1134000, reward: 16.000000, loss: 0.000942, epsilon: 0.050000, episode:  512\n",
      "frames: 1135000, reward: 16.000000, loss: 0.000385, epsilon: 0.050000, episode:  512\n",
      "frames: 1136000, reward: 16.000000, loss: 0.005036, epsilon: 0.050000, episode:  512\n",
      "frames: 1137000, reward: 16.000000, loss: 0.006100, epsilon: 0.050000, episode:  513\n",
      "frames: 1138000, reward: 16.000000, loss: 0.000640, epsilon: 0.050000, episode:  513\n",
      "frames: 1139000, reward: 16.300000, loss: 0.000989, epsilon: 0.050000, episode:  514\n",
      "frames: 1140000, reward: 16.300000, loss: 0.000658, epsilon: 0.050000, episode:  514\n",
      "frames: 1141000, reward: 16.300000, loss: 0.000326, epsilon: 0.050000, episode:  515\n",
      "frames: 1142000, reward: 16.300000, loss: 0.000868, epsilon: 0.050000, episode:  515\n",
      "frames: 1143000, reward: 16.300000, loss: 0.001822, epsilon: 0.050000, episode:  515\n",
      "frames: 1144000, reward: 16.200000, loss: 0.000779, epsilon: 0.050000, episode:  516\n",
      "frames: 1145000, reward: 16.200000, loss: 0.000575, epsilon: 0.050000, episode:  516\n",
      "frames: 1146000, reward: 16.800000, loss: 0.000347, epsilon: 0.050000, episode:  517\n",
      "frames: 1147000, reward: 16.800000, loss: 0.000626, epsilon: 0.050000, episode:  517\n",
      "frames: 1148000, reward: 16.100000, loss: 0.000548, epsilon: 0.050000, episode:  518\n",
      "frames: 1149000, reward: 16.100000, loss: 0.001717, epsilon: 0.050000, episode:  518\n",
      "frames: 1150000, reward: 16.100000, loss: 0.000589, epsilon: 0.050000, episode:  518\n",
      "frames: 1151000, reward: 15.600000, loss: 0.002162, epsilon: 0.050000, episode:  519\n",
      "frames: 1152000, reward: 15.600000, loss: 0.000323, epsilon: 0.050000, episode:  519\n",
      "frames: 1153000, reward: 15.600000, loss: 0.001602, epsilon: 0.050000, episode:  519\n",
      "frames: 1154000, reward: 15.400000, loss: 0.000354, epsilon: 0.050000, episode:  520\n",
      "frames: 1155000, reward: 15.400000, loss: 0.002905, epsilon: 0.050000, episode:  520\n",
      "frames: 1156000, reward: 15.500000, loss: 0.005024, epsilon: 0.050000, episode:  521\n",
      "frames: 1157000, reward: 15.500000, loss: 0.000495, epsilon: 0.050000, episode:  521\n",
      "frames: 1158000, reward: 15.600000, loss: 0.000302, epsilon: 0.050000, episode:  522\n",
      "frames: 1159000, reward: 15.600000, loss: 0.001095, epsilon: 0.050000, episode:  522\n",
      "frames: 1160000, reward: 15.800000, loss: 0.000306, epsilon: 0.050000, episode:  523\n",
      "frames: 1161000, reward: 15.800000, loss: 0.000440, epsilon: 0.050000, episode:  523\n",
      "frames: 1162000, reward: 15.800000, loss: 0.000401, epsilon: 0.050000, episode:  523\n",
      "frames: 1163000, reward: 15.400000, loss: 0.001406, epsilon: 0.050000, episode:  524\n",
      "frames: 1164000, reward: 15.400000, loss: 0.000418, epsilon: 0.050000, episode:  524\n",
      "frames: 1165000, reward: 15.500000, loss: 0.000456, epsilon: 0.050000, episode:  525\n",
      "frames: 1166000, reward: 15.500000, loss: 0.001488, epsilon: 0.050000, episode:  525\n",
      "frames: 1167000, reward: 15.500000, loss: 0.002143, epsilon: 0.050000, episode:  525\n",
      "frames: 1168000, reward: 15.300000, loss: 0.000648, epsilon: 0.050000, episode:  526\n",
      "frames: 1169000, reward: 15.300000, loss: 0.000489, epsilon: 0.050000, episode:  526\n",
      "frames: 1170000, reward: 15.100000, loss: 0.001681, epsilon: 0.050000, episode:  527\n",
      "frames: 1171000, reward: 15.100000, loss: 0.000852, epsilon: 0.050000, episode:  527\n",
      "frames: 1172000, reward: 15.100000, loss: 0.000832, epsilon: 0.050000, episode:  527\n",
      "frames: 1173000, reward: 15.500000, loss: 0.000687, epsilon: 0.050000, episode:  528\n",
      "frames: 1174000, reward: 15.500000, loss: 0.000509, epsilon: 0.050000, episode:  528\n",
      "frames: 1175000, reward: 16.400000, loss: 0.000767, epsilon: 0.050000, episode:  529\n",
      "frames: 1176000, reward: 16.400000, loss: 0.000745, epsilon: 0.050000, episode:  529\n",
      "frames: 1177000, reward: 17.200000, loss: 0.000654, epsilon: 0.050000, episode:  530\n",
      "frames: 1178000, reward: 17.200000, loss: 0.000437, epsilon: 0.050000, episode:  530\n",
      "frames: 1179000, reward: 16.600000, loss: 0.000389, epsilon: 0.050000, episode:  531\n",
      "frames: 1180000, reward: 16.600000, loss: 0.000545, epsilon: 0.050000, episode:  531\n",
      "frames: 1181000, reward: 16.500000, loss: 0.000708, epsilon: 0.050000, episode:  532\n",
      "frames: 1182000, reward: 16.500000, loss: 0.000831, epsilon: 0.050000, episode:  532\n",
      "frames: 1183000, reward: 16.500000, loss: 0.000270, epsilon: 0.050000, episode:  532\n",
      "frames: 1184000, reward: 16.500000, loss: 0.000964, epsilon: 0.050000, episode:  532\n",
      "frames: 1185000, reward: 16.200000, loss: 0.000413, epsilon: 0.050000, episode:  533\n",
      "frames: 1186000, reward: 16.200000, loss: 0.000302, epsilon: 0.050000, episode:  533\n",
      "frames: 1187000, reward: 16.300000, loss: 0.000981, epsilon: 0.050000, episode:  534\n",
      "frames: 1188000, reward: 16.300000, loss: 0.000225, epsilon: 0.050000, episode:  534\n",
      "frames: 1189000, reward: 16.700000, loss: 0.000776, epsilon: 0.050000, episode:  535\n",
      "frames: 1190000, reward: 16.700000, loss: 0.000581, epsilon: 0.050000, episode:  535\n",
      "frames: 1191000, reward: 17.100000, loss: 0.000547, epsilon: 0.050000, episode:  536\n",
      "frames: 1192000, reward: 17.100000, loss: 0.001090, epsilon: 0.050000, episode:  536\n",
      "frames: 1193000, reward: 17.200000, loss: 0.000761, epsilon: 0.050000, episode:  537\n",
      "frames: 1194000, reward: 17.200000, loss: 0.000272, epsilon: 0.050000, episode:  537\n",
      "frames: 1195000, reward: 17.100000, loss: 0.000261, epsilon: 0.050000, episode:  538\n",
      "frames: 1196000, reward: 17.100000, loss: 0.000377, epsilon: 0.050000, episode:  538\n",
      "frames: 1197000, reward: 17.100000, loss: 0.000362, epsilon: 0.050000, episode:  538\n",
      "frames: 1198000, reward: 16.800000, loss: 0.002426, epsilon: 0.050000, episode:  539\n",
      "frames: 1199000, reward: 16.800000, loss: 0.001852, epsilon: 0.050000, episode:  539\n",
      "frames: 1200000, reward: 16.600000, loss: 0.001939, epsilon: 0.050000, episode:  540\n",
      "frames: 1201000, reward: 16.600000, loss: 0.000905, epsilon: 0.050000, episode:  540\n",
      "frames: 1202000, reward: 16.900000, loss: 0.000605, epsilon: 0.050000, episode:  541\n",
      "frames: 1203000, reward: 16.900000, loss: 0.000515, epsilon: 0.050000, episode:  541\n",
      "frames: 1204000, reward: 16.900000, loss: 0.000395, epsilon: 0.050000, episode:  541\n",
      "frames: 1205000, reward: 16.600000, loss: 0.000833, epsilon: 0.050000, episode:  542\n",
      "frames: 1206000, reward: 17.300000, loss: 0.000513, epsilon: 0.050000, episode:  543\n",
      "frames: 1207000, reward: 17.300000, loss: 0.001116, epsilon: 0.050000, episode:  543\n",
      "frames: 1208000, reward: 17.400000, loss: 0.000655, epsilon: 0.050000, episode:  544\n",
      "frames: 1209000, reward: 17.400000, loss: 0.000657, epsilon: 0.050000, episode:  544\n",
      "frames: 1210000, reward: 17.200000, loss: 0.000503, epsilon: 0.050000, episode:  545\n",
      "frames: 1211000, reward: 17.200000, loss: 0.001088, epsilon: 0.050000, episode:  545\n",
      "frames: 1212000, reward: 17.200000, loss: 0.001080, epsilon: 0.050000, episode:  545\n",
      "frames: 1213000, reward: 16.900000, loss: 0.000272, epsilon: 0.050000, episode:  546\n",
      "frames: 1214000, reward: 16.900000, loss: 0.001271, epsilon: 0.050000, episode:  546\n",
      "frames: 1215000, reward: 16.800000, loss: 0.000588, epsilon: 0.050000, episode:  547\n",
      "frames: 1216000, reward: 16.800000, loss: 0.000810, epsilon: 0.050000, episode:  547\n",
      "frames: 1217000, reward: 17.300000, loss: 0.001360, epsilon: 0.050000, episode:  548\n",
      "frames: 1218000, reward: 17.300000, loss: 0.000839, epsilon: 0.050000, episode:  548\n",
      "frames: 1219000, reward: 17.600000, loss: 0.000541, epsilon: 0.050000, episode:  549\n",
      "frames: 1220000, reward: 17.600000, loss: 0.000867, epsilon: 0.050000, episode:  549\n",
      "frames: 1221000, reward: 17.600000, loss: 0.000446, epsilon: 0.050000, episode:  549\n",
      "frames: 1222000, reward: 17.300000, loss: 0.000581, epsilon: 0.050000, episode:  550\n",
      "frames: 1223000, reward: 17.300000, loss: 0.000575, epsilon: 0.050000, episode:  550\n",
      "frames: 1224000, reward: 17.600000, loss: 0.000410, epsilon: 0.050000, episode:  551\n",
      "frames: 1225000, reward: 17.600000, loss: 0.000243, epsilon: 0.050000, episode:  551\n",
      "frames: 1226000, reward: 17.900000, loss: 0.000249, epsilon: 0.050000, episode:  552\n",
      "frames: 1227000, reward: 17.900000, loss: 0.001281, epsilon: 0.050000, episode:  552\n",
      "frames: 1228000, reward: 17.900000, loss: 0.000312, epsilon: 0.050000, episode:  552\n",
      "frames: 1229000, reward: 17.200000, loss: 0.002608, epsilon: 0.050000, episode:  553\n",
      "frames: 1230000, reward: 17.200000, loss: 0.000409, epsilon: 0.050000, episode:  553\n",
      "frames: 1231000, reward: 17.000000, loss: 0.000295, epsilon: 0.050000, episode:  554\n",
      "frames: 1232000, reward: 17.000000, loss: 0.000718, epsilon: 0.050000, episode:  554\n",
      "frames: 1233000, reward: 17.000000, loss: 0.016475, epsilon: 0.050000, episode:  555\n",
      "frames: 1234000, reward: 17.000000, loss: 0.000415, epsilon: 0.050000, episode:  555\n",
      "frames: 1235000, reward: 17.000000, loss: 0.000822, epsilon: 0.050000, episode:  556\n",
      "frames: 1236000, reward: 17.000000, loss: 0.000488, epsilon: 0.050000, episode:  556\n",
      "frames: 1237000, reward: 17.000000, loss: 0.000642, epsilon: 0.050000, episode:  556\n",
      "frames: 1238000, reward: 16.600000, loss: 0.000653, epsilon: 0.050000, episode:  557\n",
      "frames: 1239000, reward: 16.600000, loss: 0.000201, epsilon: 0.050000, episode:  557\n",
      "frames: 1240000, reward: 16.700000, loss: 0.000466, epsilon: 0.050000, episode:  558\n",
      "frames: 1241000, reward: 16.700000, loss: 0.001598, epsilon: 0.050000, episode:  558\n",
      "frames: 1242000, reward: 16.800000, loss: 0.000404, epsilon: 0.050000, episode:  559\n",
      "frames: 1243000, reward: 16.800000, loss: 0.000485, epsilon: 0.050000, episode:  559\n",
      "frames: 1244000, reward: 17.000000, loss: 0.003492, epsilon: 0.050000, episode:  560\n",
      "frames: 1245000, reward: 17.000000, loss: 0.000382, epsilon: 0.050000, episode:  560\n",
      "frames: 1246000, reward: 16.800000, loss: 0.000213, epsilon: 0.050000, episode:  561\n",
      "frames: 1247000, reward: 16.800000, loss: 0.000164, epsilon: 0.050000, episode:  561\n",
      "frames: 1248000, reward: 17.100000, loss: 0.000646, epsilon: 0.050000, episode:  562\n",
      "frames: 1249000, reward: 17.100000, loss: 0.000515, epsilon: 0.050000, episode:  562\n",
      "frames: 1250000, reward: 17.100000, loss: 0.000427, epsilon: 0.050000, episode:  562\n",
      "frames: 1251000, reward: 17.500000, loss: 0.000220, epsilon: 0.050000, episode:  563\n",
      "frames: 1252000, reward: 18.000000, loss: 0.000425, epsilon: 0.050000, episode:  564\n",
      "frames: 1253000, reward: 18.000000, loss: 0.001195, epsilon: 0.050000, episode:  564\n",
      "frames: 1254000, reward: 18.100000, loss: 0.001734, epsilon: 0.050000, episode:  565\n",
      "frames: 1255000, reward: 18.100000, loss: 0.000274, epsilon: 0.050000, episode:  565\n",
      "frames: 1256000, reward: 18.100000, loss: 0.001369, epsilon: 0.050000, episode:  566\n",
      "frames: 1257000, reward: 18.100000, loss: 0.000640, epsilon: 0.050000, episode:  566\n",
      "frames: 1258000, reward: 18.200000, loss: 0.001135, epsilon: 0.050000, episode:  567\n",
      "frames: 1259000, reward: 18.200000, loss: 0.000271, epsilon: 0.050000, episode:  567\n",
      "frames: 1260000, reward: 17.900000, loss: 0.001044, epsilon: 0.050000, episode:  568\n",
      "frames: 1261000, reward: 17.900000, loss: 0.000271, epsilon: 0.050000, episode:  568\n",
      "frames: 1262000, reward: 17.800000, loss: 0.000222, epsilon: 0.050000, episode:  569\n",
      "frames: 1263000, reward: 17.800000, loss: 0.000530, epsilon: 0.050000, episode:  569\n",
      "frames: 1264000, reward: 17.800000, loss: 0.000588, epsilon: 0.050000, episode:  570\n",
      "frames: 1265000, reward: 17.800000, loss: 0.000182, epsilon: 0.050000, episode:  570\n",
      "frames: 1266000, reward: 18.300000, loss: 0.000407, epsilon: 0.050000, episode:  571\n",
      "frames: 1267000, reward: 18.300000, loss: 0.000528, epsilon: 0.050000, episode:  571\n",
      "frames: 1268000, reward: 18.000000, loss: 0.000250, epsilon: 0.050000, episode:  572\n",
      "frames: 1269000, reward: 18.000000, loss: 0.000715, epsilon: 0.050000, episode:  572\n",
      "frames: 1270000, reward: 18.100000, loss: 0.000307, epsilon: 0.050000, episode:  573\n",
      "frames: 1271000, reward: 18.100000, loss: 0.000756, epsilon: 0.050000, episode:  573\n",
      "frames: 1272000, reward: 18.100000, loss: 0.001636, epsilon: 0.050000, episode:  573\n",
      "frames: 1273000, reward: 17.700000, loss: 0.000333, epsilon: 0.050000, episode:  574\n",
      "frames: 1274000, reward: 17.700000, loss: 0.000349, epsilon: 0.050000, episode:  574\n",
      "frames: 1275000, reward: 17.500000, loss: 0.000425, epsilon: 0.050000, episode:  575\n",
      "frames: 1276000, reward: 17.500000, loss: 0.000720, epsilon: 0.050000, episode:  575\n",
      "frames: 1277000, reward: 17.600000, loss: 0.000252, epsilon: 0.050000, episode:  576\n",
      "frames: 1278000, reward: 17.600000, loss: 0.001045, epsilon: 0.050000, episode:  576\n",
      "frames: 1279000, reward: 18.100000, loss: 0.000314, epsilon: 0.050000, episode:  577\n",
      "frames: 1280000, reward: 18.100000, loss: 0.005048, epsilon: 0.050000, episode:  577\n",
      "frames: 1281000, reward: 18.400000, loss: 0.000340, epsilon: 0.050000, episode:  578\n",
      "frames: 1282000, reward: 18.400000, loss: 0.000731, epsilon: 0.050000, episode:  578\n",
      "frames: 1283000, reward: 18.300000, loss: 0.000166, epsilon: 0.050000, episode:  579\n",
      "frames: 1284000, reward: 18.300000, loss: 0.000982, epsilon: 0.050000, episode:  579\n",
      "frames: 1285000, reward: 18.300000, loss: 0.000899, epsilon: 0.050000, episode:  579\n",
      "frames: 1286000, reward: 18.000000, loss: 0.000204, epsilon: 0.050000, episode:  580\n",
      "frames: 1287000, reward: 18.000000, loss: 0.000212, epsilon: 0.050000, episode:  580\n",
      "frames: 1288000, reward: 17.500000, loss: 0.000128, epsilon: 0.050000, episode:  581\n",
      "frames: 1289000, reward: 17.500000, loss: 0.000580, epsilon: 0.050000, episode:  581\n",
      "frames: 1290000, reward: 17.600000, loss: 0.000175, epsilon: 0.050000, episode:  582\n",
      "frames: 1291000, reward: 17.600000, loss: 0.001254, epsilon: 0.050000, episode:  582\n",
      "frames: 1292000, reward: 17.800000, loss: 0.000328, epsilon: 0.050000, episode:  583\n",
      "frames: 1293000, reward: 17.800000, loss: 0.000392, epsilon: 0.050000, episode:  583\n",
      "frames: 1294000, reward: 17.400000, loss: 0.000564, epsilon: 0.050000, episode:  584\n",
      "frames: 1295000, reward: 17.400000, loss: 0.000919, epsilon: 0.050000, episode:  584\n",
      "frames: 1296000, reward: 17.400000, loss: 0.000565, epsilon: 0.050000, episode:  584\n",
      "frames: 1297000, reward: 17.600000, loss: 0.000279, epsilon: 0.050000, episode:  585\n",
      "frames: 1298000, reward: 17.600000, loss: 0.000855, epsilon: 0.050000, episode:  585\n",
      "frames: 1299000, reward: 17.400000, loss: 0.000474, epsilon: 0.050000, episode:  586\n",
      "frames: 1300000, reward: 17.400000, loss: 0.001097, epsilon: 0.050000, episode:  586\n",
      "frames: 1301000, reward: 16.900000, loss: 0.000281, epsilon: 0.050000, episode:  587\n",
      "frames: 1302000, reward: 16.900000, loss: 0.000216, epsilon: 0.050000, episode:  587\n",
      "frames: 1303000, reward: 16.800000, loss: 0.000249, epsilon: 0.050000, episode:  588\n",
      "frames: 1304000, reward: 16.800000, loss: 0.001531, epsilon: 0.050000, episode:  588\n",
      "frames: 1305000, reward: 16.800000, loss: 0.000269, epsilon: 0.050000, episode:  588\n",
      "frames: 1306000, reward: 16.000000, loss: 0.000187, epsilon: 0.050000, episode:  589\n",
      "frames: 1307000, reward: 16.000000, loss: 0.000323, epsilon: 0.050000, episode:  589\n",
      "frames: 1308000, reward: 16.100000, loss: 0.000279, epsilon: 0.050000, episode:  590\n",
      "frames: 1309000, reward: 16.100000, loss: 0.000337, epsilon: 0.050000, episode:  590\n",
      "frames: 1310000, reward: 16.100000, loss: 0.000552, epsilon: 0.050000, episode:  590\n",
      "frames: 1311000, reward: 16.100000, loss: 0.000205, epsilon: 0.050000, episode:  591\n",
      "frames: 1312000, reward: 16.100000, loss: 0.004044, epsilon: 0.050000, episode:  591\n",
      "frames: 1313000, reward: 15.700000, loss: 0.000470, epsilon: 0.050000, episode:  592\n",
      "frames: 1314000, reward: 15.700000, loss: 0.000244, epsilon: 0.050000, episode:  592\n",
      "frames: 1315000, reward: 15.500000, loss: 0.000404, epsilon: 0.050000, episode:  593\n",
      "frames: 1316000, reward: 15.500000, loss: 0.000684, epsilon: 0.050000, episode:  593\n",
      "frames: 1317000, reward: 15.800000, loss: 0.000517, epsilon: 0.050000, episode:  594\n",
      "frames: 1318000, reward: 15.800000, loss: 0.000396, epsilon: 0.050000, episode:  594\n",
      "frames: 1319000, reward: 15.800000, loss: 0.000262, epsilon: 0.050000, episode:  594\n",
      "frames: 1320000, reward: 15.800000, loss: 0.000269, epsilon: 0.050000, episode:  595\n",
      "frames: 1321000, reward: 15.800000, loss: 0.000246, epsilon: 0.050000, episode:  595\n",
      "frames: 1322000, reward: 16.300000, loss: 0.001171, epsilon: 0.050000, episode:  596\n",
      "frames: 1323000, reward: 16.300000, loss: 0.000646, epsilon: 0.050000, episode:  596\n",
      "frames: 1324000, reward: 16.300000, loss: 0.000983, epsilon: 0.050000, episode:  597\n",
      "frames: 1325000, reward: 16.300000, loss: 0.000289, epsilon: 0.050000, episode:  597\n",
      "frames: 1326000, reward: 16.300000, loss: 0.000613, epsilon: 0.050000, episode:  597\n",
      "frames: 1327000, reward: 16.000000, loss: 0.000245, epsilon: 0.050000, episode:  598\n",
      "frames: 1328000, reward: 16.000000, loss: 0.001077, epsilon: 0.050000, episode:  598\n",
      "frames: 1329000, reward: 16.700000, loss: 0.000277, epsilon: 0.050000, episode:  599\n",
      "frames: 1330000, reward: 16.700000, loss: 0.000298, epsilon: 0.050000, episode:  599\n",
      "frames: 1331000, reward: 17.100000, loss: 0.000488, epsilon: 0.050000, episode:  600\n",
      "frames: 1332000, reward: 17.100000, loss: 0.001177, epsilon: 0.050000, episode:  600\n",
      "frames: 1333000, reward: 17.500000, loss: 0.000182, epsilon: 0.050000, episode:  601\n",
      "frames: 1334000, reward: 17.500000, loss: 0.000320, epsilon: 0.050000, episode:  601\n",
      "frames: 1335000, reward: 18.000000, loss: 0.000273, epsilon: 0.050000, episode:  602\n",
      "frames: 1336000, reward: 18.000000, loss: 0.000673, epsilon: 0.050000, episode:  602\n",
      "frames: 1337000, reward: 18.000000, loss: 0.000955, epsilon: 0.050000, episode:  602\n",
      "frames: 1338000, reward: 17.300000, loss: 0.000617, epsilon: 0.050000, episode:  603\n",
      "frames: 1339000, reward: 17.300000, loss: 0.000350, epsilon: 0.050000, episode:  603\n",
      "frames: 1340000, reward: 17.600000, loss: 0.000262, epsilon: 0.050000, episode:  604\n",
      "frames: 1341000, reward: 17.600000, loss: 0.000149, epsilon: 0.050000, episode:  604\n",
      "frames: 1342000, reward: 17.600000, loss: 0.000409, epsilon: 0.050000, episode:  604\n",
      "frames: 1343000, reward: 17.300000, loss: 0.001336, epsilon: 0.050000, episode:  605\n",
      "frames: 1344000, reward: 17.300000, loss: 0.002019, epsilon: 0.050000, episode:  605\n",
      "frames: 1345000, reward: 16.600000, loss: 0.001543, epsilon: 0.050000, episode:  606\n",
      "frames: 1346000, reward: 16.600000, loss: 0.000310, epsilon: 0.050000, episode:  606\n",
      "frames: 1347000, reward: 16.600000, loss: 0.000308, epsilon: 0.050000, episode:  606\n",
      "frames: 1348000, reward: 16.200000, loss: 0.000233, epsilon: 0.050000, episode:  607\n",
      "frames: 1349000, reward: 16.200000, loss: 0.000204, epsilon: 0.050000, episode:  607\n",
      "frames: 1350000, reward: 16.200000, loss: 0.000238, epsilon: 0.050000, episode:  608\n",
      "frames: 1351000, reward: 16.200000, loss: 0.000615, epsilon: 0.050000, episode:  608\n",
      "frames: 1352000, reward: 16.200000, loss: 0.000310, epsilon: 0.050000, episode:  609\n",
      "frames: 1353000, reward: 16.200000, loss: 0.000517, epsilon: 0.050000, episode:  609\n",
      "frames: 1354000, reward: 16.000000, loss: 0.000189, epsilon: 0.050000, episode:  610\n",
      "frames: 1355000, reward: 16.000000, loss: 0.000133, epsilon: 0.050000, episode:  610\n",
      "frames: 1356000, reward: 15.600000, loss: 0.000570, epsilon: 0.050000, episode:  611\n",
      "frames: 1357000, reward: 15.600000, loss: 0.000504, epsilon: 0.050000, episode:  611\n",
      "frames: 1358000, reward: 15.600000, loss: 0.000513, epsilon: 0.050000, episode:  611\n",
      "frames: 1359000, reward: 14.600000, loss: 0.000253, epsilon: 0.050000, episode:  612\n",
      "frames: 1360000, reward: 15.500000, loss: 0.001132, epsilon: 0.050000, episode:  613\n",
      "frames: 1361000, reward: 15.500000, loss: 0.000496, epsilon: 0.050000, episode:  613\n",
      "frames: 1362000, reward: 15.500000, loss: 0.000602, epsilon: 0.050000, episode:  613\n",
      "frames: 1363000, reward: 15.400000, loss: 0.001269, epsilon: 0.050000, episode:  614\n",
      "frames: 1364000, reward: 15.400000, loss: 0.000765, epsilon: 0.050000, episode:  614\n",
      "frames: 1365000, reward: 15.300000, loss: 0.000251, epsilon: 0.050000, episode:  615\n",
      "frames: 1366000, reward: 15.300000, loss: 0.000461, epsilon: 0.050000, episode:  615\n",
      "frames: 1367000, reward: 15.300000, loss: 0.000545, epsilon: 0.050000, episode:  615\n",
      "frames: 1368000, reward: 15.100000, loss: 0.000554, epsilon: 0.050000, episode:  616\n",
      "frames: 1369000, reward: 15.100000, loss: 0.003253, epsilon: 0.050000, episode:  616\n",
      "frames: 1370000, reward: 15.100000, loss: 0.001208, epsilon: 0.050000, episode:  616\n",
      "frames: 1371000, reward: 15.600000, loss: 0.000214, epsilon: 0.050000, episode:  617\n",
      "frames: 1372000, reward: 15.600000, loss: 0.000524, epsilon: 0.050000, episode:  617\n",
      "frames: 1373000, reward: 15.600000, loss: 0.002600, epsilon: 0.050000, episode:  618\n",
      "frames: 1374000, reward: 15.600000, loss: 0.000601, epsilon: 0.050000, episode:  618\n",
      "frames: 1375000, reward: 15.800000, loss: 0.000231, epsilon: 0.050000, episode:  619\n",
      "frames: 1376000, reward: 15.800000, loss: 0.000367, epsilon: 0.050000, episode:  619\n",
      "frames: 1377000, reward: 16.100000, loss: 0.000870, epsilon: 0.050000, episode:  620\n",
      "frames: 1378000, reward: 16.100000, loss: 0.000412, epsilon: 0.050000, episode:  620\n",
      "frames: 1379000, reward: 15.800000, loss: 0.001173, epsilon: 0.050000, episode:  621\n",
      "frames: 1380000, reward: 15.800000, loss: 0.000322, epsilon: 0.050000, episode:  621\n",
      "frames: 1381000, reward: 16.800000, loss: 0.001655, epsilon: 0.050000, episode:  622\n",
      "frames: 1382000, reward: 16.800000, loss: 0.000578, epsilon: 0.050000, episode:  622\n",
      "frames: 1383000, reward: 16.200000, loss: 0.000462, epsilon: 0.050000, episode:  623\n",
      "frames: 1384000, reward: 16.200000, loss: 0.000334, epsilon: 0.050000, episode:  623\n",
      "frames: 1385000, reward: 16.200000, loss: 0.001049, epsilon: 0.050000, episode:  623\n",
      "frames: 1386000, reward: 16.000000, loss: 0.000506, epsilon: 0.050000, episode:  624\n",
      "frames: 1387000, reward: 16.400000, loss: 0.000751, epsilon: 0.050000, episode:  625\n",
      "frames: 1388000, reward: 16.400000, loss: 0.001016, epsilon: 0.050000, episode:  625\n",
      "frames: 1389000, reward: 16.400000, loss: 0.000835, epsilon: 0.050000, episode:  625\n",
      "frames: 1390000, reward: 16.400000, loss: 0.002664, epsilon: 0.050000, episode:  625\n",
      "frames: 1391000, reward: 15.900000, loss: 0.000949, epsilon: 0.050000, episode:  626\n",
      "frames: 1392000, reward: 15.900000, loss: 0.000676, epsilon: 0.050000, episode:  626\n",
      "frames: 1393000, reward: 15.900000, loss: 0.000589, epsilon: 0.050000, episode:  627\n",
      "frames: 1394000, reward: 15.900000, loss: 0.000436, epsilon: 0.050000, episode:  627\n",
      "frames: 1395000, reward: 15.900000, loss: 0.002670, epsilon: 0.050000, episode:  627\n",
      "frames: 1396000, reward: 15.700000, loss: 0.000991, epsilon: 0.050000, episode:  628\n",
      "frames: 1397000, reward: 15.700000, loss: 0.000352, epsilon: 0.050000, episode:  628\n",
      "frames: 1398000, reward: 15.200000, loss: 0.002079, epsilon: 0.050000, episode:  629\n",
      "frames: 1399000, reward: 15.200000, loss: 0.000855, epsilon: 0.050000, episode:  629\n",
      "frames: 1400000, reward: 15.200000, loss: 0.000625, epsilon: 0.050000, episode:  629\n",
      "frames: 1401000, reward: 14.900000, loss: 0.000771, epsilon: 0.050000, episode:  630\n",
      "frames: 1402000, reward: 15.500000, loss: 0.001088, epsilon: 0.050000, episode:  631\n",
      "frames: 1403000, reward: 15.500000, loss: 0.000807, epsilon: 0.050000, episode:  631\n",
      "frames: 1404000, reward: 15.500000, loss: 0.000509, epsilon: 0.050000, episode:  631\n",
      "frames: 1405000, reward: 15.000000, loss: 0.000847, epsilon: 0.050000, episode:  632\n",
      "frames: 1406000, reward: 15.600000, loss: 0.000838, epsilon: 0.050000, episode:  633\n",
      "frames: 1407000, reward: 15.600000, loss: 0.000413, epsilon: 0.050000, episode:  633\n",
      "frames: 1408000, reward: 15.600000, loss: 0.007061, epsilon: 0.050000, episode:  633\n",
      "frames: 1409000, reward: 15.700000, loss: 0.000419, epsilon: 0.050000, episode:  634\n",
      "frames: 1410000, reward: 15.800000, loss: 0.000907, epsilon: 0.050000, episode:  635\n",
      "frames: 1411000, reward: 15.800000, loss: 0.001270, epsilon: 0.050000, episode:  635\n",
      "frames: 1412000, reward: 15.800000, loss: 0.000387, epsilon: 0.050000, episode:  635\n",
      "frames: 1413000, reward: 16.700000, loss: 0.000907, epsilon: 0.050000, episode:  636\n",
      "frames: 1414000, reward: 16.700000, loss: 0.000397, epsilon: 0.050000, episode:  636\n",
      "frames: 1415000, reward: 16.400000, loss: 0.000220, epsilon: 0.050000, episode:  637\n",
      "frames: 1416000, reward: 16.400000, loss: 0.000285, epsilon: 0.050000, episode:  637\n",
      "frames: 1417000, reward: 16.800000, loss: 0.001311, epsilon: 0.050000, episode:  638\n",
      "frames: 1418000, reward: 16.800000, loss: 0.000222, epsilon: 0.050000, episode:  638\n",
      "frames: 1419000, reward: 16.800000, loss: 0.000220, epsilon: 0.050000, episode:  639\n",
      "frames: 1420000, reward: 16.800000, loss: 0.000571, epsilon: 0.050000, episode:  639\n",
      "frames: 1421000, reward: 16.800000, loss: 0.002218, epsilon: 0.050000, episode:  639\n",
      "frames: 1422000, reward: 16.400000, loss: 0.000652, epsilon: 0.050000, episode:  640\n",
      "frames: 1423000, reward: 16.400000, loss: 0.000442, epsilon: 0.050000, episode:  640\n",
      "frames: 1424000, reward: 16.500000, loss: 0.000770, epsilon: 0.050000, episode:  641\n",
      "frames: 1425000, reward: 16.500000, loss: 0.002097, epsilon: 0.050000, episode:  641\n",
      "frames: 1426000, reward: 16.500000, loss: 0.000282, epsilon: 0.050000, episode:  641\n",
      "frames: 1427000, reward: 16.800000, loss: 0.000747, epsilon: 0.050000, episode:  642\n",
      "frames: 1428000, reward: 16.800000, loss: 0.000617, epsilon: 0.050000, episode:  642\n",
      "frames: 1429000, reward: 16.800000, loss: 0.000918, epsilon: 0.050000, episode:  643\n",
      "frames: 1430000, reward: 16.800000, loss: 0.001483, epsilon: 0.050000, episode:  643\n",
      "frames: 1431000, reward: 16.800000, loss: 0.000366, epsilon: 0.050000, episode:  643\n",
      "frames: 1432000, reward: 16.300000, loss: 0.000580, epsilon: 0.050000, episode:  644\n",
      "frames: 1433000, reward: 16.300000, loss: 0.000216, epsilon: 0.050000, episode:  644\n",
      "frames: 1434000, reward: 16.200000, loss: 0.000260, epsilon: 0.050000, episode:  645\n",
      "frames: 1435000, reward: 16.200000, loss: 0.000706, epsilon: 0.050000, episode:  645\n",
      "frames: 1436000, reward: 16.700000, loss: 0.000253, epsilon: 0.050000, episode:  646\n",
      "frames: 1437000, reward: 16.700000, loss: 0.000266, epsilon: 0.050000, episode:  646\n",
      "frames: 1438000, reward: 17.200000, loss: 0.000524, epsilon: 0.050000, episode:  647\n",
      "frames: 1439000, reward: 17.200000, loss: 0.000245, epsilon: 0.050000, episode:  647\n",
      "frames: 1440000, reward: 17.200000, loss: 0.000765, epsilon: 0.050000, episode:  648\n",
      "frames: 1441000, reward: 17.200000, loss: 0.000563, epsilon: 0.050000, episode:  648\n",
      "frames: 1442000, reward: 17.500000, loss: 0.000421, epsilon: 0.050000, episode:  649\n",
      "frames: 1443000, reward: 17.500000, loss: 0.000777, epsilon: 0.050000, episode:  649\n",
      "frames: 1444000, reward: 17.300000, loss: 0.000444, epsilon: 0.050000, episode:  650\n",
      "frames: 1445000, reward: 17.300000, loss: 0.000470, epsilon: 0.050000, episode:  650\n",
      "frames: 1446000, reward: 17.300000, loss: 0.001349, epsilon: 0.050000, episode:  650\n",
      "frames: 1447000, reward: 16.900000, loss: 0.000365, epsilon: 0.050000, episode:  651\n",
      "frames: 1448000, reward: 16.900000, loss: 0.000359, epsilon: 0.050000, episode:  651\n",
      "frames: 1449000, reward: 16.900000, loss: 0.000275, epsilon: 0.050000, episode:  652\n",
      "frames: 1450000, reward: 16.900000, loss: 0.000594, epsilon: 0.050000, episode:  652\n",
      "frames: 1451000, reward: 16.700000, loss: 0.001330, epsilon: 0.050000, episode:  653\n",
      "frames: 1452000, reward: 16.700000, loss: 0.000757, epsilon: 0.050000, episode:  653\n",
      "frames: 1453000, reward: 16.700000, loss: 0.000594, epsilon: 0.050000, episode:  653\n",
      "frames: 1454000, reward: 17.100000, loss: 0.001687, epsilon: 0.050000, episode:  654\n",
      "frames: 1455000, reward: 17.100000, loss: 0.004740, epsilon: 0.050000, episode:  654\n",
      "frames: 1456000, reward: 17.000000, loss: 0.001052, epsilon: 0.050000, episode:  655\n",
      "frames: 1457000, reward: 17.000000, loss: 0.002941, epsilon: 0.050000, episode:  655\n",
      "frames: 1458000, reward: 17.000000, loss: 0.000310, epsilon: 0.050000, episode:  656\n",
      "frames: 1459000, reward: 17.000000, loss: 0.000456, epsilon: 0.050000, episode:  656\n",
      "frames: 1460000, reward: 16.700000, loss: 0.000191, epsilon: 0.050000, episode:  657\n",
      "frames: 1461000, reward: 16.700000, loss: 0.016484, epsilon: 0.050000, episode:  657\n",
      "frames: 1462000, reward: 16.600000, loss: 0.000713, epsilon: 0.050000, episode:  658\n",
      "frames: 1463000, reward: 16.600000, loss: 0.000822, epsilon: 0.050000, episode:  658\n",
      "frames: 1464000, reward: 17.000000, loss: 0.000374, epsilon: 0.050000, episode:  659\n",
      "frames: 1465000, reward: 17.000000, loss: 0.000291, epsilon: 0.050000, episode:  659\n",
      "frames: 1466000, reward: 17.000000, loss: 0.000844, epsilon: 0.050000, episode:  659\n",
      "frames: 1467000, reward: 17.200000, loss: 0.000690, epsilon: 0.050000, episode:  660\n",
      "frames: 1468000, reward: 17.600000, loss: 0.000289, epsilon: 0.050000, episode:  661\n",
      "frames: 1469000, reward: 17.600000, loss: 0.000448, epsilon: 0.050000, episode:  661\n",
      "frames: 1470000, reward: 17.600000, loss: 0.000238, epsilon: 0.050000, episode:  661\n",
      "frames: 1471000, reward: 17.700000, loss: 0.000764, epsilon: 0.050000, episode:  662\n",
      "frames: 1472000, reward: 17.700000, loss: 0.001661, epsilon: 0.050000, episode:  662\n",
      "frames: 1473000, reward: 17.900000, loss: 0.000759, epsilon: 0.050000, episode:  663\n",
      "frames: 1474000, reward: 17.900000, loss: 0.000488, epsilon: 0.050000, episode:  663\n",
      "frames: 1475000, reward: 17.800000, loss: 0.000338, epsilon: 0.050000, episode:  664\n",
      "frames: 1476000, reward: 17.800000, loss: 0.000328, epsilon: 0.050000, episode:  664\n",
      "frames: 1477000, reward: 18.000000, loss: 0.000871, epsilon: 0.050000, episode:  665\n",
      "frames: 1478000, reward: 18.000000, loss: 0.000751, epsilon: 0.050000, episode:  665\n",
      "frames: 1479000, reward: 18.100000, loss: 0.000421, epsilon: 0.050000, episode:  666\n",
      "frames: 1480000, reward: 18.100000, loss: 0.000205, epsilon: 0.050000, episode:  666\n",
      "frames: 1481000, reward: 18.300000, loss: 0.001778, epsilon: 0.050000, episode:  667\n",
      "frames: 1482000, reward: 18.300000, loss: 0.000753, epsilon: 0.050000, episode:  667\n",
      "frames: 1483000, reward: 18.200000, loss: 0.000617, epsilon: 0.050000, episode:  668\n",
      "frames: 1484000, reward: 18.200000, loss: 0.000436, epsilon: 0.050000, episode:  668\n",
      "frames: 1485000, reward: 18.200000, loss: 0.000523, epsilon: 0.050000, episode:  668\n",
      "frames: 1486000, reward: 18.000000, loss: 0.014278, epsilon: 0.050000, episode:  669\n",
      "frames: 1487000, reward: 18.000000, loss: 0.000300, epsilon: 0.050000, episode:  669\n",
      "frames: 1488000, reward: 18.200000, loss: 0.000747, epsilon: 0.050000, episode:  670\n",
      "frames: 1489000, reward: 18.200000, loss: 0.000246, epsilon: 0.050000, episode:  670\n",
      "frames: 1490000, reward: 18.100000, loss: 0.000302, epsilon: 0.050000, episode:  671\n",
      "frames: 1491000, reward: 18.100000, loss: 0.000263, epsilon: 0.050000, episode:  671\n",
      "frames: 1492000, reward: 18.000000, loss: 0.000684, epsilon: 0.050000, episode:  672\n",
      "frames: 1493000, reward: 18.000000, loss: 0.000257, epsilon: 0.050000, episode:  672\n",
      "frames: 1494000, reward: 17.400000, loss: 0.000442, epsilon: 0.050000, episode:  673\n",
      "frames: 1495000, reward: 17.400000, loss: 0.000693, epsilon: 0.050000, episode:  673\n",
      "frames: 1496000, reward: 17.800000, loss: 0.000799, epsilon: 0.050000, episode:  674\n",
      "frames: 1497000, reward: 17.800000, loss: 0.000671, epsilon: 0.050000, episode:  674\n",
      "frames: 1498000, reward: 18.000000, loss: 0.000842, epsilon: 0.050000, episode:  675\n",
      "frames: 1499000, reward: 18.000000, loss: 0.000205, epsilon: 0.050000, episode:  675\n",
      "frames: 1500000, reward: 17.800000, loss: 0.000213, epsilon: 0.050000, episode:  676\n",
      "frames: 1501000, reward: 17.800000, loss: 0.000229, epsilon: 0.050000, episode:  676\n",
      "frames: 1502000, reward: 17.800000, loss: 0.000490, epsilon: 0.050000, episode:  677\n",
      "frames: 1503000, reward: 17.800000, loss: 0.000508, epsilon: 0.050000, episode:  677\n",
      "frames: 1504000, reward: 17.800000, loss: 0.000149, epsilon: 0.050000, episode:  678\n",
      "frames: 1505000, reward: 17.800000, loss: 0.000220, epsilon: 0.050000, episode:  678\n",
      "frames: 1506000, reward: 17.600000, loss: 0.000173, epsilon: 0.050000, episode:  679\n",
      "frames: 1507000, reward: 17.600000, loss: 0.000523, epsilon: 0.050000, episode:  679\n",
      "frames: 1508000, reward: 17.600000, loss: 0.000210, epsilon: 0.050000, episode:  679\n",
      "frames: 1509000, reward: 17.800000, loss: 0.000426, epsilon: 0.050000, episode:  680\n",
      "frames: 1510000, reward: 17.800000, loss: 0.000442, epsilon: 0.050000, episode:  680\n",
      "frames: 1511000, reward: 17.900000, loss: 0.000496, epsilon: 0.050000, episode:  681\n",
      "frames: 1512000, reward: 17.900000, loss: 0.001132, epsilon: 0.050000, episode:  681\n",
      "frames: 1513000, reward: 17.900000, loss: 0.001334, epsilon: 0.050000, episode:  681\n",
      "frames: 1514000, reward: 18.000000, loss: 0.001242, epsilon: 0.050000, episode:  682\n",
      "frames: 1515000, reward: 18.000000, loss: 0.000244, epsilon: 0.050000, episode:  682\n",
      "frames: 1516000, reward: 18.100000, loss: 0.000238, epsilon: 0.050000, episode:  683\n",
      "frames: 1517000, reward: 18.100000, loss: 0.000431, epsilon: 0.050000, episode:  683\n",
      "frames: 1518000, reward: 18.200000, loss: 0.004912, epsilon: 0.050000, episode:  684\n",
      "frames: 1519000, reward: 18.200000, loss: 0.000338, epsilon: 0.050000, episode:  684\n",
      "frames: 1520000, reward: 18.000000, loss: 0.000175, epsilon: 0.050000, episode:  685\n",
      "frames: 1521000, reward: 18.000000, loss: 0.000298, epsilon: 0.050000, episode:  685\n",
      "frames: 1522000, reward: 17.800000, loss: 0.000369, epsilon: 0.050000, episode:  686\n",
      "frames: 1523000, reward: 17.800000, loss: 0.000268, epsilon: 0.050000, episode:  686\n",
      "frames: 1524000, reward: 18.100000, loss: 0.001038, epsilon: 0.050000, episode:  687\n",
      "frames: 1525000, reward: 18.100000, loss: 0.000477, epsilon: 0.050000, episode:  687\n",
      "frames: 1526000, reward: 18.300000, loss: 0.000356, epsilon: 0.050000, episode:  688\n",
      "frames: 1527000, reward: 18.300000, loss: 0.000290, epsilon: 0.050000, episode:  688\n",
      "frames: 1528000, reward: 18.500000, loss: 0.000276, epsilon: 0.050000, episode:  689\n",
      "frames: 1529000, reward: 18.500000, loss: 0.001195, epsilon: 0.050000, episode:  689\n",
      "frames: 1530000, reward: 18.600000, loss: 0.000172, epsilon: 0.050000, episode:  690\n",
      "frames: 1531000, reward: 18.600000, loss: 0.000454, epsilon: 0.050000, episode:  690\n",
      "frames: 1532000, reward: 18.300000, loss: 0.000378, epsilon: 0.050000, episode:  691\n",
      "frames: 1533000, reward: 18.300000, loss: 0.000363, epsilon: 0.050000, episode:  691\n",
      "frames: 1534000, reward: 18.000000, loss: 0.000505, epsilon: 0.050000, episode:  692\n",
      "frames: 1535000, reward: 18.000000, loss: 0.000243, epsilon: 0.050000, episode:  692\n",
      "frames: 1536000, reward: 18.300000, loss: 0.001868, epsilon: 0.050000, episode:  693\n",
      "frames: 1537000, reward: 18.300000, loss: 0.000205, epsilon: 0.050000, episode:  693\n",
      "frames: 1538000, reward: 17.900000, loss: 0.000432, epsilon: 0.050000, episode:  694\n",
      "frames: 1539000, reward: 17.900000, loss: 0.000833, epsilon: 0.050000, episode:  694\n",
      "frames: 1540000, reward: 17.900000, loss: 0.000451, epsilon: 0.050000, episode:  694\n",
      "frames: 1541000, reward: 17.400000, loss: 0.000379, epsilon: 0.050000, episode:  695\n",
      "frames: 1542000, reward: 17.400000, loss: 0.000375, epsilon: 0.050000, episode:  695\n",
      "frames: 1543000, reward: 17.500000, loss: 0.000359, epsilon: 0.050000, episode:  696\n",
      "frames: 1544000, reward: 17.500000, loss: 0.003183, epsilon: 0.050000, episode:  696\n",
      "frames: 1545000, reward: 17.100000, loss: 0.000703, epsilon: 0.050000, episode:  697\n",
      "frames: 1546000, reward: 17.100000, loss: 0.000425, epsilon: 0.050000, episode:  697\n",
      "frames: 1547000, reward: 16.700000, loss: 0.000293, epsilon: 0.050000, episode:  698\n",
      "frames: 1548000, reward: 16.700000, loss: 0.000509, epsilon: 0.050000, episode:  698\n",
      "frames: 1549000, reward: 16.900000, loss: 0.000349, epsilon: 0.050000, episode:  699\n",
      "frames: 1550000, reward: 16.900000, loss: 0.000268, epsilon: 0.050000, episode:  699\n",
      "frames: 1551000, reward: 16.900000, loss: 0.000325, epsilon: 0.050000, episode:  699\n",
      "frames: 1552000, reward: 16.900000, loss: 0.000405, epsilon: 0.050000, episode:  700\n",
      "frames: 1553000, reward: 16.900000, loss: 0.000309, epsilon: 0.050000, episode:  700\n",
      "frames: 1554000, reward: 17.200000, loss: 0.001175, epsilon: 0.050000, episode:  701\n",
      "frames: 1555000, reward: 17.200000, loss: 0.000293, epsilon: 0.050000, episode:  701\n",
      "frames: 1556000, reward: 17.200000, loss: 0.000174, epsilon: 0.050000, episode:  702\n",
      "frames: 1557000, reward: 17.200000, loss: 0.000573, epsilon: 0.050000, episode:  702\n",
      "frames: 1558000, reward: 17.200000, loss: 0.000589, epsilon: 0.050000, episode:  703\n",
      "frames: 1559000, reward: 17.200000, loss: 0.000288, epsilon: 0.050000, episode:  703\n",
      "frames: 1560000, reward: 17.700000, loss: 0.000326, epsilon: 0.050000, episode:  704\n",
      "frames: 1561000, reward: 17.700000, loss: 0.000734, epsilon: 0.050000, episode:  704\n",
      "frames: 1562000, reward: 17.700000, loss: 0.000319, epsilon: 0.050000, episode:  704\n",
      "frames: 1563000, reward: 18.000000, loss: 0.000240, epsilon: 0.050000, episode:  705\n",
      "frames: 1564000, reward: 18.000000, loss: 0.000702, epsilon: 0.050000, episode:  705\n",
      "frames: 1565000, reward: 17.900000, loss: 0.000945, epsilon: 0.050000, episode:  706\n",
      "frames: 1566000, reward: 17.900000, loss: 0.000241, epsilon: 0.050000, episode:  706\n",
      "frames: 1567000, reward: 18.000000, loss: 0.000257, epsilon: 0.050000, episode:  707\n",
      "frames: 1568000, reward: 18.000000, loss: 0.000144, epsilon: 0.050000, episode:  707\n",
      "frames: 1569000, reward: 18.200000, loss: 0.000540, epsilon: 0.050000, episode:  708\n",
      "frames: 1570000, reward: 18.200000, loss: 0.000295, epsilon: 0.050000, episode:  708\n",
      "frames: 1571000, reward: 17.900000, loss: 0.000232, epsilon: 0.050000, episode:  709\n",
      "frames: 1572000, reward: 17.900000, loss: 0.000293, epsilon: 0.050000, episode:  709\n",
      "frames: 1573000, reward: 17.600000, loss: 0.001014, epsilon: 0.050000, episode:  710\n",
      "frames: 1574000, reward: 17.600000, loss: 0.000244, epsilon: 0.050000, episode:  710\n",
      "frames: 1575000, reward: 17.600000, loss: 0.000416, epsilon: 0.050000, episode:  710\n",
      "frames: 1576000, reward: 17.300000, loss: 0.000171, epsilon: 0.050000, episode:  711\n",
      "frames: 1577000, reward: 17.300000, loss: 0.000454, epsilon: 0.050000, episode:  711\n",
      "frames: 1578000, reward: 17.600000, loss: 0.000804, epsilon: 0.050000, episode:  712\n",
      "frames: 1579000, reward: 17.600000, loss: 0.000294, epsilon: 0.050000, episode:  712\n",
      "frames: 1580000, reward: 17.200000, loss: 0.000813, epsilon: 0.050000, episode:  713\n",
      "frames: 1581000, reward: 17.200000, loss: 0.000202, epsilon: 0.050000, episode:  713\n",
      "frames: 1582000, reward: 17.200000, loss: 0.000302, epsilon: 0.050000, episode:  713\n",
      "frames: 1583000, reward: 16.400000, loss: 0.000326, epsilon: 0.050000, episode:  714\n",
      "frames: 1584000, reward: 16.400000, loss: 0.000185, epsilon: 0.050000, episode:  714\n",
      "frames: 1585000, reward: 16.300000, loss: 0.002786, epsilon: 0.050000, episode:  715\n",
      "frames: 1586000, reward: 16.300000, loss: 0.000127, epsilon: 0.050000, episode:  715\n",
      "frames: 1587000, reward: 16.400000, loss: 0.000221, epsilon: 0.050000, episode:  716\n",
      "frames: 1588000, reward: 16.400000, loss: 0.000390, epsilon: 0.050000, episode:  716\n",
      "frames: 1589000, reward: 16.400000, loss: 0.001645, epsilon: 0.050000, episode:  716\n",
      "frames: 1590000, reward: 16.400000, loss: 0.002067, epsilon: 0.050000, episode:  717\n",
      "frames: 1591000, reward: 16.400000, loss: 0.000182, epsilon: 0.050000, episode:  717\n",
      "frames: 1592000, reward: 16.500000, loss: 0.000391, epsilon: 0.050000, episode:  718\n",
      "frames: 1593000, reward: 16.500000, loss: 0.000564, epsilon: 0.050000, episode:  718\n",
      "frames: 1594000, reward: 16.300000, loss: 0.000369, epsilon: 0.050000, episode:  719\n",
      "frames: 1595000, reward: 16.300000, loss: 0.000450, epsilon: 0.050000, episode:  719\n",
      "frames: 1596000, reward: 16.100000, loss: 0.000505, epsilon: 0.050000, episode:  720\n",
      "frames: 1597000, reward: 16.100000, loss: 0.000333, epsilon: 0.050000, episode:  720\n",
      "frames: 1598000, reward: 16.100000, loss: 0.000314, epsilon: 0.050000, episode:  720\n",
      "frames: 1599000, reward: 16.000000, loss: 0.000398, epsilon: 0.050000, episode:  721\n",
      "frames: 1600000, reward: 16.000000, loss: 0.000446, epsilon: 0.050000, episode:  721\n",
      "frames: 1601000, reward: 15.900000, loss: 0.000226, epsilon: 0.050000, episode:  722\n",
      "frames: 1602000, reward: 15.900000, loss: 0.000418, epsilon: 0.050000, episode:  722\n",
      "frames: 1603000, reward: 16.300000, loss: 0.000418, epsilon: 0.050000, episode:  723\n",
      "frames: 1604000, reward: 16.300000, loss: 0.000562, epsilon: 0.050000, episode:  723\n",
      "frames: 1605000, reward: 16.800000, loss: 0.000506, epsilon: 0.050000, episode:  724\n",
      "frames: 1606000, reward: 16.800000, loss: 0.000233, epsilon: 0.050000, episode:  724\n",
      "frames: 1607000, reward: 16.900000, loss: 0.001595, epsilon: 0.050000, episode:  725\n",
      "frames: 1608000, reward: 16.900000, loss: 0.002934, epsilon: 0.050000, episode:  725\n",
      "frames: 1609000, reward: 17.000000, loss: 0.000210, epsilon: 0.050000, episode:  726\n",
      "frames: 1610000, reward: 17.000000, loss: 0.000258, epsilon: 0.050000, episode:  726\n",
      "frames: 1611000, reward: 16.700000, loss: 0.000765, epsilon: 0.050000, episode:  727\n",
      "frames: 1612000, reward: 16.700000, loss: 0.000170, epsilon: 0.050000, episode:  727\n",
      "frames: 1613000, reward: 16.700000, loss: 0.000601, epsilon: 0.050000, episode:  727\n",
      "frames: 1614000, reward: 15.700000, loss: 0.000194, epsilon: 0.050000, episode:  728\n",
      "frames: 1615000, reward: 15.700000, loss: 0.001256, epsilon: 0.050000, episode:  728\n",
      "frames: 1616000, reward: 15.700000, loss: 0.000361, epsilon: 0.050000, episode:  729\n",
      "frames: 1617000, reward: 15.700000, loss: 0.000317, epsilon: 0.050000, episode:  729\n",
      "frames: 1618000, reward: 15.700000, loss: 0.000587, epsilon: 0.050000, episode:  729\n",
      "frames: 1619000, reward: 15.600000, loss: 0.000561, epsilon: 0.050000, episode:  730\n",
      "frames: 1620000, reward: 15.600000, loss: 0.000232, epsilon: 0.050000, episode:  730\n",
      "frames: 1621000, reward: 15.800000, loss: 0.000729, epsilon: 0.050000, episode:  731\n",
      "frames: 1622000, reward: 15.800000, loss: 0.000405, epsilon: 0.050000, episode:  731\n",
      "frames: 1623000, reward: 15.500000, loss: 0.000326, epsilon: 0.050000, episode:  732\n",
      "frames: 1624000, reward: 15.500000, loss: 0.001011, epsilon: 0.050000, episode:  732\n",
      "frames: 1625000, reward: 15.500000, loss: 0.000314, epsilon: 0.050000, episode:  732\n",
      "frames: 1626000, reward: 15.100000, loss: 0.000259, epsilon: 0.050000, episode:  733\n",
      "frames: 1627000, reward: 15.100000, loss: 0.000261, epsilon: 0.050000, episode:  733\n",
      "frames: 1628000, reward: 15.200000, loss: 0.000945, epsilon: 0.050000, episode:  734\n",
      "frames: 1629000, reward: 15.200000, loss: 0.001327, epsilon: 0.050000, episode:  734\n",
      "frames: 1630000, reward: 15.400000, loss: 0.001389, epsilon: 0.050000, episode:  735\n",
      "frames: 1631000, reward: 15.400000, loss: 0.000225, epsilon: 0.050000, episode:  735\n",
      "frames: 1632000, reward: 15.300000, loss: 0.000401, epsilon: 0.050000, episode:  736\n",
      "frames: 1633000, reward: 15.300000, loss: 0.000247, epsilon: 0.050000, episode:  736\n",
      "frames: 1634000, reward: 15.700000, loss: 0.000185, epsilon: 0.050000, episode:  737\n",
      "frames: 1635000, reward: 15.700000, loss: 0.000239, epsilon: 0.050000, episode:  737\n",
      "frames: 1636000, reward: 16.900000, loss: 0.000277, epsilon: 0.050000, episode:  738\n",
      "frames: 1637000, reward: 16.900000, loss: 0.000215, epsilon: 0.050000, episode:  738\n",
      "frames: 1638000, reward: 16.900000, loss: 0.000247, epsilon: 0.050000, episode:  738\n",
      "frames: 1639000, reward: 16.600000, loss: 0.000377, epsilon: 0.050000, episode:  739\n",
      "frames: 1640000, reward: 16.600000, loss: 0.000788, epsilon: 0.050000, episode:  739\n",
      "frames: 1641000, reward: 16.800000, loss: 0.000947, epsilon: 0.050000, episode:  740\n",
      "frames: 1642000, reward: 16.800000, loss: 0.000664, epsilon: 0.050000, episode:  740\n",
      "frames: 1643000, reward: 16.700000, loss: 0.000447, epsilon: 0.050000, episode:  741\n",
      "frames: 1644000, reward: 16.700000, loss: 0.000350, epsilon: 0.050000, episode:  741\n",
      "frames: 1645000, reward: 17.000000, loss: 0.000811, epsilon: 0.050000, episode:  742\n",
      "frames: 1646000, reward: 17.000000, loss: 0.010367, epsilon: 0.050000, episode:  742\n",
      "frames: 1647000, reward: 17.400000, loss: 0.000574, epsilon: 0.050000, episode:  743\n",
      "frames: 1648000, reward: 17.400000, loss: 0.000606, epsilon: 0.050000, episode:  743\n",
      "frames: 1649000, reward: 17.400000, loss: 0.000355, epsilon: 0.050000, episode:  743\n",
      "frames: 1650000, reward: 17.300000, loss: 0.000685, epsilon: 0.050000, episode:  744\n",
      "frames: 1651000, reward: 17.300000, loss: 0.000219, epsilon: 0.050000, episode:  744\n",
      "frames: 1652000, reward: 17.300000, loss: 0.000407, epsilon: 0.050000, episode:  745\n",
      "frames: 1653000, reward: 17.300000, loss: 0.000684, epsilon: 0.050000, episode:  745\n",
      "frames: 1654000, reward: 17.500000, loss: 0.000352, epsilon: 0.050000, episode:  746\n",
      "frames: 1655000, reward: 17.500000, loss: 0.000321, epsilon: 0.050000, episode:  746\n",
      "frames: 1656000, reward: 17.500000, loss: 0.000314, epsilon: 0.050000, episode:  746\n",
      "frames: 1657000, reward: 17.000000, loss: 0.000262, epsilon: 0.050000, episode:  747\n",
      "frames: 1658000, reward: 17.000000, loss: 0.000400, epsilon: 0.050000, episode:  747\n",
      "frames: 1659000, reward: 16.700000, loss: 0.000581, epsilon: 0.050000, episode:  748\n",
      "frames: 1660000, reward: 16.700000, loss: 0.000290, epsilon: 0.050000, episode:  748\n",
      "frames: 1661000, reward: 17.000000, loss: 0.000191, epsilon: 0.050000, episode:  749\n",
      "frames: 1662000, reward: 17.000000, loss: 0.000651, epsilon: 0.050000, episode:  749\n",
      "frames: 1663000, reward: 17.300000, loss: 0.000601, epsilon: 0.050000, episode:  750\n",
      "frames: 1664000, reward: 17.300000, loss: 0.001082, epsilon: 0.050000, episode:  750\n",
      "frames: 1665000, reward: 17.300000, loss: 0.001746, epsilon: 0.050000, episode:  751\n",
      "frames: 1666000, reward: 17.300000, loss: 0.000530, epsilon: 0.050000, episode:  751\n",
      "frames: 1667000, reward: 17.300000, loss: 0.000310, epsilon: 0.050000, episode:  751\n",
      "frames: 1668000, reward: 16.800000, loss: 0.000509, epsilon: 0.050000, episode:  752\n",
      "frames: 1669000, reward: 16.800000, loss: 0.002035, epsilon: 0.050000, episode:  752\n",
      "frames: 1670000, reward: 16.900000, loss: 0.000306, epsilon: 0.050000, episode:  753\n",
      "frames: 1671000, reward: 16.900000, loss: 0.000410, epsilon: 0.050000, episode:  753\n",
      "frames: 1672000, reward: 16.900000, loss: 0.006689, epsilon: 0.050000, episode:  753\n",
      "frames: 1673000, reward: 17.000000, loss: 0.000198, epsilon: 0.050000, episode:  754\n",
      "frames: 1674000, reward: 17.000000, loss: 0.000404, epsilon: 0.050000, episode:  754\n",
      "frames: 1675000, reward: 16.900000, loss: 0.000188, epsilon: 0.050000, episode:  755\n",
      "frames: 1676000, reward: 16.900000, loss: 0.000394, epsilon: 0.050000, episode:  755\n",
      "frames: 1677000, reward: 16.900000, loss: 0.000676, epsilon: 0.050000, episode:  755\n",
      "frames: 1678000, reward: 16.700000, loss: 0.000870, epsilon: 0.050000, episode:  756\n",
      "frames: 1679000, reward: 16.700000, loss: 0.000260, epsilon: 0.050000, episode:  756\n",
      "frames: 1680000, reward: 17.200000, loss: 0.000617, epsilon: 0.050000, episode:  757\n",
      "frames: 1681000, reward: 17.200000, loss: 0.000526, epsilon: 0.050000, episode:  757\n",
      "frames: 1682000, reward: 17.500000, loss: 0.000407, epsilon: 0.050000, episode:  758\n",
      "frames: 1683000, reward: 17.500000, loss: 0.000970, epsilon: 0.050000, episode:  758\n",
      "frames: 1684000, reward: 17.600000, loss: 0.000376, epsilon: 0.050000, episode:  759\n",
      "frames: 1685000, reward: 17.600000, loss: 0.001209, epsilon: 0.050000, episode:  759\n",
      "frames: 1686000, reward: 17.300000, loss: 0.000278, epsilon: 0.050000, episode:  760\n",
      "frames: 1687000, reward: 17.300000, loss: 0.000544, epsilon: 0.050000, episode:  760\n",
      "frames: 1688000, reward: 17.200000, loss: 0.000141, epsilon: 0.050000, episode:  761\n",
      "frames: 1689000, reward: 17.200000, loss: 0.000309, epsilon: 0.050000, episode:  761\n",
      "frames: 1690000, reward: 17.200000, loss: 0.001143, epsilon: 0.050000, episode:  761\n",
      "frames: 1691000, reward: 17.900000, loss: 0.000470, epsilon: 0.050000, episode:  762\n",
      "frames: 1692000, reward: 17.900000, loss: 0.000318, epsilon: 0.050000, episode:  762\n",
      "frames: 1693000, reward: 17.400000, loss: 0.000403, epsilon: 0.050000, episode:  763\n",
      "frames: 1694000, reward: 17.400000, loss: 0.000205, epsilon: 0.050000, episode:  763\n",
      "frames: 1695000, reward: 17.400000, loss: 0.000374, epsilon: 0.050000, episode:  764\n",
      "frames: 1696000, reward: 17.400000, loss: 0.002457, epsilon: 0.050000, episode:  764\n",
      "frames: 1697000, reward: 17.400000, loss: 0.000257, epsilon: 0.050000, episode:  764\n",
      "frames: 1698000, reward: 17.400000, loss: 0.000406, epsilon: 0.050000, episode:  765\n",
      "frames: 1699000, reward: 17.400000, loss: 0.000236, epsilon: 0.050000, episode:  765\n",
      "frames: 1700000, reward: 17.300000, loss: 0.001371, epsilon: 0.050000, episode:  766\n",
      "frames: 1701000, reward: 17.300000, loss: 0.000270, epsilon: 0.050000, episode:  766\n",
      "frames: 1702000, reward: 17.400000, loss: 0.000438, epsilon: 0.050000, episode:  767\n",
      "frames: 1703000, reward: 17.400000, loss: 0.000361, epsilon: 0.050000, episode:  767\n",
      "frames: 1704000, reward: 17.200000, loss: 0.000234, epsilon: 0.050000, episode:  768\n",
      "frames: 1705000, reward: 17.200000, loss: 0.000677, epsilon: 0.050000, episode:  768\n",
      "frames: 1706000, reward: 17.200000, loss: 0.000448, epsilon: 0.050000, episode:  769\n",
      "frames: 1707000, reward: 17.200000, loss: 0.001252, epsilon: 0.050000, episode:  769\n",
      "frames: 1708000, reward: 17.300000, loss: 0.000281, epsilon: 0.050000, episode:  770\n",
      "frames: 1709000, reward: 17.300000, loss: 0.000511, epsilon: 0.050000, episode:  770\n",
      "frames: 1710000, reward: 17.300000, loss: 0.001590, epsilon: 0.050000, episode:  770\n",
      "frames: 1711000, reward: 17.400000, loss: 0.000118, epsilon: 0.050000, episode:  771\n",
      "frames: 1712000, reward: 17.400000, loss: 0.000369, epsilon: 0.050000, episode:  771\n",
      "frames: 1713000, reward: 17.200000, loss: 0.002009, epsilon: 0.050000, episode:  772\n",
      "frames: 1714000, reward: 17.200000, loss: 0.000137, epsilon: 0.050000, episode:  772\n",
      "frames: 1715000, reward: 17.600000, loss: 0.000502, epsilon: 0.050000, episode:  773\n",
      "frames: 1716000, reward: 17.600000, loss: 0.000248, epsilon: 0.050000, episode:  773\n",
      "frames: 1717000, reward: 17.600000, loss: 0.000722, epsilon: 0.050000, episode:  774\n",
      "frames: 1718000, reward: 17.600000, loss: 0.000455, epsilon: 0.050000, episode:  774\n",
      "frames: 1719000, reward: 17.800000, loss: 0.000345, epsilon: 0.050000, episode:  775\n",
      "frames: 1720000, reward: 17.800000, loss: 0.000246, epsilon: 0.050000, episode:  775\n",
      "frames: 1721000, reward: 18.100000, loss: 0.000310, epsilon: 0.050000, episode:  776\n",
      "frames: 1722000, reward: 18.100000, loss: 0.000458, epsilon: 0.050000, episode:  776\n",
      "frames: 1723000, reward: 18.000000, loss: 0.000543, epsilon: 0.050000, episode:  777\n",
      "frames: 1724000, reward: 18.000000, loss: 0.000250, epsilon: 0.050000, episode:  777\n",
      "frames: 1725000, reward: 18.000000, loss: 0.000340, epsilon: 0.050000, episode:  777\n",
      "frames: 1726000, reward: 17.800000, loss: 0.000412, epsilon: 0.050000, episode:  778\n",
      "frames: 1727000, reward: 17.800000, loss: 0.000332, epsilon: 0.050000, episode:  778\n",
      "frames: 1728000, reward: 18.100000, loss: 0.000335, epsilon: 0.050000, episode:  779\n",
      "frames: 1729000, reward: 18.100000, loss: 0.000403, epsilon: 0.050000, episode:  779\n",
      "frames: 1730000, reward: 18.200000, loss: 0.000465, epsilon: 0.050000, episode:  780\n",
      "frames: 1731000, reward: 18.200000, loss: 0.001223, epsilon: 0.050000, episode:  780\n",
      "frames: 1732000, reward: 18.000000, loss: 0.000297, epsilon: 0.050000, episode:  781\n",
      "frames: 1733000, reward: 18.000000, loss: 0.000344, epsilon: 0.050000, episode:  781\n",
      "frames: 1734000, reward: 18.300000, loss: 0.000803, epsilon: 0.050000, episode:  782\n",
      "frames: 1735000, reward: 18.300000, loss: 0.000182, epsilon: 0.050000, episode:  782\n",
      "frames: 1736000, reward: 18.400000, loss: 0.000381, epsilon: 0.050000, episode:  783\n",
      "frames: 1737000, reward: 18.400000, loss: 0.001121, epsilon: 0.050000, episode:  783\n",
      "frames: 1738000, reward: 18.400000, loss: 0.000391, epsilon: 0.050000, episode:  783\n",
      "frames: 1739000, reward: 18.300000, loss: 0.000156, epsilon: 0.050000, episode:  784\n",
      "frames: 1740000, reward: 18.300000, loss: 0.000651, epsilon: 0.050000, episode:  784\n",
      "frames: 1741000, reward: 18.200000, loss: 0.000495, epsilon: 0.050000, episode:  785\n",
      "frames: 1742000, reward: 18.200000, loss: 0.000624, epsilon: 0.050000, episode:  785\n",
      "frames: 1743000, reward: 17.700000, loss: 0.000252, epsilon: 0.050000, episode:  786\n",
      "frames: 1744000, reward: 17.700000, loss: 0.000328, epsilon: 0.050000, episode:  786\n",
      "frames: 1745000, reward: 17.300000, loss: 0.000252, epsilon: 0.050000, episode:  787\n",
      "frames: 1746000, reward: 17.300000, loss: 0.000507, epsilon: 0.050000, episode:  787\n",
      "frames: 1747000, reward: 17.300000, loss: 0.000276, epsilon: 0.050000, episode:  787\n",
      "frames: 1748000, reward: 17.400000, loss: 0.000322, epsilon: 0.050000, episode:  788\n",
      "frames: 1749000, reward: 17.400000, loss: 0.000788, epsilon: 0.050000, episode:  788\n",
      "frames: 1750000, reward: 17.000000, loss: 0.000617, epsilon: 0.050000, episode:  789\n",
      "frames: 1751000, reward: 17.000000, loss: 0.000962, epsilon: 0.050000, episode:  789\n",
      "frames: 1752000, reward: 17.400000, loss: 0.000214, epsilon: 0.050000, episode:  790\n",
      "frames: 1753000, reward: 17.400000, loss: 0.000204, epsilon: 0.050000, episode:  790\n",
      "frames: 1754000, reward: 17.400000, loss: 0.000547, epsilon: 0.050000, episode:  791\n",
      "frames: 1755000, reward: 17.400000, loss: 0.000204, epsilon: 0.050000, episode:  791\n",
      "frames: 1756000, reward: 17.400000, loss: 0.001162, epsilon: 0.050000, episode:  791\n",
      "frames: 1757000, reward: 17.100000, loss: 0.000620, epsilon: 0.050000, episode:  792\n",
      "frames: 1758000, reward: 17.100000, loss: 0.000173, epsilon: 0.050000, episode:  792\n",
      "frames: 1759000, reward: 16.700000, loss: 0.000378, epsilon: 0.050000, episode:  793\n",
      "frames: 1760000, reward: 16.700000, loss: 0.001051, epsilon: 0.050000, episode:  793\n",
      "frames: 1761000, reward: 16.700000, loss: 0.000383, epsilon: 0.050000, episode:  794\n",
      "frames: 1762000, reward: 16.700000, loss: 0.000801, epsilon: 0.050000, episode:  794\n",
      "frames: 1763000, reward: 16.400000, loss: 0.000390, epsilon: 0.050000, episode:  795\n",
      "frames: 1764000, reward: 16.400000, loss: 0.000773, epsilon: 0.050000, episode:  795\n",
      "frames: 1765000, reward: 16.200000, loss: 0.000328, epsilon: 0.050000, episode:  796\n",
      "frames: 1766000, reward: 16.200000, loss: 0.001419, epsilon: 0.050000, episode:  796\n",
      "frames: 1767000, reward: 16.600000, loss: 0.000269, epsilon: 0.050000, episode:  797\n",
      "frames: 1768000, reward: 16.600000, loss: 0.000524, epsilon: 0.050000, episode:  797\n",
      "frames: 1769000, reward: 16.600000, loss: 0.000495, epsilon: 0.050000, episode:  797\n",
      "frames: 1770000, reward: 16.400000, loss: 0.000739, epsilon: 0.050000, episode:  798\n",
      "frames: 1771000, reward: 16.400000, loss: 0.000114, epsilon: 0.050000, episode:  798\n",
      "frames: 1772000, reward: 16.500000, loss: 0.000405, epsilon: 0.050000, episode:  799\n",
      "frames: 1773000, reward: 16.500000, loss: 0.000310, epsilon: 0.050000, episode:  799\n",
      "frames: 1774000, reward: 16.000000, loss: 0.000475, epsilon: 0.050000, episode:  800\n",
      "frames: 1775000, reward: 16.000000, loss: 0.000366, epsilon: 0.050000, episode:  800\n",
      "frames: 1776000, reward: 16.300000, loss: 0.000315, epsilon: 0.050000, episode:  801\n",
      "frames: 1777000, reward: 16.300000, loss: 0.000352, epsilon: 0.050000, episode:  801\n",
      "frames: 1778000, reward: 16.300000, loss: 0.000275, epsilon: 0.050000, episode:  801\n",
      "frames: 1779000, reward: 15.900000, loss: 0.000230, epsilon: 0.050000, episode:  802\n",
      "frames: 1780000, reward: 16.500000, loss: 0.000235, epsilon: 0.050000, episode:  803\n",
      "frames: 1781000, reward: 16.500000, loss: 0.000491, epsilon: 0.050000, episode:  803\n",
      "frames: 1782000, reward: 16.500000, loss: 0.000537, epsilon: 0.050000, episode:  803\n",
      "frames: 1783000, reward: 15.900000, loss: 0.000456, epsilon: 0.050000, episode:  804\n",
      "frames: 1784000, reward: 15.900000, loss: 0.000446, epsilon: 0.050000, episode:  804\n",
      "frames: 1785000, reward: 16.000000, loss: 0.000302, epsilon: 0.050000, episode:  805\n",
      "frames: 1786000, reward: 16.000000, loss: 0.000899, epsilon: 0.050000, episode:  805\n",
      "frames: 1787000, reward: 16.700000, loss: 0.000385, epsilon: 0.050000, episode:  806\n",
      "frames: 1788000, reward: 16.700000, loss: 0.004223, epsilon: 0.050000, episode:  806\n",
      "frames: 1789000, reward: 16.500000, loss: 0.000438, epsilon: 0.050000, episode:  807\n",
      "frames: 1790000, reward: 16.500000, loss: 0.000604, epsilon: 0.050000, episode:  807\n",
      "frames: 1791000, reward: 16.700000, loss: 0.000393, epsilon: 0.050000, episode:  808\n",
      "frames: 1792000, reward: 16.700000, loss: 0.001088, epsilon: 0.050000, episode:  808\n",
      "frames: 1793000, reward: 16.700000, loss: 0.000321, epsilon: 0.050000, episode:  808\n",
      "frames: 1794000, reward: 16.500000, loss: 0.000508, epsilon: 0.050000, episode:  809\n",
      "frames: 1795000, reward: 16.500000, loss: 0.000947, epsilon: 0.050000, episode:  809\n",
      "frames: 1796000, reward: 16.800000, loss: 0.000243, epsilon: 0.050000, episode:  810\n",
      "frames: 1797000, reward: 16.800000, loss: 0.001023, epsilon: 0.050000, episode:  810\n",
      "frames: 1798000, reward: 16.800000, loss: 0.002225, epsilon: 0.050000, episode:  810\n",
      "frames: 1799000, reward: 16.000000, loss: 0.001534, epsilon: 0.050000, episode:  811\n",
      "frames: 1800000, reward: 16.000000, loss: 0.001277, epsilon: 0.050000, episode:  811\n",
      "frames: 1801000, reward: 16.000000, loss: 0.000467, epsilon: 0.050000, episode:  811\n",
      "frames: 1802000, reward: 16.000000, loss: 0.001060, epsilon: 0.050000, episode:  812\n",
      "frames: 1803000, reward: 16.000000, loss: 0.000600, epsilon: 0.050000, episode:  812\n",
      "frames: 1804000, reward: 15.100000, loss: 0.000644, epsilon: 0.050000, episode:  813\n",
      "frames: 1805000, reward: 15.100000, loss: 0.000279, epsilon: 0.050000, episode:  813\n",
      "frames: 1806000, reward: 15.100000, loss: 0.000636, epsilon: 0.050000, episode:  813\n",
      "frames: 1807000, reward: 15.600000, loss: 0.000581, epsilon: 0.050000, episode:  814\n",
      "frames: 1808000, reward: 15.600000, loss: 0.000427, epsilon: 0.050000, episode:  814\n",
      "frames: 1809000, reward: 15.500000, loss: 0.000349, epsilon: 0.050000, episode:  815\n",
      "frames: 1810000, reward: 15.500000, loss: 0.000531, epsilon: 0.050000, episode:  815\n",
      "frames: 1811000, reward: 15.500000, loss: 0.000300, epsilon: 0.050000, episode:  815\n",
      "frames: 1812000, reward: 15.200000, loss: 0.000161, epsilon: 0.050000, episode:  816\n",
      "frames: 1813000, reward: 15.200000, loss: 0.000312, epsilon: 0.050000, episode:  816\n",
      "frames: 1814000, reward: 15.300000, loss: 0.003043, epsilon: 0.050000, episode:  817\n",
      "frames: 1815000, reward: 15.300000, loss: 0.001389, epsilon: 0.050000, episode:  817\n",
      "frames: 1816000, reward: 15.400000, loss: 0.000471, epsilon: 0.050000, episode:  818\n",
      "frames: 1817000, reward: 15.400000, loss: 0.000299, epsilon: 0.050000, episode:  818\n",
      "frames: 1818000, reward: 15.400000, loss: 0.000237, epsilon: 0.050000, episode:  818\n",
      "frames: 1819000, reward: 15.700000, loss: 0.000991, epsilon: 0.050000, episode:  819\n",
      "frames: 1820000, reward: 15.700000, loss: 0.000338, epsilon: 0.050000, episode:  819\n",
      "frames: 1821000, reward: 15.700000, loss: 0.000792, epsilon: 0.050000, episode:  820\n",
      "frames: 1822000, reward: 15.700000, loss: 0.000697, epsilon: 0.050000, episode:  820\n",
      "frames: 1823000, reward: 16.400000, loss: 0.000549, epsilon: 0.050000, episode:  821\n",
      "frames: 1824000, reward: 16.400000, loss: 0.000176, epsilon: 0.050000, episode:  821\n",
      "frames: 1825000, reward: 16.600000, loss: 0.002702, epsilon: 0.050000, episode:  822\n",
      "frames: 1826000, reward: 16.600000, loss: 0.000391, epsilon: 0.050000, episode:  822\n",
      "frames: 1827000, reward: 17.100000, loss: 0.000455, epsilon: 0.050000, episode:  823\n",
      "frames: 1828000, reward: 17.100000, loss: 0.000390, epsilon: 0.050000, episode:  823\n",
      "frames: 1829000, reward: 17.100000, loss: 0.000592, epsilon: 0.050000, episode:  823\n",
      "frames: 1830000, reward: 17.100000, loss: 0.000753, epsilon: 0.050000, episode:  824\n",
      "frames: 1831000, reward: 17.200000, loss: 0.001449, epsilon: 0.050000, episode:  825\n",
      "frames: 1832000, reward: 17.200000, loss: 0.000522, epsilon: 0.050000, episode:  825\n",
      "frames: 1833000, reward: 17.200000, loss: 0.002546, epsilon: 0.050000, episode:  825\n",
      "frames: 1834000, reward: 17.300000, loss: 0.000281, epsilon: 0.050000, episode:  826\n",
      "frames: 1835000, reward: 17.300000, loss: 0.000390, epsilon: 0.050000, episode:  826\n",
      "frames: 1836000, reward: 17.300000, loss: 0.000805, epsilon: 0.050000, episode:  826\n",
      "frames: 1837000, reward: 17.100000, loss: 0.000462, epsilon: 0.050000, episode:  827\n",
      "frames: 1838000, reward: 17.100000, loss: 0.000283, epsilon: 0.050000, episode:  827\n",
      "frames: 1839000, reward: 17.000000, loss: 0.000655, epsilon: 0.050000, episode:  828\n",
      "frames: 1840000, reward: 17.000000, loss: 0.000213, epsilon: 0.050000, episode:  828\n",
      "frames: 1841000, reward: 17.000000, loss: 0.000285, epsilon: 0.050000, episode:  829\n",
      "frames: 1842000, reward: 17.000000, loss: 0.000528, epsilon: 0.050000, episode:  829\n",
      "frames: 1843000, reward: 16.800000, loss: 0.000312, epsilon: 0.050000, episode:  830\n",
      "frames: 1844000, reward: 16.800000, loss: 0.001036, epsilon: 0.050000, episode:  830\n",
      "frames: 1845000, reward: 16.800000, loss: 0.000651, epsilon: 0.050000, episode:  831\n",
      "frames: 1846000, reward: 16.800000, loss: 0.000465, epsilon: 0.050000, episode:  831\n",
      "frames: 1847000, reward: 17.000000, loss: 0.000531, epsilon: 0.050000, episode:  832\n",
      "frames: 1848000, reward: 17.000000, loss: 0.000301, epsilon: 0.050000, episode:  832\n",
      "frames: 1849000, reward: 17.000000, loss: 0.003059, epsilon: 0.050000, episode:  832\n",
      "frames: 1850000, reward: 17.000000, loss: 0.000260, epsilon: 0.050000, episode:  833\n",
      "frames: 1851000, reward: 17.000000, loss: 0.000686, epsilon: 0.050000, episode:  833\n",
      "frames: 1852000, reward: 16.900000, loss: 0.000474, epsilon: 0.050000, episode:  834\n",
      "frames: 1853000, reward: 16.900000, loss: 0.000263, epsilon: 0.050000, episode:  834\n",
      "frames: 1854000, reward: 16.600000, loss: 0.000335, epsilon: 0.050000, episode:  835\n",
      "frames: 1855000, reward: 16.600000, loss: 0.000611, epsilon: 0.050000, episode:  835\n",
      "frames: 1856000, reward: 16.800000, loss: 0.001377, epsilon: 0.050000, episode:  836\n",
      "frames: 1857000, reward: 16.800000, loss: 0.000351, epsilon: 0.050000, episode:  836\n",
      "frames: 1858000, reward: 17.200000, loss: 0.000512, epsilon: 0.050000, episode:  837\n",
      "frames: 1859000, reward: 17.200000, loss: 0.000746, epsilon: 0.050000, episode:  837\n",
      "frames: 1860000, reward: 17.100000, loss: 0.000476, epsilon: 0.050000, episode:  838\n",
      "frames: 1861000, reward: 17.100000, loss: 0.000400, epsilon: 0.050000, episode:  838\n",
      "frames: 1862000, reward: 17.200000, loss: 0.000964, epsilon: 0.050000, episode:  839\n",
      "frames: 1863000, reward: 17.200000, loss: 0.000182, epsilon: 0.050000, episode:  839\n",
      "frames: 1864000, reward: 17.300000, loss: 0.000368, epsilon: 0.050000, episode:  840\n",
      "frames: 1865000, reward: 17.300000, loss: 0.000432, epsilon: 0.050000, episode:  840\n",
      "frames: 1866000, reward: 17.300000, loss: 0.000374, epsilon: 0.050000, episode:  841\n",
      "frames: 1867000, reward: 17.300000, loss: 0.000740, epsilon: 0.050000, episode:  841\n",
      "frames: 1868000, reward: 17.300000, loss: 0.001255, epsilon: 0.050000, episode:  841\n",
      "frames: 1869000, reward: 17.500000, loss: 0.001075, epsilon: 0.050000, episode:  842\n",
      "frames: 1870000, reward: 17.500000, loss: 0.001259, epsilon: 0.050000, episode:  842\n",
      "frames: 1871000, reward: 17.400000, loss: 0.001666, epsilon: 0.050000, episode:  843\n",
      "frames: 1872000, reward: 17.400000, loss: 0.000873, epsilon: 0.050000, episode:  843\n",
      "frames: 1873000, reward: 17.800000, loss: 0.000291, epsilon: 0.050000, episode:  844\n",
      "frames: 1874000, reward: 17.800000, loss: 0.000321, epsilon: 0.050000, episode:  844\n",
      "frames: 1875000, reward: 17.800000, loss: 0.000224, epsilon: 0.050000, episode:  844\n",
      "frames: 1876000, reward: 16.300000, loss: 0.001379, epsilon: 0.050000, episode:  845\n",
      "frames: 1877000, reward: 16.300000, loss: 0.000406, epsilon: 0.050000, episode:  845\n",
      "frames: 1878000, reward: 15.800000, loss: 0.000143, epsilon: 0.050000, episode:  846\n",
      "frames: 1879000, reward: 15.800000, loss: 0.000427, epsilon: 0.050000, episode:  846\n",
      "frames: 1880000, reward: 15.800000, loss: 0.000153, epsilon: 0.050000, episode:  846\n",
      "frames: 1881000, reward: 15.300000, loss: 0.000326, epsilon: 0.050000, episode:  847\n",
      "frames: 1882000, reward: 15.300000, loss: 0.001274, epsilon: 0.050000, episode:  847\n",
      "frames: 1883000, reward: 15.300000, loss: 0.000806, epsilon: 0.050000, episode:  848\n",
      "frames: 1884000, reward: 15.300000, loss: 0.000370, epsilon: 0.050000, episode:  848\n",
      "frames: 1885000, reward: 15.400000, loss: 0.001484, epsilon: 0.050000, episode:  849\n",
      "frames: 1886000, reward: 15.400000, loss: 0.000722, epsilon: 0.050000, episode:  849\n",
      "frames: 1887000, reward: 15.400000, loss: 0.000544, epsilon: 0.050000, episode:  850\n",
      "frames: 1888000, reward: 15.400000, loss: 0.001196, epsilon: 0.050000, episode:  850\n",
      "frames: 1889000, reward: 15.300000, loss: 0.000806, epsilon: 0.050000, episode:  851\n",
      "frames: 1890000, reward: 15.300000, loss: 0.000445, epsilon: 0.050000, episode:  851\n",
      "frames: 1891000, reward: 15.300000, loss: 0.000403, epsilon: 0.050000, episode:  851\n",
      "frames: 1892000, reward: 15.200000, loss: 0.000188, epsilon: 0.050000, episode:  852\n",
      "frames: 1893000, reward: 15.400000, loss: 0.000921, epsilon: 0.050000, episode:  853\n",
      "frames: 1894000, reward: 15.400000, loss: 0.000435, epsilon: 0.050000, episode:  853\n",
      "frames: 1895000, reward: 15.400000, loss: 0.000458, epsilon: 0.050000, episode:  853\n",
      "frames: 1896000, reward: 15.100000, loss: 0.000572, epsilon: 0.050000, episode:  854\n",
      "frames: 1897000, reward: 15.100000, loss: 0.000224, epsilon: 0.050000, episode:  854\n",
      "frames: 1898000, reward: 16.600000, loss: 0.000152, epsilon: 0.050000, episode:  855\n",
      "frames: 1899000, reward: 16.600000, loss: 0.000242, epsilon: 0.050000, episode:  855\n",
      "frames: 1900000, reward: 16.800000, loss: 0.000419, epsilon: 0.050000, episode:  856\n",
      "frames: 1901000, reward: 16.800000, loss: 0.000264, epsilon: 0.050000, episode:  856\n",
      "frames: 1902000, reward: 16.700000, loss: 0.000475, epsilon: 0.050000, episode:  857\n",
      "frames: 1903000, reward: 16.700000, loss: 0.000289, epsilon: 0.050000, episode:  857\n",
      "frames: 1904000, reward: 16.700000, loss: 0.000941, epsilon: 0.050000, episode:  857\n",
      "frames: 1905000, reward: 16.800000, loss: 0.000633, epsilon: 0.050000, episode:  858\n",
      "frames: 1906000, reward: 16.800000, loss: 0.000273, epsilon: 0.050000, episode:  858\n",
      "frames: 1907000, reward: 16.400000, loss: 0.000432, epsilon: 0.050000, episode:  859\n",
      "frames: 1908000, reward: 16.400000, loss: 0.000288, epsilon: 0.050000, episode:  859\n",
      "frames: 1909000, reward: 16.400000, loss: 0.001559, epsilon: 0.050000, episode:  860\n",
      "frames: 1910000, reward: 16.400000, loss: 0.000431, epsilon: 0.050000, episode:  860\n",
      "frames: 1911000, reward: 16.400000, loss: 0.000282, epsilon: 0.050000, episode:  860\n",
      "frames: 1912000, reward: 16.600000, loss: 0.004424, epsilon: 0.050000, episode:  861\n",
      "frames: 1913000, reward: 16.600000, loss: 0.000377, epsilon: 0.050000, episode:  861\n",
      "frames: 1914000, reward: 16.300000, loss: 0.000417, epsilon: 0.050000, episode:  862\n",
      "frames: 1915000, reward: 16.300000, loss: 0.000308, epsilon: 0.050000, episode:  862\n",
      "frames: 1916000, reward: 16.300000, loss: 0.001284, epsilon: 0.050000, episode:  863\n",
      "frames: 1917000, reward: 16.300000, loss: 0.000339, epsilon: 0.050000, episode:  863\n",
      "frames: 1918000, reward: 16.300000, loss: 0.005103, epsilon: 0.050000, episode:  864\n",
      "frames: 1919000, reward: 16.300000, loss: 0.000540, epsilon: 0.050000, episode:  864\n",
      "frames: 1920000, reward: 16.800000, loss: 0.000432, epsilon: 0.050000, episode:  865\n",
      "frames: 1921000, reward: 16.800000, loss: 0.000294, epsilon: 0.050000, episode:  865\n",
      "frames: 1922000, reward: 17.100000, loss: 0.000296, epsilon: 0.050000, episode:  866\n",
      "frames: 1923000, reward: 17.600000, loss: 0.000344, epsilon: 0.050000, episode:  867\n",
      "frames: 1924000, reward: 17.600000, loss: 0.000526, epsilon: 0.050000, episode:  867\n",
      "frames: 1925000, reward: 17.600000, loss: 0.000307, epsilon: 0.050000, episode:  867\n",
      "frames: 1926000, reward: 17.500000, loss: 0.000364, epsilon: 0.050000, episode:  868\n",
      "frames: 1927000, reward: 17.500000, loss: 0.000654, epsilon: 0.050000, episode:  868\n",
      "frames: 1928000, reward: 17.100000, loss: 0.001212, epsilon: 0.050000, episode:  869\n",
      "frames: 1929000, reward: 17.100000, loss: 0.000452, epsilon: 0.050000, episode:  869\n",
      "frames: 1930000, reward: 17.000000, loss: 0.000382, epsilon: 0.050000, episode:  870\n",
      "frames: 1931000, reward: 17.000000, loss: 0.000345, epsilon: 0.050000, episode:  870\n",
      "frames: 1932000, reward: 17.000000, loss: 0.000731, epsilon: 0.050000, episode:  871\n",
      "frames: 1933000, reward: 17.000000, loss: 0.000636, epsilon: 0.050000, episode:  871\n",
      "frames: 1934000, reward: 17.400000, loss: 0.000601, epsilon: 0.050000, episode:  872\n",
      "frames: 1935000, reward: 17.400000, loss: 0.000367, epsilon: 0.050000, episode:  872\n",
      "frames: 1936000, reward: 17.200000, loss: 0.000133, epsilon: 0.050000, episode:  873\n",
      "frames: 1937000, reward: 17.200000, loss: 0.000390, epsilon: 0.050000, episode:  873\n",
      "frames: 1938000, reward: 17.200000, loss: 0.000402, epsilon: 0.050000, episode:  873\n",
      "frames: 1939000, reward: 17.300000, loss: 0.000404, epsilon: 0.050000, episode:  874\n",
      "frames: 1940000, reward: 17.300000, loss: 0.000127, epsilon: 0.050000, episode:  874\n",
      "frames: 1941000, reward: 17.500000, loss: 0.001179, epsilon: 0.050000, episode:  875\n",
      "frames: 1942000, reward: 17.500000, loss: 0.000992, epsilon: 0.050000, episode:  875\n",
      "frames: 1943000, reward: 17.200000, loss: 0.001059, epsilon: 0.050000, episode:  876\n",
      "frames: 1944000, reward: 17.200000, loss: 0.000303, epsilon: 0.050000, episode:  876\n",
      "frames: 1945000, reward: 17.200000, loss: 0.000168, epsilon: 0.050000, episode:  876\n",
      "frames: 1946000, reward: 17.000000, loss: 0.000539, epsilon: 0.050000, episode:  877\n",
      "frames: 1947000, reward: 17.000000, loss: 0.001596, epsilon: 0.050000, episode:  877\n",
      "frames: 1948000, reward: 16.900000, loss: 0.000198, epsilon: 0.050000, episode:  878\n",
      "frames: 1949000, reward: 16.900000, loss: 0.000473, epsilon: 0.050000, episode:  878\n",
      "frames: 1950000, reward: 17.300000, loss: 0.000761, epsilon: 0.050000, episode:  879\n",
      "frames: 1951000, reward: 17.300000, loss: 0.000235, epsilon: 0.050000, episode:  879\n",
      "frames: 1952000, reward: 17.100000, loss: 0.000612, epsilon: 0.050000, episode:  880\n",
      "frames: 1953000, reward: 17.100000, loss: 0.000482, epsilon: 0.050000, episode:  880\n",
      "frames: 1954000, reward: 17.100000, loss: 0.000300, epsilon: 0.050000, episode:  880\n",
      "frames: 1955000, reward: 16.600000, loss: 0.000480, epsilon: 0.050000, episode:  881\n",
      "frames: 1956000, reward: 16.600000, loss: 0.000392, epsilon: 0.050000, episode:  881\n",
      "frames: 1957000, reward: 16.300000, loss: 0.000256, epsilon: 0.050000, episode:  882\n",
      "frames: 1958000, reward: 16.300000, loss: 0.000545, epsilon: 0.050000, episode:  882\n",
      "frames: 1959000, reward: 16.300000, loss: 0.000566, epsilon: 0.050000, episode:  882\n",
      "frames: 1960000, reward: 16.300000, loss: 0.000875, epsilon: 0.050000, episode:  882\n",
      "frames: 1961000, reward: 16.000000, loss: 0.000323, epsilon: 0.050000, episode:  883\n",
      "frames: 1962000, reward: 16.000000, loss: 0.000400, epsilon: 0.050000, episode:  883\n",
      "frames: 1963000, reward: 16.000000, loss: 0.000364, epsilon: 0.050000, episode:  884\n",
      "frames: 1964000, reward: 16.000000, loss: 0.000932, epsilon: 0.050000, episode:  884\n",
      "frames: 1965000, reward: 15.700000, loss: 0.000507, epsilon: 0.050000, episode:  885\n",
      "frames: 1966000, reward: 15.700000, loss: 0.000466, epsilon: 0.050000, episode:  885\n",
      "frames: 1967000, reward: 15.700000, loss: 0.000420, epsilon: 0.050000, episode:  885\n",
      "frames: 1968000, reward: 14.800000, loss: 0.000473, epsilon: 0.050000, episode:  886\n",
      "frames: 1969000, reward: 14.800000, loss: 0.001202, epsilon: 0.050000, episode:  886\n",
      "frames: 1970000, reward: 14.800000, loss: 0.009905, epsilon: 0.050000, episode:  886\n",
      "frames: 1971000, reward: 14.900000, loss: 0.000497, epsilon: 0.050000, episode:  887\n",
      "frames: 1972000, reward: 14.900000, loss: 0.000452, epsilon: 0.050000, episode:  887\n",
      "frames: 1973000, reward: 14.600000, loss: 0.000877, epsilon: 0.050000, episode:  888\n",
      "frames: 1974000, reward: 14.600000, loss: 0.000456, epsilon: 0.050000, episode:  888\n",
      "frames: 1975000, reward: 14.200000, loss: 0.000469, epsilon: 0.050000, episode:  889\n",
      "frames: 1976000, reward: 14.200000, loss: 0.000221, epsilon: 0.050000, episode:  889\n",
      "frames: 1977000, reward: 14.600000, loss: 0.000573, epsilon: 0.050000, episode:  890\n",
      "frames: 1978000, reward: 14.600000, loss: 0.000446, epsilon: 0.050000, episode:  890\n",
      "frames: 1979000, reward: 14.600000, loss: 0.000954, epsilon: 0.050000, episode:  890\n",
      "frames: 1980000, reward: 14.900000, loss: 0.000846, epsilon: 0.050000, episode:  891\n",
      "frames: 1981000, reward: 15.300000, loss: 0.000280, epsilon: 0.050000, episode:  892\n",
      "frames: 1982000, reward: 15.300000, loss: 0.000697, epsilon: 0.050000, episode:  892\n",
      "frames: 1983000, reward: 15.700000, loss: 0.000470, epsilon: 0.050000, episode:  893\n",
      "frames: 1984000, reward: 15.700000, loss: 0.000555, epsilon: 0.050000, episode:  893\n",
      "frames: 1985000, reward: 15.700000, loss: 0.002193, epsilon: 0.050000, episode:  893\n",
      "frames: 1986000, reward: 15.400000, loss: 0.000712, epsilon: 0.050000, episode:  894\n",
      "frames: 1987000, reward: 15.400000, loss: 0.000314, epsilon: 0.050000, episode:  894\n",
      "frames: 1988000, reward: 15.300000, loss: 0.000679, epsilon: 0.050000, episode:  895\n",
      "frames: 1989000, reward: 15.300000, loss: 0.000302, epsilon: 0.050000, episode:  895\n",
      "frames: 1990000, reward: 16.300000, loss: 0.000669, epsilon: 0.050000, episode:  896\n",
      "frames: 1991000, reward: 16.300000, loss: 0.000419, epsilon: 0.050000, episode:  896\n",
      "frames: 1992000, reward: 16.300000, loss: 0.001172, epsilon: 0.050000, episode:  896\n",
      "frames: 1993000, reward: 16.000000, loss: 0.002443, epsilon: 0.050000, episode:  897\n",
      "frames: 1994000, reward: 16.000000, loss: 0.000241, epsilon: 0.050000, episode:  897\n",
      "frames: 1995000, reward: 16.100000, loss: 0.000244, epsilon: 0.050000, episode:  898\n",
      "frames: 1996000, reward: 16.100000, loss: 0.000499, epsilon: 0.050000, episode:  898\n",
      "frames: 1997000, reward: 16.300000, loss: 0.000323, epsilon: 0.050000, episode:  899\n",
      "frames: 1998000, reward: 16.300000, loss: 0.000550, epsilon: 0.050000, episode:  899\n",
      "frames: 1999000, reward: 16.300000, loss: 0.000362, epsilon: 0.050000, episode:  900\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "    \n",
    "# Training DQN in PongNoFrameskip-v4 \n",
    "env = make_atari('PongNoFrameskip-v4')\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True)\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.05\n",
    "eps_decay = 30000\n",
    "frames = 2000000\n",
    "USE_CUDA = True\n",
    "learning_rate = 2e-4\n",
    "max_buff = 100000\n",
    "update_tar_interval = 1000\n",
    "batch_size = 32\n",
    "print_interval = 1000\n",
    "log_interval = 1000\n",
    "learning_start = 10000\n",
    "win_reward = 18     # Pong-v4\n",
    "win_break = True\n",
    "\n",
    "action_space = env.action_space\n",
    "action_dim = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "state_channel = env.observation_space.shape[2]\n",
    "agent = DQNAgent(in_channels = state_channel, action_space= action_space, USE_CUDA = USE_CUDA, lr = learning_rate, memory_size = max_buff)\n",
    "\n",
    "frame = env.reset()\n",
    "\n",
    "episode_reward = 0\n",
    "all_rewards = []\n",
    "losses = []\n",
    "episode_num = 0\n",
    "is_win = False\n",
    "\n",
    "\n",
    "# e-greedy decay\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_min + (epsilon_max - epsilon_min) * math.exp(\n",
    "            -1. * frame_idx / eps_decay)\n",
    "# plt.plot([epsilon_by_frame(i) for i in range(10000)])\n",
    "\n",
    "for i in range(frames):\n",
    "    epsilon = epsilon_by_frame(i)\n",
    "    state_tensor = agent.observe(frame)\n",
    "    action = agent.act(state_tensor, epsilon)\n",
    "    \n",
    "    next_frame, reward, done, _ = env.step(action)\n",
    "    \n",
    "    episode_reward += reward\n",
    "    agent.memory_buffer.push(frame, action, reward, next_frame, done)\n",
    "    frame = next_frame\n",
    "    \n",
    "    loss = 0\n",
    "    if agent.memory_buffer.size() >= learning_start:\n",
    "        loss = agent.learn_from_experience(batch_size)\n",
    "        losses.append(loss)\n",
    "\n",
    "    if i % print_interval == 0:\n",
    "        print(\"frames: %5d, reward: %5f, loss: %4f, epsilon: %5f, episode: %4d\" % (i, np.mean(all_rewards[-10:]), loss, epsilon, episode_num))\n",
    "\n",
    "        \n",
    "    if i % update_tar_interval == 0:\n",
    "        agent.DQN_target.load_state_dict(agent.DQN.state_dict())\n",
    "    \n",
    "    if done:\n",
    "        \n",
    "        frame = env.reset()\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        episode_num += 1\n",
    "        avg_reward = float(np.mean(all_rewards[-100:]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('xclds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f1e55e74c766e763530812631a72120f1fc83c0fcf0b61c7a0638716cdefb0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
